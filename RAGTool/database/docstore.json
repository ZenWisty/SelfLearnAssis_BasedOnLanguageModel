{"docstore/data": {"81cdf377-8113-4980-8b5e-0a2f384af6cd": {"__data__": {"id_": "81cdf377-8113-4980-8b5e-0a2f384af6cd", "embedding": null, "metadata": {"page_label": "1", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb9e5f88-8af3-4b52-b78c-1b15532bde96", "node_type": "4", "metadata": {"page_label": "1", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c942b0b16a6ba63b04a5b1b5f01ec54ca2964287faeb71260f1ca5930281e9d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5a56d15-5de0-482a-8045-45d63103d8c2", "node_type": "1", "metadata": {}, "hash": "cfa62848744f5c8114e92582f374be3a74b30c0308068968dbaa19492faa1998", "class_name": "RelatedNodeInfo"}}, "text": "AGENT AI:\nSURVEYING THE HORIZONS OF MULTIMODAL INTERACTION\nZane Durante1\u2020*, Qiuyuan Huang2\u2021\u2217, Naoki Wake2\u2217,\nRan Gong3\u2020, Jae Sung Park4\u2020, Bidipta Sarkar1\u2020, Rohan Taori1\u2020, Yusuke Noda5,\nDemetri Terzopoulos3, Yejin Choi4, Katsushi Ikeuchi2, Hoi Vo5, Li Fei-Fei1, Jianfeng Gao2\n1Stanford University;2Microsoft Research, Redmond;\n3University of California, Los Angeles;4University of Washington;5Microsoft Gaming\nFigure 1: Overview of an Agent AI system that can perceive and act in different domains and applications. Agent AI is\nemerging as a promising avenue toward Artificial General Intelligence (AGI). Agent AI training has demonstrated the\ncapacity for multi-modal understanding in the physical world. It provides a framework for reality-agnostic training by\nleveraging generative AI alongside multiple independent data sources. Large foundation models trained for agent and\naction-related tasks can be applied to physical and virtual worlds when trained on cross-reality data. We present the\ngeneral overview of an Agent AI system that can perceive and act in many different domains and applications, possibly\nserving as a route towards AGI using an agent paradigm.\n\u2217Equal Contribution.\u2021Project Lead.\u2020Work done while interning at Microsoft Research, Redmond.arXiv:2401.03568v2  [cs.AI]  25 Jan 2024", "start_char_idx": 0, "end_char_idx": 1301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5a56d15-5de0-482a-8045-45d63103d8c2": {"__data__": {"id_": "c5a56d15-5de0-482a-8045-45d63103d8c2", "embedding": null, "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "cd9d4f6aedb55431ab43139a7c910ff48283465cfca577e7d99e4bd55d708e82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81cdf377-8113-4980-8b5e-0a2f384af6cd", "node_type": "1", "metadata": {"page_label": "1", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c942b0b16a6ba63b04a5b1b5f01ec54ca2964287faeb71260f1ca5930281e9d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2606b0d-d911-4a9a-a32a-cedfd67ba04d", "node_type": "1", "metadata": {}, "hash": "c1fbb5b4869db4d87f3fa56c11d879fce6ff0fe11927cdc3e1042214851de65d", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nABSTRACT\nMulti-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising\napproach to making these systems more interactive is to embody them as agents within physical\nand virtual environments. At present, systems leverage existing foundation models as the basic\nbuilding blocks for the creation of embodied agents. Embedding agents within such environments\nfacilitates the ability of models to process and interpret visual and contextual data, which is critical\nfor the creation of more sophisticated and context-aware AI systems. For example, a system that can\nperceive user actions, human behavior, environmental objects, audio expressions, and the collective\nsentiment of a scene can be used to inform and direct agent responses within the given environment.\nTo accelerate research on agent-based multimodal intelligence, we define \u201cAgent AI\u201d as a class of\ninteractive systems that can perceive visual stimuli, language inputs, and other environmentally-\ngrounded data, and can produce meaningful embodied actions. In particular, we explore systems\nthat aim to improve agents based on next-embodied action prediction by incorporating external\nknowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI\nsystems in grounded environments, one can also mitigate the hallucinations of large foundation\nmodels and their tendency to generate environmentally incorrect outputs. The emerging field of Agent\nAI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents\nacting and interacting in the physical world, we envision a future where people can easily create any\nvirtual reality or simulated scene and interact with agents embodied within the virtual environment.\nContents\n1 Introduction 5\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2 Agent AI Integration 7\n2.1 Infinite AI agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Agent AI with Large Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.1 Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "start_char_idx": 0, "end_char_idx": 2594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2606b0d-d911-4a9a-a32a-cedfd67ba04d": {"__data__": {"id_": "d2606b0d-d911-4a9a-a32a-cedfd67ba04d", "embedding": null, "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "cd9d4f6aedb55431ab43139a7c910ff48283465cfca577e7d99e4bd55d708e82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5a56d15-5de0-482a-8045-45d63103d8c2", "node_type": "1", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "fb7b0a5ed12c2b2888f6e927bc346ca008e4d2fdb8f4132b6e7899c3c4351383", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10f95459-622b-4d7c-8a75-d7da83407b25", "node_type": "1", "metadata": {}, "hash": "1525ad0c63a4a10c95bbe31867aa89996f627c9869b337670254d64a34d0b20f", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . . . . . 7\n2.2 Agent AI with Large Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.1 Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.2 Biases and Inclusivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.3 Data Privacy and Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.4 Interpretability and Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2.5 Inference Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.6 Regulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3 Agent AI for Emergent Abilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3 Agent AI Paradigm 15\n3.1 LLMs and VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.2 Agent Transformer Definition . . . . .", "start_char_idx": 2350, "end_char_idx": 3558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10f95459-622b-4d7c-8a75-d7da83407b25": {"__data__": {"id_": "10f95459-622b-4d7c-8a75-d7da83407b25", "embedding": null, "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "cd9d4f6aedb55431ab43139a7c910ff48283465cfca577e7d99e4bd55d708e82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2606b0d-d911-4a9a-a32a-cedfd67ba04d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "59993d68e8f65ee8595e130f76400ebed6acfedcc1af416ee1fc6e78e7620f4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2734990a-266e-46d8-afba-ede52abe7fdc", "node_type": "1", "metadata": {}, "hash": "5288bb6ff0b4d74ed812cf6c2d1b66f57221d09a19e5dcaa9b47e79ba4cd8c08", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3 Agent AI Paradigm 15\n3.1 LLMs and VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.2 Agent Transformer Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3 Agent Transformer Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4 Agent AI Learning 17\n4.1 Strategy and Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.1.1 Reinforcement Learning (RL) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.1.2 Imitation Learning (IL) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.1.3 Traditional RGB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.1.4 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.1.5 Optimization in the Agent System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "start_char_idx": 3307, "end_char_idx": 4507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2734990a-266e-46d8-afba-ede52abe7fdc": {"__data__": {"id_": "2734990a-266e-46d8-afba-ede52abe7fdc", "embedding": null, "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "cd9d4f6aedb55431ab43139a7c910ff48283465cfca577e7d99e4bd55d708e82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10f95459-622b-4d7c-8a75-d7da83407b25", "node_type": "1", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "276a7642b8805811ebfc84f20998500940302f45b30b4ae792738f176d4fdc4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6f8ec65-31f6-4edd-9cd8-d2e7dada01b4", "node_type": "1", "metadata": {}, "hash": "5edfd08ca60f135252cf762134b1cfbe77219a94bf64ef674bb58c368f48ebaa", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . 18\n4.1.4 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.1.5 Optimization in the Agent System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.2 Agent Systems (zero-shot and few-shot level) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2.1 Agent Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2.2 Agent Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.3 Agentic Foundation Models (pretraining and finetune level) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2", "start_char_idx": 4259, "end_char_idx": 5009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6f8ec65-31f6-4edd-9cd8-d2e7dada01b4": {"__data__": {"id_": "b6f8ec65-31f6-4edd-9cd8-d2e7dada01b4", "embedding": null, "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9af32e7f7b65e4e54fbcac08012f6e34f4a7ad6dd5ddf4c3bdd11c5cbbb30313", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2734990a-266e-46d8-afba-ede52abe7fdc", "node_type": "1", "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "8628a276f818bbbaaac7114f1bcd9677619e969508ee164d465652e0e37339b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "341b7487-7583-4265-8d9d-a784e40ca00a", "node_type": "1", "metadata": {}, "hash": "d3334860752626f7dcd7d8bb28e322fd64407f75ed5d7dc63a69535d9c26df18", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n5 Agent AI Categorization 20\n5.1 Generalist Agent Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2 Embodied Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2.1 Action Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2.2 Interactive Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.3 Simulation and Environments Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.4 Generative Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.4.1 AR/VR/mixed-reality Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.5 Knowledge and Logical Inference Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.5.1 Knowledge Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.5.2 Logic Agents . . . . . . . . . . . . . .", "start_char_idx": 0, "end_char_idx": 1252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "341b7487-7583-4265-8d9d-a784e40ca00a": {"__data__": {"id_": "341b7487-7583-4265-8d9d-a784e40ca00a", "embedding": null, "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9af32e7f7b65e4e54fbcac08012f6e34f4a7ad6dd5ddf4c3bdd11c5cbbb30313", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6f8ec65-31f6-4edd-9cd8-d2e7dada01b4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9154002315f7df25477c2a4bb3471712b1f04afc8e502358103f245ff49bec29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80d0e71a-6b8b-4d6c-9574-d9ef431ee753", "node_type": "1", "metadata": {}, "hash": "8f216c137aa796ff4dc9314bbfbc0deef28e4294c1dc20cf52dd81cf3760a5f1", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.5.1 Knowledge Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.5.2 Logic Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.5.3 Agents for Emotional Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.5.4 Neuro-Symbolic Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.6 LLMs and VLMs Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n6 Agent AI Application Tasks 24\n6.1 Agents for Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n6.1.1 NPC Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n6.1.2 Human-NPC Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n6.1.3 Agent-based Analysis of Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "start_char_idx": 1024, "end_char_idx": 2194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80d0e71a-6b8b-4d6c-9574-d9ef431ee753": {"__data__": {"id_": "80d0e71a-6b8b-4d6c-9574-d9ef431ee753", "embedding": null, "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9af32e7f7b65e4e54fbcac08012f6e34f4a7ad6dd5ddf4c3bdd11c5cbbb30313", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "341b7487-7583-4265-8d9d-a784e40ca00a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0e7c130756a0b5dfd8581e8a7b4816367090d118079956076126e6cc5296c67a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "068754be-3e1d-4282-bb2c-2a4c5c95cd7c", "node_type": "1", "metadata": {}, "hash": "a8e190f4d79bec25eff6fbabf606c4be4ed088f25dbad1b0ee3eeb766197b88e", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . 24\n6.1.2 Human-NPC Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n6.1.3 Agent-based Analysis of Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n6.1.4 Scene Synthesis for Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n6.1.5 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n6.2 Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n6.2.1 LLM/VLM Agent for Robotics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n6.2.2 Experiments and Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.3 Healthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n6.3.1 Current Healthcare Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.4 Multimodal Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "start_char_idx": 1948, "end_char_idx": 3130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "068754be-3e1d-4282-bb2c-2a4c5c95cd7c": {"__data__": {"id_": "068754be-3e1d-4282-bb2c-2a4c5c95cd7c", "embedding": null, "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9af32e7f7b65e4e54fbcac08012f6e34f4a7ad6dd5ddf4c3bdd11c5cbbb30313", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80d0e71a-6b8b-4d6c-9574-d9ef431ee753", "node_type": "1", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "bbf49ddcfa4f0cfc1a4e5993b1db462af179b0fd3c6b63ea8d9804e93fb61ed6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5fa4170-3305-474b-b507-3540bf92eb79", "node_type": "1", "metadata": {}, "hash": "9a9f93218af71ebd5f368eb04f0979b40fe616d178f878718775c3644e3a7c65", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . . . . . 35\n6.3.1 Current Healthcare Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.4 Multimodal Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.4.1 Image-Language Understanding and Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.4.2 Video and Language Understanding and Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.4.3 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n6.5 Video-language Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n6.6 Agent for NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.6.1 LLM agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.6.2 General LLM agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.6.3 Instruction-following LLM agents . . . . . . . . . . . . . . . . . . . . . . . . . . .", "start_char_idx": 2885, "end_char_idx": 4106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5fa4170-3305-474b-b507-3540bf92eb79": {"__data__": {"id_": "b5fa4170-3305-474b-b507-3540bf92eb79", "embedding": null, "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9af32e7f7b65e4e54fbcac08012f6e34f4a7ad6dd5ddf4c3bdd11c5cbbb30313", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "068754be-3e1d-4282-bb2c-2a4c5c95cd7c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "95cbea69d883b6d1f2047a99ada3669656d28a2c35ede83fbde49b634f67e1d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4b24378-77d9-46a7-844a-4f5b9176661e", "node_type": "1", "metadata": {}, "hash": "ece2c9c3e57641fef42d9dcd54e8ed7a3e26bf9581a09c1ccc7e3231026981f3", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . 45\n6.6.2 General LLM agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.6.3 Instruction-following LLM agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n6.6.4 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7 Agent AI Across Modalities, Domains, and Realities 48\n7.1 Agents for Cross-modal Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.2 Agents for Cross-domain Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.3 Interactive agent for cross-modality and cross-reality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n7.4 Sim to Real Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n8 Continuous and Self-improvement for Agent AI 49\n8.1 Human-based Interaction Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n8.2 Foundation Model Generated Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n9 Agent Dataset and Leaderboard 50\n9.1 \u201cCuisineWorld\u201d Dataset for Multi-agent Gaming . . . . . . . . . . . . . . . . . . . . . . . .", "start_char_idx": 3862, "end_char_idx": 5239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4b24378-77d9-46a7-844a-4f5b9176661e": {"__data__": {"id_": "e4b24378-77d9-46a7-844a-4f5b9176661e", "embedding": null, "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9af32e7f7b65e4e54fbcac08012f6e34f4a7ad6dd5ddf4c3bdd11c5cbbb30313", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5fa4170-3305-474b-b507-3540bf92eb79", "node_type": "1", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a8395677a57335474f6aed071b8379486b1f0810ab99a5d872c1f6e5b7b9ca6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a38e7a1-46b1-4515-9da5-66d3076df1a7", "node_type": "1", "metadata": {}, "hash": "8e5d14aad9eb85acdb451f2c434008afdff203636ec31e20e979bdb707c4609e", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . 49\n8.2 Foundation Model Generated Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n9 Agent Dataset and Leaderboard 50\n9.1 \u201cCuisineWorld\u201d Dataset for Multi-agent Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n9.1.1 Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n9.1.2 Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n9.1.3 Metrics and Judging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n9.1.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n9.2 Audio-Video-Language Pre-training Dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n10 Broader Impact Statement 52\n11 Ethical Considerations 53\n3", "start_char_idx": 4947, "end_char_idx": 5934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a38e7a1-46b1-4515-9da5-66d3076df1a7": {"__data__": {"id_": "3a38e7a1-46b1-4515-9da5-66d3076df1a7", "embedding": null, "metadata": {"page_label": "4", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c6642c2-3767-4031-aa01-cfae22714085", "node_type": "4", "metadata": {"page_label": "4", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e91590c556d044cb84eb2929b71df0cc096667c3a3f490fafc12e03b39a09769", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4b24378-77d9-46a7-844a-4f5b9176661e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c85f68e990a47f8f24c9a09ec3717160bfbd8709eaf21d6ae157fdd191a128eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69852459-d242-4ee8-af63-b1f48ebdfd21", "node_type": "1", "metadata": {}, "hash": "8d89d313b58b4c99bfd3c37b24d3dd5a2f83881b371ceb9b2aeaf469001b5492", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n12 Diversity Statement 53\nReferences 55\nAppendix 69\nA GPT-4V Agent Prompt Details 69\nB GPT-4V for Bleeding Edge 69\nC GPT-4V for Microsoft Fight Simulator 69\nD GPT-4V for Assassin\u2019s Creed Odyssey 69\nE GPT-4V for GEARS of WAR 4 69\nF GPT-4V for Starfield 75\nAuthor Biographies 77\nAcknowledgemets 80\n4", "start_char_idx": 0, "end_char_idx": 368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69852459-d242-4ee8-af63-b1f48ebdfd21": {"__data__": {"id_": "69852459-d242-4ee8-af63-b1f48ebdfd21", "embedding": null, "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97d1adc-c0f6-4420-9548-7b569b72a06f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "19b30f1296d6f30ad37a709b981c593ac4b7ee32420920f2d451b4a482fc1ad3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a38e7a1-46b1-4515-9da5-66d3076df1a7", "node_type": "1", "metadata": {"page_label": "4", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e91590c556d044cb84eb2929b71df0cc096667c3a3f490fafc12e03b39a09769", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1825435-cf21-4e5c-a227-4496aa8a2a3e", "node_type": "1", "metadata": {}, "hash": "b86f0373914d4379eb92bc4c23b83288af3fb1cb47775a955d2ab8da3a1dd9f2", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n1 Introduction\n1.1 Motivation\nHistorically, AI systems were defined at the 1956 Dartmouth Conference as artificial life forms that could collect\ninformation from the environment and interact with it in useful ways. Motivated by this definition, Minsky\u2019s MIT\ngroup built in 1970 a robotics system, called the \u201cCopy Demo,\u201d that observed \u201cblocks world\u201d scenes and successfully\nreconstructed the observed polyhedral block structures. The system, which comprised observation, planning, and\nmanipulation modules, revealed that each of these subproblems is highly challenging and further research was necessary.\nThe AI field fragmented into specialized subfields that have largely independently made great progress in tackling these\nand other problems, but over-reductionism has blurred the overarching goals of AI research.\nTo advance beyond the status quo, it is necessary to return to AI fundamentals motivated by Aristotelian Holism.\nFortunately, the recent revolution in Large Language Models (LLMs) and Visual Language Models (VLMs) has made it\npossible to create novel AI agents consistent with the holistic ideal. Seizing upon this opportunity, this article explores\nmodels that integrate language proficiency, visual cognition, context memory, intuitive reasoning, and adaptability.\nIt explores the potential completion of this holistic synthesis using LLMs and VLMs. In our exploration, we also\nrevisit system design based on Aristotle\u2019s Final Cause, the teleological \u201cwhy the system exists\u201d, which may have been\noverlooked in previous rounds of AI development.\nWith the advent of powerful pretrained LLMs and VLMs, a renaissance in natural language processing and computer\nvision has been catalyzed. LLMs now demonstrate an impressive ability to decipher the nuances of real-world linguistic\ndata, often achieving abilities that parallel or even surpass human expertise (OpenAI, 2023). Recently, researchers have\nshown that LLMs may be extended to act as agents within various environments, performing intricate actions and tasks\nwhen paired with domain-specific knowledge and modules (Xi et al., 2023). These scenarios, characterized by complex\nreasoning, understanding of the agent\u2019s role and its environment, along with multi-step planning, test the agent\u2019s ability\nto make highly nuanced and intricate decisions within its environmental constraints (Wu et al., 2023; Meta Fundamental\nAI Research (FAIR) Diplomacy Team et al., 2022).\nBuilding upon these initial efforts, the AI community is on the cusp of a significant paradigm shift, transitioning from\ncreating AI models for passive, structured tasks to models capable of assuming dynamic, agentic roles in diverse and\ncomplex environments. In this context, this article investigates the immense potential of using LLMs and VLMs as\nagents, emphasizing models that have a blend of linguistic proficiency, visual cognition, contextual memory, intuitive\nreasoning, and adaptability. Leveraging LLMs and VLMs as agents, especially within domains like gaming, robotics,\nand healthcare, promises not just a rigorous evaluation platform for state-of-the-art AI systems, but also foreshadows\nthe transformative impacts that Agent-centric AI will have across society and industries. When fully harnessed, agentic\nmodels can redefine human experiences and elevate operational standards. The potential for sweeping automation\nushered in by these models portends monumental shifts in industries and socio-economic dynamics. Such advancements\nwill be intertwined with multifaceted leader-board, not only technical but also ethical, as we will elaborate upon in\nSection 11. We delve into the overlapping areas of these sub-fields of Agent AI and illustrate their interconnectedness\nin Fig.1.\n1.2 Background\nWe will now introduce relevant research papers that support the concepts, theoretical background, and modern\nimplementations of Agent AI.\nLarge Foundation Models: LLMs and VLMs have been driving the effort to develop general intelligent machines\n(Bubeck et al., 2023; Mirchandani et al., 2023). Although they are trained using large text corpora, their superior\nproblem-solving capacity is not limited to canonical language processing domains. LLMs can potentially tackle complex\ntasks that were previously presumed to be exclusive to human experts or domain-specific algorithms, ranging from\nmathematical reasoning (Imani et al., 2023; Wei et al., 2022; Zhu et al., 2022) to answering questions of professional\nlaw (Blair-Stanek et al., 2023; Choi et al., 2023; Nay, 2022).", "start_char_idx": 0, "end_char_idx": 4598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1825435-cf21-4e5c-a227-4496aa8a2a3e": {"__data__": {"id_": "d1825435-cf21-4e5c-a227-4496aa8a2a3e", "embedding": null, "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97d1adc-c0f6-4420-9548-7b569b72a06f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "19b30f1296d6f30ad37a709b981c593ac4b7ee32420920f2d451b4a482fc1ad3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69852459-d242-4ee8-af63-b1f48ebdfd21", "node_type": "1", "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "47ed8f1c3219defa8258cbd292a0d1935e6f38bef74a95f0678fd6fc63fb0405", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0158dc11-f838-471c-9d82-1355826b0b2c", "node_type": "1", "metadata": {}, "hash": "07ba5644352e6970be400b7dc32f5496e96de8969218541f6aefd3499d45a822", "class_name": "RelatedNodeInfo"}}, "text": "1.2 Background\nWe will now introduce relevant research papers that support the concepts, theoretical background, and modern\nimplementations of Agent AI.\nLarge Foundation Models: LLMs and VLMs have been driving the effort to develop general intelligent machines\n(Bubeck et al., 2023; Mirchandani et al., 2023). Although they are trained using large text corpora, their superior\nproblem-solving capacity is not limited to canonical language processing domains. LLMs can potentially tackle complex\ntasks that were previously presumed to be exclusive to human experts or domain-specific algorithms, ranging from\nmathematical reasoning (Imani et al., 2023; Wei et al., 2022; Zhu et al., 2022) to answering questions of professional\nlaw (Blair-Stanek et al., 2023; Choi et al., 2023; Nay, 2022). Recent research has shown the possibility of using LLMs\nto generate complex plans for robots and game AI (Liang et al., 2022; Wang et al., 2023a,b; Yao et al., 2023a; Huang\net al., 2023a), marking an important milestone for LLMs as general-purpose intelligent agents.\n5", "start_char_idx": 3809, "end_char_idx": 4868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0158dc11-f838-471c-9d82-1355826b0b2c": {"__data__": {"id_": "0158dc11-f838-471c-9d82-1355826b0b2c", "embedding": null, "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f3e4fe8-abc2-494a-96e6-68a2d06962f9", "node_type": "4", "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "3c2adc6c2d2d9a9375b26e7fbabba793e4cb35f140a56ea5ea1cd3e07e17bb2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1825435-cf21-4e5c-a227-4496aa8a2a3e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "3818fc2ee2907410508ff364a602b6a161878d69115022811783b4c690a88e24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37b7c7e6-02fb-445d-a6cb-354e6c2f46a0", "node_type": "1", "metadata": {}, "hash": "e7736cf648a082060108cd3e200d3556b60fbd53cb092d61047bdcd316740bfc", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nEmbodied AI: A number of works leverage LLMs to perform task planning (Huang et al., 2022a; Wang et al.,\n2023b; Yao et al., 2023a; Li et al., 2023a), specifically the LLMs\u2019 WWW-scale domain knowledge and emergent\nzero-shot embodied abilities to perform complex task planning and reasoning. Recent robotics research also leverages\nLLMs to perform task planning (Ahn et al., 2022a; Huang et al., 2022b; Liang et al., 2022) by decomposing natural\nlanguage instruction into a sequence of subtasks, either in the natural language form or in Python code, then using a\nlow-level controller to execute these subtasks. Additionally, they incorporate environmental feedback to improve task\nperformance (Huang et al., 2022b), (Liang et al., 2022), (Wang et al., 2023a), and (Ikeuchi et al., 2023).\nInteractive Learning: AI agents designed for interactive learning operate using a combination of machine learning\ntechniques and user interactions. Initially, the AI agent is trained on a large dataset. This dataset includes various\ntypes of information, depending on the intended function of the agent. For instance, an AI designed for language tasks\nwould be trained on a massive corpus of text data. The training involves using machine learning algorithms, which\ncould include deep learning models like neural networks. These training models enable the AI to recognize patterns,\nmake predictions, and generate responses based on the data on which it was trained. The AI agent can also learn from\nreal-time interactions with users. This interactive learning can occur in various ways: 1) Feedback-based learning:\nThe AI adapts its responses based on direct user feedback (Li et al., 2023b; Yu et al., 2023a; Parakh et al., 2023; Zha\net al., 2023; Wake et al., 2023a,b,c). For example, if a user corrects the AI\u2019s response, the AI can use this information\nto improve future responses (Zha et al., 2023; Liu et al., 2023a). 2) Observational Learning: The AI observes user\ninteractions and learns implicitly. For example, if users frequently ask similar questions or interact with the AI in a\nparticular way, the AI might adjust its responses to better suit these patterns. It allows the AI agent to understand and\nprocess human language, multi-model setting, interpret the cross reality-context, and generate human-users\u2019 responses.\nOver time, with more user interactions and feedback, the AI agent\u2019s performance generally continuous improves. This\nprocess is often supervised by human operators or developers who ensure that the AI is learning appropriately and not\ndeveloping biases or incorrect patterns.\n1.3 Overview\nMultimodal Agent AI (MAA) is a family of systems that generate effective actions in a given environment based\non the understanding of multimodal sensory input. With the advent of Large Language Models (LLMs) and Vision-\nLanguage Models (VLMs), numerous MAA systems have been proposed in fields ranging from basic research to\napplications. While these research areas are growing rapidly by integrating with the traditional technologies of each\ndomain (e.g., visual question answering and vision-language navigation), they share common interests such as data\ncollection, benchmarking, and ethical perspectives. In this paper, we focus on the some representative research areas of\nMAA, namely multimodality, gaming (VR/AR/MR), robotics, and healthcare, and we aim to provide comprehensive\nknowledge on the common concerns discussed in these fields. As a result we expect to learn the fundamentals of MAA\nand gain insights to further advance their research. Specific learning outcomes include:\n\u2022MAA Overview: A deep dive into its principles and roles in contemporary applications, providing researcher\nwith a thorough grasp of its importance and uses.\n\u2022Methodologies: Detailed examples of how LLMs and VLMs enhance MAAs, illustrated through case studies\nin gaming, robotics, and healthcare.\n\u2022Performance Evaluation: Guidance on the assessment of MAAs with relevant datasets, focusing on their\neffectiveness and generalization.\n\u2022Ethical Considerations: A discussion on the societal impacts and ethical leader-board of deploying Agent AI,\nhighlighting responsible development practices.\n\u2022Emerging Trends and Future leader-board: Categorize the latest developments in each domain and discuss the\nfuture directions.", "start_char_idx": 0, "end_char_idx": 4386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37b7c7e6-02fb-445d-a6cb-354e6c2f46a0": {"__data__": {"id_": "37b7c7e6-02fb-445d-a6cb-354e6c2f46a0", "embedding": null, "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f3e4fe8-abc2-494a-96e6-68a2d06962f9", "node_type": "4", "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "3c2adc6c2d2d9a9375b26e7fbabba793e4cb35f140a56ea5ea1cd3e07e17bb2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0158dc11-f838-471c-9d82-1355826b0b2c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2ab6aaed5fd292cc5f41fee59b362062aa320eb9718b844cd033557da38177ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dc18b0a-0d85-40b5-a7cc-123007d0bb74", "node_type": "1", "metadata": {}, "hash": "7921c4512a5763b3a067507d3106e8baa641286502bfb368831698ad13cde370", "class_name": "RelatedNodeInfo"}}, "text": "As a result we expect to learn the fundamentals of MAA\nand gain insights to further advance their research. Specific learning outcomes include:\n\u2022MAA Overview: A deep dive into its principles and roles in contemporary applications, providing researcher\nwith a thorough grasp of its importance and uses.\n\u2022Methodologies: Detailed examples of how LLMs and VLMs enhance MAAs, illustrated through case studies\nin gaming, robotics, and healthcare.\n\u2022Performance Evaluation: Guidance on the assessment of MAAs with relevant datasets, focusing on their\neffectiveness and generalization.\n\u2022Ethical Considerations: A discussion on the societal impacts and ethical leader-board of deploying Agent AI,\nhighlighting responsible development practices.\n\u2022Emerging Trends and Future leader-board: Categorize the latest developments in each domain and discuss the\nfuture directions.\nComputer-based action and generalist agents (GAs) are useful for many tasks. A GA to become truly valuable to its\nusers, it can natural to interact with, and generalize to a broad range of contexts and modalities. We aims to cultivate a\nvibrant research ecosystem and create a shared sense of identity and purpose among the Agent AI community. MAA has\nthe potential to be widely applicable across various contexts and modalities, including input from humans. Therefore,\nwe believe this Agent AI area can engage a diverse range of researchers, fostering a dynamic Agent AI community and\n6", "start_char_idx": 3525, "end_char_idx": 4974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dc18b0a-0d85-40b5-a7cc-123007d0bb74": {"__data__": {"id_": "3dc18b0a-0d85-40b5-a7cc-123007d0bb74", "embedding": null, "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "065f9110-844b-4e66-b977-7d5bafb7e81c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c137413ea132173583dd4d2c8c96e0dda829e901a01196a89dfac25a7173280a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37b7c7e6-02fb-445d-a6cb-354e6c2f46a0", "node_type": "1", "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0b78723aefd4a79902751a3277eeb3112263a9d9632d22775f91f8b4c4e921cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3450a045-2bd4-47fb-aee6-4123b6ed71db", "node_type": "1", "metadata": {}, "hash": "732e251966e4c087c588820acd13c08984f0b05b23cce902bb2df3398593e210", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nshared goals. Led by esteemed experts from academia and industry, we expect that this paper will be an interactive and\nenriching experience, complete with agent instruction, case studies, tasks sessions, and experiments discussion ensuring\na comprehensive and engaging learning experience for all researchers.\nThis paper aims to provide general and comprehensive knowledge about the current research in the field of Agent AI.\nTo this end, the rest of the paper is organized as follows. Section 2 outlines how Agent AI benefits from integrating\nwith related emerging technologies, particularly large foundation models. Section 3 describes a new paradigm and\nframework that we propose for training Agent AI. Section 4 provides an overview of the methodologies that are widely\nused in the training of Agent AI. Section 5 categorizes and discusses various types of agents. Section 6 introduces\nAgent AI applications in gaming, robotics, and healthcare. Section 7 explores the research community\u2019s efforts to\ndevelop a versatile Agent AI, capable of being applied across various modalities, domains, and bridging the sim-to-real\ngap. Section 8 discusses the potential of Agent AI that not only relies on pre-trained foundation models, but also\ncontinuously learns and self-improves by leveraging interactions with the environment and users. Section 9 introduces\nour new datasets that are designed for the training of multimodal Agent AI. Section 11 discusses the hot topic of the\nethics consideration of AI agent, limitations, and societal impact of our paper.\n2 Agent AI Integration\nFoundation models based on LLMs and VLMs, as proposed in previous research, still exhibit limited performance in\nthe area of embodied AI, particularly in terms of understanding, generating, editing, and interacting within unseen\nenvironments or scenarios (Huang et al., 2023a; Zeng et al., 2023). Consequently, these limitations lead to sub-optimal\noutputs from AI agents. Current agent-centric AI modeling approaches focus on directly accessible and clearly defined\ndata (e.g. text or string representations of the world state) and generally use domain and environment-independent\npatterns learned from their large-scale pretraining to predict action outputs for each environment (Xi et al., 2023; Wang\net al., 2023c; Gong et al., 2023a; Wu et al., 2023). In (Huang et al., 2023a), we investigate the task of knowledge-guided\ncollaborative and interactive scene generation by combining large foundation models, and show promising results that\nindicate knowledge-grounded LLM agents can improve the performance of 2D and 3D scene understanding, generation,\nand editing, alongside with other human-agent interactions (Huang et al., 2023a). By integrating an Agent AI framework,\nlarge foundation models are able to more deeply understand user input to form a complex and adaptive HCI system.\nEmergent ability of LLM and VLM works invisible in generative AI, embodied AI, knowledge augmentation for\nmulti-model learning, mix-reality generation, text to vision editing, human interaction for 2D/3D simulation in gaming\nor robotics tasks. Agent AI recent progress in foundation models present an imminent catalyst for unlocking general\nintelligence in embodied agents. The large action models, or agent-vision-language models open new possibilities for\ngeneral-purpose embodied systems such as planning, problem-solving and learning in complex environments. Agent AI\ntest further step in metaverse, and route the early version of AGI.\n2.1 Infinite AI agent\nAI agents have the capacity to interpret, predict, and respond based on its training and input data. While these capabilities\nare advanced and continually improving, it\u2019s important to recognize their limitations and the influence of the underlying\ndata they are trained on. AI agent systems generally possess the following abilities: 1) Predictive Modeling: AI agents\ncan predict likely outcomes or suggest next steps based on historical data and trends. For instance, they might predict\nthe continuation of a text, the answer to a question, the next action for a robot, or the resolution of a scenario. 2)\nDecision Making: In some applications, AI agents can make decisions based on their inferences. Generally, the agent\nwill base their decision on what is most likely to achieve a specified goal. For AI applications like recommendation\nsystems, an agent can decide what products or content to recommend based on its inferences about user preferences. 3)\nHandling Ambiguity: AI agents can often handle ambiguous input by inferring the most likely interpretation based\non context and training.", "start_char_idx": 0, "end_char_idx": 4697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3450a045-2bd4-47fb-aee6-4123b6ed71db": {"__data__": {"id_": "3450a045-2bd4-47fb-aee6-4123b6ed71db", "embedding": null, "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "065f9110-844b-4e66-b977-7d5bafb7e81c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c137413ea132173583dd4d2c8c96e0dda829e901a01196a89dfac25a7173280a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dc18b0a-0d85-40b5-a7cc-123007d0bb74", "node_type": "1", "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "61631dca5ce0f8d083f8a40d769915ff665423088a950e8dc28e91a49a123902", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0f6d2a8-8a83-45fd-b126-4c144aa7b99c", "node_type": "1", "metadata": {}, "hash": "db2a9eafd192869b121febe92f103e291f6117792064efaac54f5c78a9b89956", "class_name": "RelatedNodeInfo"}}, "text": "While these capabilities\nare advanced and continually improving, it\u2019s important to recognize their limitations and the influence of the underlying\ndata they are trained on. AI agent systems generally possess the following abilities: 1) Predictive Modeling: AI agents\ncan predict likely outcomes or suggest next steps based on historical data and trends. For instance, they might predict\nthe continuation of a text, the answer to a question, the next action for a robot, or the resolution of a scenario. 2)\nDecision Making: In some applications, AI agents can make decisions based on their inferences. Generally, the agent\nwill base their decision on what is most likely to achieve a specified goal. For AI applications like recommendation\nsystems, an agent can decide what products or content to recommend based on its inferences about user preferences. 3)\nHandling Ambiguity: AI agents can often handle ambiguous input by inferring the most likely interpretation based\non context and training. However, their ability to do so is limited by the scope of their training data and algorithms.\n4) Continuous Improvement: While some AI agents have the ability to learn from new data and interactions, many\nlarge language models do not continuously update their knowledge-base or internal representation after training. Their\ninferences are usually based solely on the data that was available up to the point of their last training update.\nWe show augmented interactive agents for multi-modality and cross reality-agnostic integration with an emergence\nmechanism in Fig. 2. An AI agent requires collecting extensive training data for every new task, which can be costly or\nimpossible for many domains. In this study, we develop an infinite agent that learns to transfer memory information from\n7", "start_char_idx": 3703, "end_char_idx": 5492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0f6d2a8-8a83-45fd-b126-4c144aa7b99c": {"__data__": {"id_": "f0f6d2a8-8a83-45fd-b126-4c144aa7b99c", "embedding": null, "metadata": {"page_label": "8", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4cd14a75-06c9-41c3-a79c-766096002532", "node_type": "4", "metadata": {"page_label": "8", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "daf8cc871e90c588cfa0178a2733e893afd284eeea3dff8d4d6c93df36a3decf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3450a045-2bd4-47fb-aee6-4123b6ed71db", "node_type": "1", "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "80c74a9fa99fa4367d49e37e04b03da371f2e113ebcc7ec65e2686c1322b3979", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "058624aa-3281-48c3-9007-1aa2e0b1c654", "node_type": "1", "metadata": {}, "hash": "243671cf783184b5146017b9fe7a545e10a84631770820f23948a6a264a88413", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 2: The multi-model agent AI for 2D/3D embodied generation and editing interaction in cross-reality.\ngeneral foundation models (e.g., GPT-X, DALL-E) to novel domains or scenarios for scene understanding, generation,\nand interactive editing in physical or virtual worlds.\nAn application of such an infinite agent in robotics is RoboGen (Wang et al., 2023d). In this study, the authors propose\na pipeline that autonomously run the cycles of task proposition, environment generation, and skill learning. RoboGen is\nan effort to transfer the knowledge embedded in large models to robotics.\n2.2 Agent AI with Large Foundation Models\nRecent studies have indicated that large foundation models play a crucial role in creating data that act as benchmarks\nfor determining the actions of agents within environment-imposed constraints. For example, using foundation models\nfor robotic manipulation (Black et al., 2023; Ko et al., 2023) and navigation (Shah et al., 2023a; Zhou et al., 2023a). To\nillustrate, Black et al. employed an image-editing model as a high-level planner to generate images of future sub-goals,\nthereby guiding low-level policies (Black et al., 2023). For robot navigation, Shah et al. proposed a system that employs\na LLM to identify landmarks from text and a VLM to associate these landmarks with visual inputs, enhancing navigation\nthrough natural language instructions (Shah et al., 2023a).\nThere is also growing interest in the generation of conditioned human motions in response to language and environmental\nfactors. Several AI systems have been proposed to generate motions and actions that are tailored to specific linguistic\ninstructions (Kim et al., 2023; Zhang et al., 2022; Tevet et al., 2022) and to adapt to various 3D scenes (Wang et al.,\n2022a). This body of research emphasizes the growing capabilities of generative models in enhancing the adaptability\nand responsiveness of AI agents across diverse scenarios.\n2.2.1 Hallucinations\nAgents that generate text are often prone to hallucinations, which are instances where the generated text is nonsensical\nor unfaithful to the provided source content (Raunak et al., 2021; Maynez et al., 2020). Hallucinations can be split into\ntwo categories, intrinsic andextrinsic (Ji et al., 2023). Intrinsic hallucinations are hallucinations that are contradictory\n8", "start_char_idx": 0, "end_char_idx": 2408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "058624aa-3281-48c3-9007-1aa2e0b1c654": {"__data__": {"id_": "058624aa-3281-48c3-9007-1aa2e0b1c654", "embedding": null, "metadata": {"page_label": "9", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd7fe1c8-bbbe-47c6-9806-26f3d8fd5d05", "node_type": "4", "metadata": {"page_label": "9", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2ae0f1ba95b5d0c3feb473b365c3c3b57e4027f4178a8109fd6c3caf48d99721", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0f6d2a8-8a83-45fd-b126-4c144aa7b99c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "daf8cc871e90c588cfa0178a2733e893afd284eeea3dff8d4d6c93df36a3decf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66aed68a-5b01-468c-8c0c-79a85e5d3fc6", "node_type": "1", "metadata": {}, "hash": "51884e1adfc81ce994de9eb4248ba9820456d4e6c087261a1df0b4320e247f44", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nto the source material, whereas extrinsic hallucinations are when the generated text contains additional information that\nwas not originally included in the source material.\nSome promising routes for reducing the rate of hallucination in language generation involve using retrieval-augmented\ngeneration (Lewis et al., 2020; Shuster et al., 2021) or other methods for grounding natural language outputs via\nexternal knowledge retrieval (Dziri et al., 2021; Peng et al., 2023). Generally, these methods seek to augment language\ngeneration by retrieving additional source material and by providing mechanisms to check for contradictions between\nthe generated response and the source material.\nWithin the context of multi-modal agent systems, VLMs have been shown to hallucinate as well (Zhou et al., 2023b).\nOne common cause of hallucination for vision-based language-generation is due to the over-reliance on co-occurrence\nof objects and visual cues in the training data (Rohrbach et al., 2018). AI agents that exclusively rely upon pretrained\nLLMs or VLMs and use limited environment-specific finetuning can be particularly vulnerable to hallucinations since\nthey rely upon the internal knowledge-base of the pretrained models for generating actions and may not accurately\nunderstand the dynamics of the world state in which they are deployed.\n2.2.2 Biases and Inclusivity\nAI agents based on LLMs or LMMs (large multimodal models) have biases due to several factors inherent in their\ndesign and training process. When designing these AI agents, we must be mindful of being inclusive and aware of the\nneeds of all end users and stakeholders. In the context of AI agents, inclusivity refers to the measures and principles\nemployed to ensure that the agent\u2019s responses and interactions are inclusive, respectful, and sensitive to a wide range of\nusers from diverse backgrounds. We list key aspects of agent biases and inclusivity below.\n\u2022Training Data : Foundation models are trained on vast amounts of text data collected from the internet,\nincluding books, articles, websites, and other text sources. This data often reflects the biases present in\nhuman society, and the model can inadvertently learn and reproduce these biases. This includes stereotypes,\nprejudices, and slanted viewpoints related to race, gender, ethnicity, religion, and other personal attributes. In\nparticular, by training on internet data and often only English text, models implicitly learn the cultural norms of\nWestern, Educated, Industrialized, Rich, and Democratic (WEIRD) societies (Henrich et al., 2010) who have a\ndisproportionately large internet presence. However, it is essential to recognize that datasets created by humans\ncannot be entirely devoid of bias, since they frequently mirror the societal biases and the predispositions of the\nindividuals who generated and/or compiled the data initially.\n\u2022Historical and Cultural Biases : AI models are trained on large datasets sourced from diverse content. Thus,\nthe training data often includes historical texts or materials from various cultures. In particular, training\ndata from historical sources may contain offensive or derogatory language representing a particular society\u2019s\ncultural norms, attitudes, and prejudices. This can lead to the model perpetuating outdated stereotypes or not\nfully understanding contemporary cultural shifts and nuances.\n\u2022Language and Context Limitations : Language models might struggle with understanding and accurately\nrepresenting nuances in language, such as sarcasm, humor, or cultural references. This can lead to misinterpre-\ntations or biased responses in certain contexts. Furthermore, there are many aspects of spoken language that\nare not captured by pure text data, leading to a potential disconnect between human understanding of language\nand how models understand language.\n\u2022Policies and Guidelines : AI agents operate under strict policies and guidelines to ensure fairness and\ninclusivity. For instance, in generating images, there are rules to diversify depictions of people, avoiding\nstereotypes related to race, gender, and other attributes.\n\u2022Overgeneralization : These models tend to generate responses based on patterns seen in the training data.\nThis can lead to overgeneralizations, where the model might produce responses that seem to stereotype or\nmake broad assumptions about certain groups.\n\u2022Constant Monitoring and Updating : AI systems are continuously monitored and updated to address any\nemerging biases or inclusivity issues. Feedback from users and ongoing research in AI ethics play a crucial\nrole in this process.\n9", "start_char_idx": 0, "end_char_idx": 4688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66aed68a-5b01-468c-8c0c-79a85e5d3fc6": {"__data__": {"id_": "66aed68a-5b01-468c-8c0c-79a85e5d3fc6", "embedding": null, "metadata": {"page_label": "10", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc08dc10-0ffc-43bd-a1a2-b416435300ae", "node_type": "4", "metadata": {"page_label": "10", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "44480c1fb2ed877be145d9c6f29c58b9e1ae04887f05e048529854b21f5f11d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058624aa-3281-48c3-9007-1aa2e0b1c654", "node_type": "1", "metadata": {"page_label": "9", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2ae0f1ba95b5d0c3feb473b365c3c3b57e4027f4178a8109fd6c3caf48d99721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a474ebd-2604-4648-bf82-3b5260e97a56", "node_type": "1", "metadata": {}, "hash": "bba011239efe74aa694422638da3654a7fd40ab1b270174677a9866e3ba1c216", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n\u2022Amplification of Dominant Views : Since the training data often includes more content from dominant\ncultures or groups, the model may be more biased towards these perspectives, potentially underrepresenting or\nmisrepresenting minority viewpoints.\n\u2022Ethical and Inclusive Design : AI tools should be designed with ethical considerations and inclusivity as core\nprinciples. This includes respecting cultural differences, promoting diversity, and ensuring that the AI does not\nperpetuate harmful stereotypes.\n\u2022User Guidelines : Users are also guided on how to interact with AI in a manner that promotes inclusivity and\nrespect. This includes refraining from requests that could lead to biased or inappropriate outputs. Furthermore,\nit can help mitigate models learning harmful material from user interactions.\nDespite these measures, AI agents still exhibit biases. Ongoing efforts in agent AI research and development are focused\non further reducing these biases and enhancing the inclusivity and fairness of agent AI systems. Efforts to Mitigate\nBiases:\n\u2022Diverse and Inclusive Training Data : Efforts are made to include a more diverse and inclusive range of\nsources in the training data.\n\u2022Bias Detection and Correction : Ongoing research focuses on detecting and correcting biases in model\nresponses.\n\u2022Ethical Guidelines and Policies : Models are often governed by ethical guidelines and policies designed to\nmitigate biases and ensure respectful and inclusive interactions.\n\u2022Diverse Representation: Ensuring that the content generated or the responses provided by the AI agent\nrepresent a wide range of human experiences, cultures, ethnicities, and identities. This is particularly relevant\nin scenarios like image generation or narrative construction.\n\u2022Bias Mitigation : Actively working to reduce biases in the AI\u2019s responses. This includes biases related to race,\ngender, age, disability, sexual orientation, and other personal characteristics. The goal is to provide fair and\nbalanced responses that do not perpetuate stereotypes or prejudices.\n\u2022Cultural Sensitivity : The AI is designed to be culturally sensitive, acknowledging and respecting the diversity\nof cultural norms, practices, and values. This includes understanding and appropriately responding to cultural\nreferences and nuances.\n\u2022Accessibility : Ensuring that the AI agent is accessible to users with different abilities, including those with\ndisabilities. This can involve incorporating features that make interactions easier for people with visual,\nauditory, motor, or cognitive impairments.\n\u2022Language-based Inclusivity : Providing support for multiple languages and dialects to cater to a global user\nbase, and being sensitive to the nuances and variations within a language (Liu et al., 2023b).\n\u2022Ethical and Respectful Interactions : The Agent is programmed to interact ethically and respectfully with all\nusers, avoiding responses that could be deemed offensive, harmful, or disrespectful.\n\u2022User Feedback and Adaptation : Incorporating user feedback to continually improve the inclusivity and\neffectiveness of the AI agent. This includes learning from interactions to better understand and serve a diverse\nuser base.\n\u2022Compliance with Inclusivity Guidelines : Adhering to established guidelines and standards for inclusivity in\nAI agent, which are often set by industry groups, ethical boards, or regulatory bodies.\nDespite these efforts, it\u2019s important to be aware of the potential for biases in responses and to interpret them with critical\nthinking. Continuous improvements in AI agent technology and ethical practices aim to reduce these biases over time.\nOne of the overarching goals for inclusivity in agent AI is to create an agent that is respectful and accessible to all users,\nregardless of their background or identity.\n2.2.3 Data Privacy and Usage\nOne key ethical consideration of AI agents involves comprehending how these systems handle, store, and potentially\nretrieve user data. We discuss key aspects below:\n10", "start_char_idx": 0, "end_char_idx": 4072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a474ebd-2604-4648-bf82-3b5260e97a56": {"__data__": {"id_": "0a474ebd-2604-4648-bf82-3b5260e97a56", "embedding": null, "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78ac21d3-d4e2-432f-9d8f-6d5454633ddf", "node_type": "4", "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "504e5ab5e5972e45b7b91f8478231af8c3cf32bcf31610f043aa7bd01458a2d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66aed68a-5b01-468c-8c0c-79a85e5d3fc6", "node_type": "1", "metadata": {"page_label": "10", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "44480c1fb2ed877be145d9c6f29c58b9e1ae04887f05e048529854b21f5f11d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc4ba41a-d951-4293-9239-3a4140235c12", "node_type": "1", "metadata": {}, "hash": "74e3ca10e530117c083b4ab6d9245cf551c3826690e9b5cf0a89967a2fe89a53", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nData Collection, Usage and Purpose. When using user data to improve model performance, model developers\naccess the data the AI agent has collected while in production and interacting with users. Some systems allow users\nto view their data through user accounts or by making a request to the service provider. It is important to recognize\nwhat data the AI agent collects during these interactions. This could include text inputs, user usage patterns, personal\npreferences, and sometimes more sensitive personal information. Users should also understand how the data collected\nfrom their interactions is used. If, for some reason, the AI holds incorrect information about a particular person or\ngroup, there should be a mechanism for users to help correct this once identified. This is important for both accuracy\nand to be respectful of all users and groups. Common uses for retrieving and analyzing user data include improving\nuser interaction, personalizing responses, and system optimization. It is extremely important for developers to ensure\nthe data is not used for purposes that users have not consented to, such as unsolicited marketing.\nStorage and Security. Developers should know where the user interaction data is stored and what security measures\nare in place to protect it from unauthorized access or breaches. This includes encryption, secure servers, and data\nprotection protocols. It is extremely important to determine if agent data is shared with third parties and under what\nconditions. This should be transparent and typically requires user consent.\nData Deletion and Retention. It is also important for users to understand how long user data is stored and how users\ncan request its deletion. Many data protection laws give users the right to be forgotten, meaning they can request their\ndata be erased. AI agents must adhere to data protection laws like GDPR in the EU or CCPA in California. These laws\ngovern data handling practices and user rights regarding their personal data.\nData Portability and Privacy Policy. Furthermore, developers must create the AI agent\u2019s privacy policy to document\nand explain to users how their data is handled. This should detail data collection, usage, storage, and user rights.\nDevelopers should ensure that they obtain user consent for data collection, especially for sensitive information. Users\ntypically have the option to opt-out or limit the data they provide. In some jurisdictions, users may even have the right\nto request a copy of their data in a format that can be transferred to another service provider.\nAnonymization. For data used in broader analysis or AI training, it should ideally be anonymized to protect individual\nidentities. Developers must understand how their AI agent retrieves and uses historical user data during interactions.\nThis could be for personalization or improving response relevance.\nIn summary, understanding data privacy for AI agents involves being aware of how user data is collected, used, stored,\nand protected, and ensuring that users understand their rights regarding accessing, correcting, and deleting their data.\nAwareness of the mechanisms for data retrieval, both by users and the AI agent, is also crucial for a comprehensive\nunderstanding of data privacy.\n2.2.4 Interpretability and Explainability\nImitation Learning \u2192Decoupling. Agents are typically trained using a continuous feedback loop in Reinforcement\nLearning (RL) or Imitation Learning (IL), starting with a randomly initialized policy. However, this approach faces\nleader-board in obtaining initial rewards in unfamiliar environments, particularly when rewards are sparse or only\navailable at the end of a long-step interaction. Thus, a superior solution is to use an infinite-memory agent trained\nthrough IL, which can learn policies from expert data, improving exploration and utilization of unseen environmental\nspace with emergent infrastructure as shown in Fig. 3. With expert characteristics to help the agent explore better and\nutilize the unseen environmental space. Agent AI, can learn policies and new paradigm flow directly from expert data.\nTraditional IL has an agent mimicking an expert demonstrator\u2019s behavior to learn a policy. However, learning the expert\npolicy directly may not always be the best approach, as the agent may not generalize well to unseen situations. To\ntackle this, we propose learning an agent with in-context prompt or a implicit reward function that captures key aspects\nof the expert\u2019s behavior, as shown in Fig. 3. This equips the infinite memory agent with physical-world behavior data\nfor task execution, learned from expert demonstrations. It helps overcome existing imitation learning drawbacks like\nthe need for extensive expert data and potential errors in complex tasks.", "start_char_idx": 0, "end_char_idx": 4857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc4ba41a-d951-4293-9239-3a4140235c12": {"__data__": {"id_": "cc4ba41a-d951-4293-9239-3a4140235c12", "embedding": null, "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78ac21d3-d4e2-432f-9d8f-6d5454633ddf", "node_type": "4", "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "504e5ab5e5972e45b7b91f8478231af8c3cf32bcf31610f043aa7bd01458a2d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a474ebd-2604-4648-bf82-3b5260e97a56", "node_type": "1", "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "51acb918f5f572da82e969b1a37614aa67ffcd38457d242a0777f8044908bd3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ff778eb-45ae-419e-998d-7694c8ca980d", "node_type": "1", "metadata": {}, "hash": "6c3bab30fab091b57c00e65e9b3d4c4fb52db51cf768a7fc81e143d05d03468e", "class_name": "RelatedNodeInfo"}}, "text": "3. With expert characteristics to help the agent explore better and\nutilize the unseen environmental space. Agent AI, can learn policies and new paradigm flow directly from expert data.\nTraditional IL has an agent mimicking an expert demonstrator\u2019s behavior to learn a policy. However, learning the expert\npolicy directly may not always be the best approach, as the agent may not generalize well to unseen situations. To\ntackle this, we propose learning an agent with in-context prompt or a implicit reward function that captures key aspects\nof the expert\u2019s behavior, as shown in Fig. 3. This equips the infinite memory agent with physical-world behavior data\nfor task execution, learned from expert demonstrations. It helps overcome existing imitation learning drawbacks like\nthe need for extensive expert data and potential errors in complex tasks. The key idea behind the Agent AI has two\nparts: 1) the infinite agent that collects physical-world expert demonstrations as state-action pairs and 2) the virtual\nenvironment that imitates the agent generator. The imitating agent produces actions that mimic the expert\u2019s behavior,\nwhile the agent learns a policy mapping from states to actions by reducing a loss function of the disparity between the\nexpert\u2019s actions and the actions generated by the learned policy.\n11", "start_char_idx": 4007, "end_char_idx": 5326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ff778eb-45ae-419e-998d-7694c8ca980d": {"__data__": {"id_": "7ff778eb-45ae-419e-998d-7694c8ca980d", "embedding": null, "metadata": {"page_label": "12", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3613fd4e-e733-44ba-ba02-a699034694d7", "node_type": "4", "metadata": {"page_label": "12", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a7186bf97917d7698c8d39f0a13dce85c19db341806028f6550af55c83ce2d6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc4ba41a-d951-4293-9239-3a4140235c12", "node_type": "1", "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ff924a2428a3f3545fc7bfcd9d84d5504b7b9be6fb476126ead1e8898105ff96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71d6ff1b-be09-471d-9421-190980a063e8", "node_type": "1", "metadata": {}, "hash": "eb03e77aaa904ca8a0ab0eabefc66e8fec93abd7555e251a845cef78d5f08ec4", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nDecoupling \u2192Generalization. Rather than relying on a task-specific reward function, the agent learns from\nexpert demonstrations, which provide a diverse set of state-action pairs covering various task aspects. The agent\nthen learns a policy that maps states to actions by imitating the expert\u2019s behavior. Decoupling in imitation learning\nrefers to separating the learning process from the task-specific reward function, allowing the policy to generalize\nacross different tasks without explicit reliance on the task-specific reward function. By decoupling, the agent can\nlearn from expert demonstrations and learn a policy that is adaptable to a variety of situations. Decoupling enables\ntransfer learning, where a policy learned in one domain can adapt to others with minimal fine-tuning. By learning\na general policy that is not tied to a specific reward function, the agent can leverage the knowledge it acquired in\none task to perform well in other related tasks. Since the agent does not rely on a specific reward function, it can\nadapt to changes in the reward function or environment without the need for significant retraining. This makes the\nlearned policy more robust and generalizable across different environments. Decoupling in this context refers to\nthe separation of two tasks in the learning process: learning the reward function and learning the optimal policy.\nFigure 3: Example of the Emergent Interactive Mechanism\nusing an agent to identify text relevant to the image from\ncandidates. The task involves using a multi-modal AI agent\nfrom the web and human-annotated knowledge interaction\nsamples to incorporate external world information.Generalization \u2192Emergent Behavior. Generaliza-\ntion explains how emergent properties or behaviors\ncan arise from simpler components or rules. The key\nidea lies in identifying the basic elements or rules that\ngovern the behavior of the system, such as individual\nneurons or basic algorithms. Consequently, by observ-\ning how these simple components or rules interact with\none another. These interactions of these components of-\nten lead to the emergence of complex behaviors, which\nare not predictable by examining individual compo-\nnents alone. Generalization across different levels of\ncomplexity allows a system to learn general princi-\nples applicable across these levels, leading to emergent\nproperties. This enables the system to adapt to new\nsituations, demonstrating the emergence of more com-\nplex behaviors from simpler rules. Furthermore, the\nability to generalize across different complexity levels\nfacilitates knowledge transfer from one domain to an-\nother, which contributes to the emergence of complex\nbehaviors in new contexts as the system adapts.\n2.2.5 Inference Augmentation\nThe inference ability of an AI agent lies in its capacity to interpret, predict, and respond based on its training and input\ndata. While these capabilities are advanced and continually improving, it\u2019s important to recognize their limitations and\nthe influence of the underlying data they are trained on. Particularly, in the context of large language models, it refers to\nits capacity to draw conclusions, make predictions, and generate responses based on the data it has been trained on and\nthe input it receives. Inference augmentation in AI agents refers to enhancing the AI\u2019s natural inference abilities with\nadditional tools, techniques, or data to improve its performance, accuracy, and utility. This can be particularly important\nin complex decision-making scenarios or when dealing with nuanced or specialized content. We denote particularly\nimportant sources for inference augmentation below:\nData Enrichment. Incorporating additional, often external, data sources to provide more context or background\ncan help the AI agent make more informed inferences, especially in areas where its training data may be limited. For\nexample, AI agents can infer meaning from the context of a conversation or text. They analyze the given information\nand use it to understand the intent and relevant details of user queries. These models are proficient at recognizing\npatterns in data. They use this ability to make inferences about language, user behavior, or other relevant phenomena\nbased on the patterns they\u2019ve learned during training.\n12", "start_char_idx": 0, "end_char_idx": 4359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71d6ff1b-be09-471d-9421-190980a063e8": {"__data__": {"id_": "71d6ff1b-be09-471d-9421-190980a063e8", "embedding": null, "metadata": {"page_label": "13", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "739862a7-883d-4fc3-ac4f-9ea7e5658add", "node_type": "4", "metadata": {"page_label": "13", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "34b93644802fc5db37a88fe91e6a35fe7754abf494a88f4078c7d17beae59cce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ff778eb-45ae-419e-998d-7694c8ca980d", "node_type": "1", "metadata": {"page_label": "12", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a7186bf97917d7698c8d39f0a13dce85c19db341806028f6550af55c83ce2d6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc9dfe85-1466-42a3-9d49-cdad7b633955", "node_type": "1", "metadata": {}, "hash": "699de54e8e2bdc6942f5c678276aa450375c3647a1fca852e4bf12e16f8ab5b2", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nAlgorithm Enhancement. Improving the AI\u2019s underlying algorithms to make better inferences. This could involve\nusing more advanced machine learning models, integrating different types of AI (like combining NLP with image\nrecognition), or updating algorithms to better handle complex tasks. Inference in language models involves understand-\ning and generating human language. This includes grasping nuances like tone, intent, and the subtleties of different\nlinguistic constructions.\nHuman-in-the-Loop (HITL). Involving human input to augment the AI\u2019s inferences can be particularly useful in\nareas where human judgment is crucial, such as ethical considerations, creative tasks, or ambiguous scenarios. Humans\ncan provide guidance, correct errors, or offer insights that the agent would not be able to infer on its own.\nReal-Time Feedback Integration. Using real-time feedback from users or the environment to enhance inferences\nis another promising method for improving performance during inference. For example, an AI might adjust its\nrecommendations based on live user responses or changing conditions in a dynamic system. Or, if the agent is taking\nactions in a simulated environment that break certain rules, the agent can be dynamically given feedback to help correct\nitself.\nCross-Domain Knowledge Transfer. Leveraging knowledge or models from one domain to improve inferences in\nanother can be particularly helpful when producing outputs within a specialized discipline. For instance, techniques\ndeveloped for language translation might be applied to code generation, or insights from medical diagnostics could\nenhance predictive maintenance in machinery.\nCustomization for Specific Use Cases. Tailoring the AI\u2019s inference capabilities for particular applications or\nindustries can involve training the AI on specialized datasets or fine-tuning its models to better suit specific tasks,\nsuch as legal analysis, medical diagnosis, or financial forecasting. Since the particular language or information within\none domain can greatly contrast with the language from other domains, it can be beneficial to finetune the agent on\ndomain-specific information.\nEthical and Bias Considerations. It is important to ensure that the augmentation process does not introduce new\nbiases or ethical issues. This involves careful consideration of the sources of additional data or the impact of the\nnew inference augmentation algorithms on fairness and transparency. When making inferences, especially about\nsensitive topics, AI agents must sometimes navigate ethical considerations. This involves avoiding harmful stereotypes,\nrespecting privacy, and ensuring fairness.\nContinuous Learning and Adaptation. Regularly updating and refining the AI\u2019s capabilities to keep up with new\ndevelopments, changing data landscapes, and evolving user needs.\nIn summmary, winference augmentation in AI agents involves methods in which their natural inference abilities can be\nenhanced through additional data, improved algorithms, human input, and other techniques. Depending on the use-case,\nthis augmentation is often essential for dealing with complex tasks and ensuring accuracy in the agent\u2019s outputs.\n2.2.6 Regulation\nRecently, Agent AI has made significant advancements, and its integration into embodied systems has opened new\npossibilities for interacting with agents via more immersive, dynamic, and engaging experiences. To expedite the\nprocess and ease the cumbersome work in agent AI developing, we are proposing to develop the next-generation\nAI-empowered pipeline for agent interaction. Develop a human-machine collaboration system where humans and\nmachines can communicate and interact meaningfully. The system can leverage the LLM\u2019s or VLM dialog capabilities\nand vast action to talk with human players and identify human needs. Then it will perform proper actions to help human\nplayers upon request.\nWhen employing LLM/VLMs for a human-machine collaboration system, it is essential to note that these operate as\nblack boxes, generating unpredictable output. This uncertainty can become crucial in a physical setup, such as operating\nactual robotics. An approach to address this challenge is constraining the focus of the LLM/VLM through prompt\nengineering. For instance, in robotic task planning from instructions, providing environmental information within the\nprompt has been reported to yield more stable outputs than relying solely on text (Gramopadhye and Szafir, 2022). This\n13", "start_char_idx": 0, "end_char_idx": 4549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc9dfe85-1466-42a3-9d49-cdad7b633955": {"__data__": {"id_": "bc9dfe85-1466-42a3-9d49-cdad7b633955", "embedding": null, "metadata": {"page_label": "14", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72b39cf4-4bff-4439-aaeb-59ae990f2130", "node_type": "4", "metadata": {"page_label": "14", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "d5654504313bbdc71bc3f2d076f0ffd73672c9a7b2404e140f570c1c82745170", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71d6ff1b-be09-471d-9421-190980a063e8", "node_type": "1", "metadata": {"page_label": "13", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "34b93644802fc5db37a88fe91e6a35fe7754abf494a88f4078c7d17beae59cce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cb0e16d-55e4-416e-94c4-b45d05a3516f", "node_type": "1", "metadata": {}, "hash": "0982713f71af1880eabe055d64af380aa7814f85a23be5b0e98ea7616495353d", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 4: A robot teaching system developed in (Wake et al., 2023c). (Left) The system workflow. The process involves\nthree steps: Task planning, where ChatGPT plans robotic tasks from instructions and environmental information;\nDemonstration, where the user visually demonstrates the action sequence. All the steps are reviewed by the user, and if\nany step fails or shows deficiencies, the previous steps can be revisited as necessary. (Right) A web application that\nenables uploading of demonstration data and the interaction between the user and ChatGPT.\nreport is supported by the Minsky\u2019s frame theory of AI (Minsky, 1975), suggesting that the problem space to be solved\nby LLM/VLMs is defined by the given prompts. Another approach is designing prompts to make LLM/VLMs include\nexplanatory text to allow users understand what the model has focused on or recognized. Additionally, implementing\na higher layer that allows for pre-execution verification and modification under human guidance can facilitate the\noperation of systems working under such guidance (Fig. 4).\n2.3 Agent AI for Emergent Abilities\nDespite the growing adoption of interactive agent AI systems, the majority of proposed methods still face a challenge in\nterms of their generalization performance in unseen environments or scenarios. Current modeling practices require\ndevelopers to prepare large datasets for each domain to finetune/pretrain models; however, this process is costly and even\nimpossible if the domain is new. To address this issue, we build interactive agents that leverage the knowledge-memory\nof general-purpose foundation models (ChatGPT, Dall-E, GPT-4, etc.) for a novel scenario, specifically for generating\na collaboration space between humans and agents. We discover an emergent mechanism\u2014 which we name Mixed\nReality with Knowledge Inference Interaction\u2014that facilitates collaboration with humans to solve challenging tasks in\ncomplex real-world environments and enables the exploration of unseen environments for adaptation to virtual reality.\nFor this mechanism, the agent learns i) micro-reactions in cross-modality: collecting relevant individual knowledge\nfor each interaction task (e.g., understanding unseen scenes) from the explicit web source and by implicitly inferring\nfrom the output of pretrained models; ii) macro-behavior in reality-agnostic: improving interactive dimensions and\npatterns in language and multi-modality domains, and make changes based on characterized roles, certain target\nvariable, influenced diversification of collaborative information in mixed-reality and LLMs. We investigate the task\nof knowledge-guided interactive synergistic effects to collaborated scene generation with combining various OpenAI\nmodels, and show promising results of how the interactive agent system can further boost the large foundation models in\n14", "start_char_idx": 0, "end_char_idx": 2930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cb0e16d-55e4-416e-94c4-b45d05a3516f": {"__data__": {"id_": "1cb0e16d-55e4-416e-94c4-b45d05a3516f", "embedding": null, "metadata": {"page_label": "15", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1e59be4-bdc5-4ed4-9f0c-e6c4b12e308f", "node_type": "4", "metadata": {"page_label": "15", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2f7728187a3eba03b98f639c7ba1ecf624207ae670d2dbdc7ad2dbd179ea71c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc9dfe85-1466-42a3-9d49-cdad7b633955", "node_type": "1", "metadata": {"page_label": "14", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "d5654504313bbdc71bc3f2d076f0ffd73672c9a7b2404e140f570c1c82745170", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "427dcd85-6f64-4975-8d5c-f1d74833b537", "node_type": "1", "metadata": {}, "hash": "86fa5b1b39bc8ec7a8e161cbba111ae9286a9feb5ffd8670ea4209c143be535c", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 5: Our proposed new agent paradigm for a multi-modal generalist agent. There are 5 main modules as shown in\nthe figures: 1) Environment and Perception with task-planning and skill observation; 2) Agent learning; 3) Memory; 4)\nAgent action; 5) Cognition.\nour setting. It integrates and improves the depth of generalization, conscious and interpretability of a complex adaptive\nAI systems.\n3 Agent AI Paradigm\nIn this section, we discuss a new paradigm and framework for training Agent AI. We seek to accomplish several goals\nwith our proposed framework:\n\u2022Make use of existing pre-trained models and pre-training strategies to effectively bootstrap our agents with\neffective understanding of important modalities, such as text or visual inputs.\n\u2022 Support for sufficient long-term task-planning capabilities.\n\u2022 Incorporate a framework for memory that allows for learned knowledge to be encoded and retrieved later.\n\u2022 Allow for environmental feedback to be used to effectively train the agent to learn which actions to take.\nWe show a high-level new agent diagram outlining the important submodules of such a system in Fig. 5.\n3.1 LLMs and VLMs\nWe can use the LLM or VLM model to bootstrap the components of the Agent as showed in Fig. 5. In particular, LLMs\nhave been shown to perform well for task-planning (Gong et al., 2023a), contain significant world knowledge (Yu et al.,\n2023b), and display impressive logical reasoning capabilities (Creswell et al., 2022). Additionally, VLMs such as CLIP\n(Radford et al., 2021) provide a general visual encoder that is language-aligned, as well as providing zero-shot visual\nrecognition capabilities. For example, state-of-the-art open-source multi-modal models such as LLaV A (Liu et al.,\n2023c) and InstructBLIP (Dai et al., 2023) rely upon frozen CLIP models as visual encoders.\n3.2 Agent Transformer Definition\nInstead of using frozen LLMs and VLMs for the AI agent, it is also possible to use a single-agent transformer model\nthat takes visual tokens and language tokens as input, similar to Gato (Reed et al., 2022). In addition to vision and\nlanguage, we add a third general type of input, which we denote as agent tokens. Conceptually, agent tokens are\nused to reserve a specific subspace of the input and output space of the model for agentic behaviors. For robotics or\ngame playing, this may be represented as the input action space of the controller. When training agents to use specific\ntools, such as image-generation or image-editing models, or for other API calls, agent tokens can also be used. As\nshowed in Fig. 7, we can combine the agent tokens with visual and language tokens to generate a unified interface for\ntraining multi-modal agent AI. Compared to using large, proprietary LLMs as agents, there are several advantages\nto using an agent transformer. Firstly, the model can be easily customized to very specific agentic tasks that may be\ndifficult to represent in natural language (e.g. controller inputs or other specific actions). Thus, the agent can learn from\nenvironmental interactions and domain-specific data to improve performance. Secondly, it can be easier to understand\nwhy the model does or does not take specific actions by having access to the probabilities of the agent tokens. Thirdly,\n15", "start_char_idx": 0, "end_char_idx": 3346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "427dcd85-6f64-4975-8d5c-f1d74833b537": {"__data__": {"id_": "427dcd85-6f64-4975-8d5c-f1d74833b537", "embedding": null, "metadata": {"page_label": "16", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48d8965c-c3d3-4b0b-86ca-79e67ca0d5fd", "node_type": "4", "metadata": {"page_label": "16", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "debf1084f672b794571b42328e5025faf0a854c5508cc43c50f7342acf397ff3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cb0e16d-55e4-416e-94c4-b45d05a3516f", "node_type": "1", "metadata": {"page_label": "15", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2f7728187a3eba03b98f639c7ba1ecf624207ae670d2dbdc7ad2dbd179ea71c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58ac2fd7-4809-49ff-85c8-d1e42efae233", "node_type": "1", "metadata": {}, "hash": "624e2b4bd8b25119bb530debf2c78a4dedcdeee3808c817d54bd5aeb813d47a1", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 6: We show the current paradigm for creating multi-modal AI agents by incorporating a Large Language Model\n(LLM) with a Large Vision Model (LVM). Generally, these models take visual or language inputs and use pre-trained\nand frozen visual and language models, learning smaller sub-network that connect and bridge modalities. Examples\ninclude Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023c), InstructBLIP (Dai et al., 2023), and LLaV A (Liu\net al., 2023c).\nFigure 7: The unified agent multi-modal transformer model. Instead of connecting frozen submodules and using existing\nfoundation models as building blocks, we propose a unified and end-to-end training paradigm for agent systems. We\ncan still initialize the submodules with LLMs and LVMs as in Figure 6 but also make use of agent tokens, specialized\ntokens for training the model to perform agentic behaviors in a specific domain (e.g., robotics). For more details about\nagent tokens, see Section 3.2\nthere are certain domains such as healthcare and law that have strict data privacy requirements. Finally, a relatively\nsmaller agent transformer can potentially be significantly cheaper than a larger proprietary language model.\n3.3 Agent Transformer Creation\nAs shown above in Fig. 5, we can use the new agent paradigm with LLM and VLM-bootstrapped agents, as well as\nleveraging data generated from large foundation models to train the agent transformer model for learning to execute\nspecific goals. Within this process, the agent model is trained to be specialized and tailored for specific tasks and\ndomains. This approach allows you to leverage a pre-existing, foundation model\u2019s learned features and knowledge. We\nshow a simplified overview of the process in two steps below:\nDefine Objectives within the Domain. In order to train the agent transformer, the objectives and the action-space\nof the agent within the context of each specific environment needs to be clearly defined. This includes determining\nwhich specific tasks or actions the agent needs to perform and assigning unique agent tokens for each. Furthermore, any\nautomatic rules or procedures that can be used to identify successful completion of tasks can significantly improve the\namount of data available for training. Otherwise, foundation-model generated or human-annotated data will be required\n16", "start_char_idx": 0, "end_char_idx": 2415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58ac2fd7-4809-49ff-85c8-d1e42efae233": {"__data__": {"id_": "58ac2fd7-4809-49ff-85c8-d1e42efae233", "embedding": null, "metadata": {"page_label": "17", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfe73f08-25c6-4f70-93b2-b3b3104776e4", "node_type": "4", "metadata": {"page_label": "17", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "d9d377ec700fefae70033dda60a2b328f47b5488b3ce44874da56860352f84f7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "427dcd85-6f64-4975-8d5c-f1d74833b537", "node_type": "1", "metadata": {"page_label": "16", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "debf1084f672b794571b42328e5025faf0a854c5508cc43c50f7342acf397ff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c1d2e9f-6e08-49a0-ab8c-0c5854191b6f", "node_type": "1", "metadata": {}, "hash": "0a5d6572953f075b4d92a1ba77d20ffbb8d4121200d13585059b75ad53ab33a3", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nfor training the model. After the data is collected and it is possible to evaluate the performance of the agent, the process\nof continuous improvement can begin.\nContinuous Improvement. Continuous monitoring of the model\u2019s performance and collection of feedback are\nessential steps in the process. Feedback should be used for further fine-tuning and updates. It is also crucial to ensure\nthat the model does not perpetuate biases or unethical outcomes. This necessitates a careful examination of the training\ndata, regular checks for biases in outputs, and, if needed, training the model to recognize and avoid biases. Once the\nmodel achieves satisfactory performance, it can be deployed for the intended application. Continuous monitoring\nremains vital to ensure that the model performs as expected and to facilitate necessary adjustments. More details on this\nprocess, sources of training data, and details surrounding continous learning for agent AI can be found in Section 8.\n4 Agent AI Learning\n4.1 Strategy and Mechanism\nThe strategy of interactive AI on different domains which extends the paradigm of calling large foundation models\nwith a trained agent that actively seeks to collect user feedback, action information, useful knowledge for generation\nand interaction. Some times, the LLM/VLM models are not need to trained again, and we improve their performance\nby providing improved contextual prompts at test time for an agent. On the other hand, it always involves a knowl-\nedge/reasoning/commonsense/inference interactive modeling through a combination of triple systems - one performing\nknowledge retrieval from multi-model query, second performing interactive generation from the relevant agent, and\nlast one the trained a new, informative self-supervised training or pre-training with reinforcement learning or imitation\nlearning with improved way.\n4.1.1 Reinforcement Learning (RL)\nThere is a rich history of leveraging reinforcement learning (RL) to train interactive agents that exhibits intelligent\nbehaviors. RL is a methodology to learn the optimal relationship between states and actions based on rewards (or\npenalties) received as a result of its actions. RL is a highly scalable framework that has been applied to numerous\napplications including robotics, however, it generally faces several leader-board and LLM/VLMs have shown their\npotential to mitigate or overcome some of those difficulties:\n\u2022Reward designing The efficiency of policy learning greatly depends on the design of the reward function.\nDesigning the reward function requires not only knowledge of RL algorithms but also a deep understanding of\nthe nature of the task, and thus often necessitates crafting the function based on expert experience. Several\nstudies explored the use of LLM/VLMs for designing reward functions (Yu et al., 2023a; Katara et al., 2023;\nMa et al., 2023).\n\u2022Data collection and efficiency Given its exploratory nature, RL-based policy learning requires a significant\namount of data (Padalkar et al., 2023). The necessity for extensive data becomes particularly evident when\nthe policy involves managing long sequences or integrating complex actions. This is because these scenarios\ndemand more nuanced decision-making and learning from a wider range of situations. In recent studies, efforts\nhave been directed towards enhancing data generation to support policy learning (Kumar et al., 2023; Du et al.,\n2023). Additionally, in some studies, these models have been integrated into the reward function to improve\npolicy learning (Sontakke et al., 2023). Parallel to these developments, another strand of research has focused\non achieving parameter efficiency in learning processes using VLMs (Tang et al., 2023; Li et al., 2023d) and\nLLMs (Shi et al., 2023)\n\u2022Long-horizon steps In relation to the issue of data efficiency, RL becomes more challenging as the length\nof action sequences increases. This is due to the ambiguity in the relationship between actions and rewards,\nknown as the credit assignment problem, and the increase in the number of states to be explored, necessitating\na significant amount of time and data. One typical approach for long and complex tasks is to break them down\ninto a sequence of subgoals and apply pretrained policies to solve each subgoal (e.g., (Takamatsu et al., 2022)).\nThis idea falls within the framework called the task and motion planning (TAMP)(Garrett et al., 2021). TAMP\nis composed of two primary components: task planning, which entails identifying sequences of high-level\n17", "start_char_idx": 0, "end_char_idx": 4608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c1d2e9f-6e08-49a0-ab8c-0c5854191b6f": {"__data__": {"id_": "1c1d2e9f-6e08-49a0-ab8c-0c5854191b6f", "embedding": null, "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b20f27f-24ca-47d6-ba95-591b509dbb83", "node_type": "4", "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "fa74459c7099d7e77d8ece830316649d2ae50b1baa59743c9f2d85015b3dd16a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58ac2fd7-4809-49ff-85c8-d1e42efae233", "node_type": "1", "metadata": {"page_label": "17", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "d9d377ec700fefae70033dda60a2b328f47b5488b3ce44874da56860352f84f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13b2453f-f279-4d4e-bf74-a051e1e955f6", "node_type": "1", "metadata": {}, "hash": "2610ffa6b61074138891700f21ba763d1c1fb40cec6fed7b9a25877fcd12bae7", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nactions, and motion planning, which involves finding physically consistent, collision-free trajectories to achieve\nthe objectives of the task plan.\nLLMs are well-suited to TAMP, and recent research has often adopted an approach where LLMs are used to\nexecute high-level task planning, while low-level controls are addressed with RL-based policies (Xu et al.,\n2023; Sun et al., 2023a; Li et al., 2023b; Parakh et al., 2023). The advanced capabilities of LLMs enable them\nto effectively decompose even abstract instructions into subgoals (Wake et al., 2023c), contributing to the\nenhancement of language understanding abilities in robotic systems.\n4.1.2 Imitation Learning (IL)\nWhile RL aims to train a policy based on exploratory behavior and maximizing rewards through interactions with the\nenvironment, imitation learning (IL) seeks to leverage expert data to mimic the actions of experienced agents or experts.\nFor example, in robotics, one of the major frameworks based on IL is Behavioral Cloning (BC). BC is an approach\nwhere a robot is trained to mimic the actions of an expert by directly copying them. In this approach, the expert\u2019s actions\nin performing specific tasks are recorded, and the robot is trained to replicate these actions in similar situations. Recent\nBC-based methods often incorporate technologies from LLM/VLMs, enabling more advanced end-to-end models. For\nexample, Brohan et al. proposed RT-1 (Brohan et al., 2022) and RT-2 (Brohan et al., 2023), transformer-based models\nthat output an action sequence for the base and arm, taking a series of images and language as input. These models are\nreported to show high generalization performance as the result of training on a large amount of training data.\n4.1.3 Traditional RGB\nLearning intelligent agent behavior leveraging image inputs has been of interest for many years (Mnih et al., 2015). The\ninherent challenge of using RGB input is the curse of dimensionality. To solve this problem, researchers either use\nmore data (Jang et al., 2022; Ha et al., 2023) or introduce inductive biases into the model design to improve sample\nefficiency. In particular, authors incorporate 3D structures into the model architecture for manipulations (Zeng et al.,\n2021; Shridhar et al., 2023; Goyal et al., 2023; James and Davison, 2022). For robot navigation, authors (Chaplot et al.,\n2020a,b) leverage maps as a representation. Maps can either be learned from a neural network aggregating all previous\nRGB inputs or through 3D reconstruction methods such as Neural Radiance Fields (Rosinol et al., 2022).\nTo obtain more data, researchers synthesize synthetic data using graphics simulators (Mu et al., 2021; Gong et al.,\n2023b), and try to close the sim2real gap (Tobin et al., 2017; Sadeghi and Levine, 2016; Peng et al., 2018). Recently,\nthere has been some collective effort to curate large-scale dataset that aims to resolve the data scarcity problem (Padalkar\net al., 2023; Brohan et al., 2023). On the other hand, to improve sample complexity, data augmentation techniques have\nbeen extensively studied as well (Zeng et al., 2021; Rao et al., 2020; Haarnoja et al., 2023; Lifshitz et al., 2023).\n4.1.4 In-context Learning\nIn-context learning was shown to be an effective method for solving tasks in NLP with the advent of large language\nmodels like GPT-3 (Brown et al., 2020; Min et al., 2022). Few-shot prompts were seen to be an effective way to\ncontextualize model output\u2019s across a variety of tasks in NLP by providing examples of the task within the context of the\nLLM prompt. Factors like the diversity of examples and quality of examples shown for the in-context demonstrations\nmay improve the quality of model outputs (An et al., 2023; Dong et al., 2022).", "start_char_idx": 0, "end_char_idx": 3804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13b2453f-f279-4d4e-bf74-a051e1e955f6": {"__data__": {"id_": "13b2453f-f279-4d4e-bf74-a051e1e955f6", "embedding": null, "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b20f27f-24ca-47d6-ba95-591b509dbb83", "node_type": "4", "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "fa74459c7099d7e77d8ece830316649d2ae50b1baa59743c9f2d85015b3dd16a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c1d2e9f-6e08-49a0-ab8c-0c5854191b6f", "node_type": "1", "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e8bb44608d0f08df4b421611b1f72d640899b5b9b8b1c5a77d961b51251416c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88331076-f849-41c8-81eb-82835373c36b", "node_type": "1", "metadata": {}, "hash": "1f2b8e718198bf08fea3dc75380fa853e9246574211554fd78413a345167411f", "class_name": "RelatedNodeInfo"}}, "text": "On the other hand, to improve sample complexity, data augmentation techniques have\nbeen extensively studied as well (Zeng et al., 2021; Rao et al., 2020; Haarnoja et al., 2023; Lifshitz et al., 2023).\n4.1.4 In-context Learning\nIn-context learning was shown to be an effective method for solving tasks in NLP with the advent of large language\nmodels like GPT-3 (Brown et al., 2020; Min et al., 2022). Few-shot prompts were seen to be an effective way to\ncontextualize model output\u2019s across a variety of tasks in NLP by providing examples of the task within the context of the\nLLM prompt. Factors like the diversity of examples and quality of examples shown for the in-context demonstrations\nmay improve the quality of model outputs (An et al., 2023; Dong et al., 2022). Within the context of multi-modal\nfoundation models, models like Flamingo and BLIP-2 (Alayrac et al., 2022; Li et al., 2023c) have been shown to be\neffective at a variety of visual understanding tasks when given only given a small number of examples. In context\nlearning can be further improved for agents within environments by incorporating environment-specific feedback when\ncertain actions are taken (Gong et al., 2023a).\n4.1.5 Optimization in the Agent System\nThe optimization of agent systems can be divided into spatial and temporal aspects. Spatial optimization considers how\nagents operate within a physical space to execute tasks. This includes inter-robot coordination, resource allocation, and\nkeeping an organized space.\nIn order to effectively optimize agent AI systems, especially systems with large numbers of agents acting in parallel,\nprevious works have focused on using large batch reinforcement learning (Shacklett et al., 2023). Since datasets of\n18", "start_char_idx": 3036, "end_char_idx": 4776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88331076-f849-41c8-81eb-82835373c36b": {"__data__": {"id_": "88331076-f849-41c8-81eb-82835373c36b", "embedding": null, "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f01b989-b4a4-4787-ba77-ae4066721911", "node_type": "4", "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ad7b3c504b8c6e5c780953ef8b74f83c59a4ccbc15b618ad734fcf29f127b296", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13b2453f-f279-4d4e-bf74-a051e1e955f6", "node_type": "1", "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "cd74ea43c42919810cc4a4516705fa0cd415ddf3e6dc53a23182f8a7ff0505af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbd86485-c5e6-45b9-8356-e0e9b9cbc054", "node_type": "1", "metadata": {}, "hash": "2227e4d2dc4b4782055bc08c4373e2221160d2e2c3960e977dbb418280ec5b96", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nmulti-agent interactions for specific tasks are rare, self-play reinforcement learning enables a team of agents to improve\nover time. However, this may also lead to very brittle agents that can only work under self-play and not with humans or\nother independent agents since they over-fit to the self-play training paradigm. To address this issue, we can instead\ndiscover a diverse set of conventions (Cui et al., 2023; Sarkar et al., 2023), and train an agent that is aware of a wide\nrange of conventions. Foundation models can further help to establish conventions with humans or other independent\nagents, enabling smooth coordination with new agents.\nTemporal optimization, on the other hand, focuses on how agents execute tasks over time. This encompasses task\nscheduling, sequencing, and timeline efficiency. For instance, optimizing the trajectory of a robot\u2019s arm is an example\nof efficiently optimizing movement between consecutive tasks (Zhou et al., 2023c). At the level of task scheduling,\nmethods like LLM-DP (Dagan et al., 2023) and ReAct (Yao et al., 2023a) have been proposed to solve efficient task\nplanning by incorporating environmental factors interactively.\n4.2 Agent Systems (zero-shot and few-shot level)\n4.2.1 Agent Modules\nOur foray into the agent paradigm involves the development of Agent AI \"Modules\" for interactive multi-modal agents\nusing LLMs or VLMs. Our initial Agent Modules facilitate training or in-context learning and adopt a minimalist\ndesign for the purposes of demonstrating the agent\u2019s ability to schedule and coordinate effectively. We also explored\ninitial prompt-based memory techniques that facilitate better planning and inform future actions approaches within the\ndomain. To illustrate, our \u201cMindAgent\" infrastructure comprises 5 main modules: 1) environment perception with task\nplanning, 2) agent learning, 3) memory, 4) general agent action prediction and 5) cognition, as shown in Figure 5.\n4.2.2 Agent Infrastructure\nAgent-based AI is a large and fast-growing community within the domains of entertainment, research, and industry.\nThe development of large foundation models has significantly improved the performance of agent AI systems. However,\ncreating agents in this vein is limited by the increasing effort necessary to create high-quality datasets and overall\ncost. At Microsoft, building high-quality agent infrastructure has significantly impacted multi-modal agent copilots by\nusing advanced hardware, diverse data sources, and powerful software libraries. As Microsoft continues to push the\nboundaries of agent technology, AI agent platforms are poised to remain a dominant force in the world of multimodal\nintelligence for years to come. Nevertheless, agent AI interaction is currently still a complex process that requires a\ncombination of multiple skills. The recent advancements in the space of large generative AI models have the potential\nto greatly reduce the current high cost and time required for interactive content, both for large studios, as well as\nempowering smaller independent content creators to design high quality experiences beyond what they are currently\ncapable of. The current human-machine interaction systems inside multi-modal agents are primarily rule-based. They\ndo have intelligent behaviors in response to human/user actions and possess web knowledge to some extent. However,\nthese interactions are often limited by software development costs to enable specific behaviors in the system. In addition,\ncurrent models are not designed to help human to achieve a goal in the case of users\u2019 inability to achieve specific tasks.\nTherefore, there is a need for an agent AI system infrastructure to analyze users behaviors and provide proper support\nwhen needed.\n4.3 Agentic Foundation Models (pretraining and finetune level)\nThe use of pre-trained foundation models offers a significant advantage in their wide applicability across diverse use\ncases. The integration of these models enables the development of customized solutions for various applications,\ncircumventing the need for extensive labeled datasets for each specific task.\nA notable example in the field of navigation is the LM-Nav system (Shah et al., 2023a), which incorporates GPT-3\nand CLIP in a novel approach. It effectively uses textual landmarks generated by the language model, anchoring them\nin images acquired by robots for navigation. This method demonstrates a seamless fusion of textual and visual data,\nsignificantly enhancing the capabilities of robotic navigation, while maintaining wide applicability.\nIn robot manipulation, several studies have proposed the use of off-the-shelf LLMs (e.g., ChatGPT) while using open\nvocabulary object detectors.", "start_char_idx": 0, "end_char_idx": 4783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbd86485-c5e6-45b9-8356-e0e9b9cbc054": {"__data__": {"id_": "fbd86485-c5e6-45b9-8356-e0e9b9cbc054", "embedding": null, "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f01b989-b4a4-4787-ba77-ae4066721911", "node_type": "4", "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ad7b3c504b8c6e5c780953ef8b74f83c59a4ccbc15b618ad734fcf29f127b296", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88331076-f849-41c8-81eb-82835373c36b", "node_type": "1", "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2ca49ade9a55c923ebfb33255db93fb4f20c228aaf46776ab820986858649eb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2413bbcb-e474-4694-9f29-836ce83b572d", "node_type": "1", "metadata": {}, "hash": "de076b39d4c74fc4e39ebb29e99a3458f713cc5e8121fcf276cb89dc35dd4e3b", "class_name": "RelatedNodeInfo"}}, "text": "4.3 Agentic Foundation Models (pretraining and finetune level)\nThe use of pre-trained foundation models offers a significant advantage in their wide applicability across diverse use\ncases. The integration of these models enables the development of customized solutions for various applications,\ncircumventing the need for extensive labeled datasets for each specific task.\nA notable example in the field of navigation is the LM-Nav system (Shah et al., 2023a), which incorporates GPT-3\nand CLIP in a novel approach. It effectively uses textual landmarks generated by the language model, anchoring them\nin images acquired by robots for navigation. This method demonstrates a seamless fusion of textual and visual data,\nsignificantly enhancing the capabilities of robotic navigation, while maintaining wide applicability.\nIn robot manipulation, several studies have proposed the use of off-the-shelf LLMs (e.g., ChatGPT) while using open\nvocabulary object detectors. The combination of LLM and advanced object detectors (e.g., Detic (Zhou et al., 2022)) fa-\ncilitates the understanding of human instruction while grounding the textual information in scenery information (Parakh\n19", "start_char_idx": 3819, "end_char_idx": 4997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2413bbcb-e474-4694-9f29-836ce83b572d": {"__data__": {"id_": "2413bbcb-e474-4694-9f29-836ce83b572d", "embedding": null, "metadata": {"page_label": "20", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "230ec16e-1645-47ad-a8e7-19d480d15d33", "node_type": "4", "metadata": {"page_label": "20", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "08a2b685c603b3f7fb5b98b6e34e7d8f4a71e29176099ce5e6d9f2c176e9b37b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbd86485-c5e6-45b9-8356-e0e9b9cbc054", "node_type": "1", "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "6c003e495751f7a563a892d546b85d24dd9628206619185e5078c2a7bd8ede70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9a4fa6c-ce7a-4980-8c8c-873147cff90e", "node_type": "1", "metadata": {}, "hash": "cd9ed6787802005a946e20508888f0fd88e46fd020037e141f502964cb1f03ad", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\net al., 2023). Furthermore, the latest advancements showcase the potential of using prompt engineering with advanced\nmulti-modal models such as GPT-4V(ision) (Wake et al., 2023b). This technique opens avenues for multi-modal task\nplanning, underscoring the versatility and adaptability of pre-trained models in a variety of contexts.\n5 Agent AI Categorization\n5.1 Generalist Agent Areas\nComputer-based action and generalist agents (GAs) are useful for many tasks. Recent progress in the field of large\nfoundation models and interactive AI has enabled new functionalities for GAs. However, for a GA to become truly\nvaluable to its users, it must be natural to interact with, and generalize to a broad range of contexts and modalities. We\nhigh-quality extended main Chapters on Agent foundation AI in Sec.6, especially in areas relevant to the themes in\ngeneral of these topics:\nMultimodal Agent AI (MMA) is an upcoming forum2for our research and industry communities to engage with each\nother and with the broader research and technology communities in Agent AI. Recent progress in the field of large\nfoundation models and interactive AI has enabled new functionalities for generalist agents (GAs), such as predicting\nuser actions and task planning in constrained settings (e.g., MindAgent (Gong et al., 2023a), fine-grained multimodal\nvideo understanding (Luo et al., 2022), Robotics (Ahn et al., 2022b; Brohan et al., 2023)), or providing a chat companion\nfor users that incorporates knowledge feedback (e.g., website customer support for healthcare systems (Peng et al.,\n2023)). More details about the representative works and most recent representative works are shown below. We hope to\ndiscuss our vision for the future of MAA and inspire future researchers to work in this space. This article and our forum\ncovers the following main topics, but is not limited exclusively to these:\n\u2022Primary Subject Topics: Multimodal Agent AI, General Agent AI\n\u2022Secondary Subject Topics: Embodied Agents, Action Agents, Language-based Agents, Vision & Language\nAgents, Knowledge and Inference Agents, Agents for Gaming, Robotics, Healthcare, etc.\n\u2022Extend Subject Topics: Visual Navigation, Simulation Environments, Rearrangement, Agentic Foundation\nModels, VR/AR/MR, Embodied Vision & Language.\nNext, we present a specific lists of representative agent categories as follows:\n5.2 Embodied Agents\nOur biological minds live in bodies, and our bodies move through a changing world. The goal of embodied artificial\nintelligence is to create agents, such as robots, which learn to creatively solve challenging tasks requiring interaction with\nthe environment. While this is a significant challenge, important advances in deep learning and the increasing availability\nof large datasets like ImageNet have enabled superhuman performance on a variety of AI tasks previously thought\nintractable. Computer vision, speech recognition and natural language processing have experienced transformative\nrevolutions at passive input-output tasks like language translation and image classification, and reinforcement learning\nhas similarly achieved world-class performance at interactive tasks like game playing. These advances have supercharged\nembodied AI, enabling a growing collection of users to make rapid progress towards intelligent agents can interactive\nwith machine.\n5.2.1 Action Agents\nAction agents refer to the agents that need to execute physical actions in the simulated physical environment or real\nworld. In particular, they need to be actively engaging in activities with the environment. We broadly classify action\nagents into two different categories based on their application domains: gaming AI and robotics.\nIn gaming AI, the agents will interact with the game environment and other independent entities. In these settings,\nnatural language can enable smooth communication between agents and humans. Depending on the game, there may be\na specific task to accomplish, providing a true reward signal. For instance, in the competitive Diplomacy game, training\n2Current URL: https://multimodalagentai.github.io/\n20", "start_char_idx": 0, "end_char_idx": 4176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9a4fa6c-ce7a-4980-8c8c-873147cff90e": {"__data__": {"id_": "f9a4fa6c-ce7a-4980-8c8c-873147cff90e", "embedding": null, "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08534bd2-dac8-462b-af06-01f5ccc93a23", "node_type": "4", "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "3d575d9b6714b1d3d2871e4e56171ce8dcdb9c09685aab58cb1b3157b872943a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2413bbcb-e474-4694-9f29-836ce83b572d", "node_type": "1", "metadata": {"page_label": "20", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "08a2b685c603b3f7fb5b98b6e34e7d8f4a71e29176099ce5e6d9f2c176e9b37b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "595a9f74-c856-4a6d-a667-10b128b02bc6", "node_type": "1", "metadata": {}, "hash": "a954cbbe542ea83c253df1cecd00e60f32bb726ac3373c510f2d6388800adc01", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\na language model using human conversation data along with an action policy with RL enables human-level play (Meta\nFundamental AI Research (FAIR) Diplomacy Team et al., 2022).\nThere are also settings where we agents act as normal residents in a town (Park et al., 2023a), without trying to optimize\na specific goal. Foundation models are useful in these settings because they can model interactions that appear more\nnatural by mimicking human behavior. When augmented with external memory, they produce convincing agents that\ncan have conversations, daily schedules, form relationships, and have a virtual life.\n5.2.2 Interactive Agents\nInteractive agents simply refer to agents that can interact with the world, a broader class of agents than action agents.\nTheir forms of interaction do not necessarily require physical actions, but may involve communicating information to\nusers or modifying the environment. For instance, an embodied interactive agent may answer a user\u2019s questions about a\ntopic through dialogue or help users parse through existing information similar to a chatbot. By extending an agent\u2019s\ncapabilities to include information sharing, the core designs and algorithms of Agent AI can be effectively adapted for a\nrange of applications, such as diagnostic (Lee et al., 2023) and knowledge-retrieval (Peng et al., 2023) agents.\n5.3 Simulation and Environments Agents\nAn effective approach for AI agents to learn how to act in an environment is to go through trial-and-error experiences\nvia interactions with the environment. A representative method is RL, which requires extensive experience of failures\nto train an agent. Although there exist approaches that use physical agents (Kalashnikov et al., 2018), using physical\nagents is time-consuming and costly. Furthermore, training in the physical environment is often feasible when failure in\nactual environments can be dangerous (e.g., autonomous driving, underwater vehicles). Hence, using simulators to\nlearn policies is a common approach.\nMany simulation platforms have been proposed for research in embodied AI, ranging from navigation (Tsoi et al.,\n2022; Deitke et al., 2020; Kolve et al., 2017) to object manipulation (Wang et al., 2023d; Mees et al., 2022; Yang\net al., 2023a; Ehsani et al., 2021). One example is Habitat (Savva et al., 2019; Szot et al., 2021), which provides a\n3D indoor environment where human- and robotic-agents can perform various tasks such as navigation, instruction\nfollowing, and question answering. Another representative simulation platform is VirtualHome (Puig et al., 2018),\nsupporting human avatars for object manipulation in 3D indoor environments. In the field of gaming, Carroll et al.\nhave introduced \"Overcooked-AI,\" a benchmark environment designed to study cooperative tasks between humans and\nAI (Carroll et al., 2019). Along similar lines, several works aim to incorporate real human intervention beyond the\nfocus of interaction between agents and the environment (Puig et al., 2023; Li et al., 2021a; Srivastava et al., 2022).\nThese simulators contribute to the learning of policies in practical settings involving agent and robot interactions, and\nIL-based policy learning utilizing human demonstrative actions.\nIn certain scenarios, the process of learning a policy may necessitate the integration of specialized features within\nsimulators. For example, in the case of learning image-based policies, realistic rendering is often required to facilitate\nadaptability to real environments (Mittal et al., 2023; Zhong et al., 2023). Utilizing a realistic rendering engine is\neffective for generating images that reflect various conditions, such as lighting environments. Moreover, simulators\nemploying physics engines are required to simulate physical interactions with objects (Liu and Negrut, 2021). The\nintegration of physics engines in simulation has been shown to facilitate the acquisition of skills that are applicable in\nreal-world scenarios (Saito et al., 2023).\n5.4 Generative Agents\nThe recent advancements in the space of large generative AI models have the potential to greatly reduce the current high\ncost and time required for interactive content, both for large gaming studios, as well as empower smaller independent\nstudios to create high quality experiences beyond what they are currently capable of.", "start_char_idx": 0, "end_char_idx": 4406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "595a9f74-c856-4a6d-a667-10b128b02bc6": {"__data__": {"id_": "595a9f74-c856-4a6d-a667-10b128b02bc6", "embedding": null, "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08534bd2-dac8-462b-af06-01f5ccc93a23", "node_type": "4", "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "3d575d9b6714b1d3d2871e4e56171ce8dcdb9c09685aab58cb1b3157b872943a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9a4fa6c-ce7a-4980-8c8c-873147cff90e", "node_type": "1", "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b535d01eb0d9f8058f3a5a01da193e66c116171d217cc443014970c96c132921", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "852d7686-a25d-4195-b529-775ab3f21c5f", "node_type": "1", "metadata": {}, "hash": "4eef5eb56489649a4c9b5fc8fbcdce10b8d464ed378e6557973ad2a74f60b8bd", "class_name": "RelatedNodeInfo"}}, "text": "For example, in the case of learning image-based policies, realistic rendering is often required to facilitate\nadaptability to real environments (Mittal et al., 2023; Zhong et al., 2023). Utilizing a realistic rendering engine is\neffective for generating images that reflect various conditions, such as lighting environments. Moreover, simulators\nemploying physics engines are required to simulate physical interactions with objects (Liu and Negrut, 2021). The\nintegration of physics engines in simulation has been shown to facilitate the acquisition of skills that are applicable in\nreal-world scenarios (Saito et al., 2023).\n5.4 Generative Agents\nThe recent advancements in the space of large generative AI models have the potential to greatly reduce the current high\ncost and time required for interactive content, both for large gaming studios, as well as empower smaller independent\nstudios to create high quality experiences beyond what they are currently capable of. Additionally, embedding large AI\nmodels within a sandbox environment will allow users to author their own experiences and express their creativity in\nways that are currently out of reach.\nThe goals of this agent go beyond simply adding interactive 3d content to scenes, but also include:\n21", "start_char_idx": 3433, "end_char_idx": 4697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "852d7686-a25d-4195-b529-775ab3f21c5f": {"__data__": {"id_": "852d7686-a25d-4195-b529-775ab3f21c5f", "embedding": null, "metadata": {"page_label": "22", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38470abe-515b-4c66-80ba-22edbec22ea8", "node_type": "4", "metadata": {"page_label": "22", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a21fee2ceb6c196e6d275364a94a33b7d15560c32be919e35a67f60ab5ec18a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "595a9f74-c856-4a6d-a667-10b128b02bc6", "node_type": "1", "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "f088bc6ca9b7cadf4bfbfe3534f07f37025736a558b2c966a4d4e6ee0b359519", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44b03b3c-2789-44e2-a99e-77f12e8fea54", "node_type": "1", "metadata": {}, "hash": "814c5175c7646107ab44c87f085eef2a2f29172c469cdaf246a5a14f4a3ca4e2", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n\u2022Adding arbitrary behavior and rules of interactions to the objects, allowing the user to create their own VR\nrules with minimal prompting.\n\u2022Generating whole level geometry from a sketch on a piece of paper, by using the multimodal GPT4-v model,\nas well as other chains of models involving vision AI models\n\u2022 Retexturing content in scenes using diffusion models\n\u2022 Creating custom shaders and visual special effects from simple user prompts\nOne potential application in the short term is the VR creation of a storyboarding/prototype tool allowing a single user to\ncreate a rough (but functional) sketch of an experience/game an order of magnitude faster than currently feasible. Such\na prototype then could be expanded and made more polished using these tools as well.\n5.4.1 AR/VR/mixed-reality Agents\nAR/VR/mixed-reality (jointly referred to as XR) settings currently require skilled artists and animators to create\ncharacters, environments, and objects to be used to model interactions in virtual worlds. This is a costly process that\ninvolves concept art, 3D modeling, texturing, rigging, and animation. XR agents can assist in this process by facilitating\ninteractions between creators and building tools to help build the final virtual environment.\nOur early experiments have already demonstrated that GPT models can be used in the few-shot regime inside of the\nUnity engine (without any additional fine-tuning) to call engine-specific methods, use API calls to download 3d models\nfrom the internet and place them into the scene, and assign state trees of behavior and animations to them (Huang et al.,\n2023a). This behavior likely emerges due to the presence of similar code in open source game repositories that use\nUnity. Therefore, GPT models are capable of building rich visual scenes in terms of loading in many objects into the\nscene from a simple user prompt.\nThe aim of this category of agents is to build a platform and a set of tools that provide an efficient interface between\nlarge AI models (both GPT-family ones as well as diffusion image models) and a rendering engine. We explore two\nprimary avenues here:\n\u2022Integration of large models into the various editor tools in the agent infrastructure, allowing for significant\nspeedups in development.\n\u2022Controlling the rendering engine from within a user experience, by generating code that follows user instruction\nand then compiling it at runtime, allowing for users to potentially edit the VR/simulation they are interacting\nwith in arbitrary ways, even by introducing new agent mechanics.\nIntroducing an AI copilot focused on XR settings would be useful for XR creators, who can use the copilot to complete\ntedious tasks, like providing simple assets or writing code boilerplate, freeing creators to focus on their creative vision\nand quickly iterate on ideas.\nFurthermore, agents can help users interactively modify the environment by adding new assets, changing the dynamics\nof the environment, or building new settings. This form of dynamic generation during runtime can also be specified by\na creator, enabling the user\u2019s experience to feel fresh and continue evolving over time.\n5.5 Knowledge and Logical Inference Agents\nThe capacity to infer and apply knowledge is a defining feature of human cognition, particularly evident in complex\ntasks such as logical deduction, and understanding theory of mind3. Making inferences on knowledge ensures that\nthe AI\u2019s responses and actions are consistent with known facts and logical principles. This coherence is a crucial\nmechanism for maintaining trust and reliability in AI systems, especially in critical applications like medical diagnosis\nor legal analysis. Here, we introduce agents that incorporate the interplay between knowledge and inference that address\nspecific facets of intelligence and reasoning.\n3https://plato.stanford.edu/entries/cognitive-science\n22", "start_char_idx": 0, "end_char_idx": 3954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44b03b3c-2789-44e2-a99e-77f12e8fea54": {"__data__": {"id_": "44b03b3c-2789-44e2-a99e-77f12e8fea54", "embedding": null, "metadata": {"page_label": "23", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ba492ac-4fa3-492e-858b-b8fe2a35bcdc", "node_type": "4", "metadata": {"page_label": "23", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "237d6e82b52fe72ad97ab51eeeef1b4f920afd09ed918333cf6062b48458779b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "852d7686-a25d-4195-b529-775ab3f21c5f", "node_type": "1", "metadata": {"page_label": "22", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a21fee2ceb6c196e6d275364a94a33b7d15560c32be919e35a67f60ab5ec18a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8302bc0-81ee-40f1-bdbf-caccdab84cda", "node_type": "1", "metadata": {}, "hash": "216a538bf3267f2029d7fcce37d178dc32c8fe4a1110933a499f0d33a7d5af7d", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n5.5.1 Knowledge Agent\nKnowledge Agents reason over their acquired knowledge systems in two directions: implicit and explicit. Implicit\nknowledge is typically what large-scale language models like the GPT series (Brown et al., 2020; OpenAI, 2023)\nencapsulate after being trained on vast amounts of text data. These models can generate responses that give the\nimpression of understanding, as they draw on patterns and information implicitly learned during training. Explicit\nknowledge, conversely, is structured and can be directly queried, such as the information found in knowledge bases\nor databases, which was traditionally used to enhance AI reasoning capabilities by referencing verifiable external\nresources.\nDespite the advancements in language models, their implicit knowledge is static and becomes outdated as the world\nevolves (Lewis et al., 2020; Peng et al., 2023). This limitation necessitates the integration of explicit knowledge sources\nthat are updated continuously, ensuring that AI systems can provide accurate and current responses. The fusion of\nimplicit and explicit knowledge equips AI agents with a more nuanced understanding and the ability to apply knowledge\ncontextually, akin to human intelligence (Gao et al., 2022). Such integration is crucial for crafting knowledge-centric AI\nagents that not only possess information but can also understand, explain, and employ it, thereby narrowing the chasm\nbetween extensive learning and profound knowledge (Marcus and Davis, 2019; Gao et al., 2020). These agents are\ndesigned to reason with flexibility and dynamic information about the world, enhancing their robustness and adaptability\n(Marcus, 2020).\n5.5.2 Logic Agents\nGenerally, a logic agent is a component of a system designed to apply logical reasoning to process data or solve tasks\nspecific to logical inference or logical reasoning. Logic agents within the context of large foundation models like GPT-4\nrefers to a specialized component or submodules designed to handle logical reasoning tasks. These tasks often involve\nunderstanding and manipulating abstract concepts, deducing conclusions from given premises, or solving problems\nthat require a structured, logical approach. Broadly, foundation models like GPT-4 are trained on a vast corpus of text\ndata and learn to perform a wide range of tasks, including those that require some form of logical reasoning. Thus,\ntheir capability for logical reasoning is integrated into the overall architecture, and they generally do not possess a\ndistinct, isolated \"Logic agent\". While GPT-4 and similar models can perform tasks that involve logic, their approach\nis fundamentally different from how humans or traditional logic-based systems operate. They do not follow formal\nlogical rules or have an explicit understanding of logic; rather, they generate responses based on patterns learned from\nthe training data. As a result, their performance in logical tasks can be impressive, but it can also be inconsistent or\nlimited by the nature of the training data and the inherent limitations of the model\u2019s design. One example of embedding\na separate logical submodule into the architecture is (Wang et al., 2023e), which modifies the token embedding process\nused by LLMs during pre-training by parsing text into logical segments and explicitly modeling logical hierarchies in\nthe token embeddings.\n5.5.3 Agents for Emotional Reasoning\nEmotional understanding and empathy are important skills for agents in many human-machine interactions. To illustrate,\none important goal for creating engaging dialogue agents is to have the agents act with increased emotion and empathy\nwhile minimizing socially inappropriate or offensive outputs. To advance towards this goal for dialogue agents, we\nreleased the Neural Image Commenting with Empathy ( NICE ) dataset (Chen et al., 2021) consisting of almost two\nmillion images and the corresponding human-generated comments and a set of human emotion annotations. We also\nprovided a novel pre-training model - Modeling Affect Gneration for Image Comments ( MAGIC ) (Chen et al., 2021) -\nwhich aims to generate comments for images, conditioned on linguistic representations that capture style and affect, and\nto help generate more empathetic, emotional, engaging and socially appropriate comments. Our experiments show that\nthe approach is effective in training a more human-like and engaging image comment agent. Developing empathy-aware\nagents is a promising direction for interactive agents, and it is important to create agents with emotional understanding\ncapabilities across a wide range of groups and populations, especially considering that many current language models\nexhibit bias in their emotional understanding and empathetic reasoning capabilities (Mao et al., 2022; Wake et al.,\n2023d).\n23", "start_char_idx": 0, "end_char_idx": 4884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8302bc0-81ee-40f1-bdbf-caccdab84cda": {"__data__": {"id_": "c8302bc0-81ee-40f1-bdbf-caccdab84cda", "embedding": null, "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a06d8914-305f-4ad4-b739-9c2375dcdc40", "node_type": "4", "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "573f60436561a1e87332483180e5647fcb0c4904d3dbd93f68a8ac50efdccc7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44b03b3c-2789-44e2-a99e-77f12e8fea54", "node_type": "1", "metadata": {"page_label": "23", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "237d6e82b52fe72ad97ab51eeeef1b4f920afd09ed918333cf6062b48458779b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29a81eb8-17df-48d1-86d2-40e941b92255", "node_type": "1", "metadata": {}, "hash": "c266d45163c24e880d276cccb6a45354957a6d87c08bd93e894c586dd858cb7b", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n5.5.4 Neuro-Symbolic Agents\nNeuro-Symbolic agents operate on a hybrid system of neurons and symbols (d\u2019Avila Garcez and Lamb, 2020). To solve\nproblems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic\nstructural information implicit in the input. However, most general neural sequence models do not explicitly capture\nsuch structural information, limiting their performance on these tasks. The work (Chen et al., 2020) propose a new\nencoder-decoder model based on a structured neural representation agent, The encoder of TP-N2F employs TPR\n\u2018binding\u2019 to encode natural-language symbolic structure in vector space and the decoder uses TPR \u2018unbinding\u2019 to\ngenerate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or\noperation) and a number of arguments.\nInstruction following vision-language (VL) models like GPT-4 offer a flexible interface that supports a broad range\nof multimodal tasks in a zero-shot fashion. However, interfaces that operate on full images do not directly enable\nthe user to \u201cpoint to\u201d and access specific regions within images. This capability is important not only to support\nreference-grounded VL benchmarks, but also, for practical applications that require precise within-image reasoning.\nIn (Park et al., 2023b), we build Localized Visual Commonsense model which allows users to specify (multiple)\nregions-as-input. We train our model by sampling localized commonsense knowledge from a large language model\n(LLM): specifically, we prompt a LLM to collect common sense knowledge given a global literal image description and\na local literal region description automatically generated by a set of VL models. This pipeline is scalable and fully\nautomatic, as no aligned or human-authored image and text pairs are required. With a separately trained critic model\nthat selects high quality examples, we find that training on the localized commonsense corpus expanded solely from\nimages can successfully distill existing VL models to support a reference-as-input interface. Empirical results and\nhuman evaluations in zero-shot settings demonstrate that our distillation method results in more precise VL models of\nreasoning compared to a baseline of passing a generated referring expression.\n5.6 LLMs and VLMs Agent\nA number of works leverage LLMs as agents to perform task planning (Huang et al., 2022a; Wang et al., 2023b;\nYao et al., 2023a; Li et al., 2023a), and leverage the LLMs\u2019 large internet-scale domain knowledge and zero-shot\nplanning abilities to perform agentic tasks like planning and reasoning. Recent robotics research also leverages LLMs\nto perform task planning (Ahn et al., 2022a; Huang et al., 2022b; Liang et al., 2022) by decomposing natural language\ninstruction into a sequence of subtasks, either in the natural language form or in Python code , then using a low-level\ncontroller to execute these subtasks. Additionally, (Huang et al., 2022b), (Liang et al., 2022), and (Wang et al., 2023a)\nalso incorporate environmental feedback to improve task performance. There have also been a number of works that\ndemonstrate the ability of general-purpose visually-aligned large language models trained on large-scale text, image,\nand video data to serve as a foundation for creating multi-modal agents that are embodied and can act in various\nenvironments (Baker et al., 2022; Driess et al., 2023; Brohan et al., 2023).\n6 Agent AI Application Tasks\n6.1 Agents for Gaming\nGames provide a unique sandbox to test the agentic behavior of LLMs and VLMs, pushing the boundaries of their\ncollaborative and decision-making abilities. We describe three areas in particular that highlight agent\u2019s abilities to\ninteract with human players and other agents, as well as their ability to take meaningful actions within an environment.\n6.1.1 NPC Behavior\nIn modern gaming systems, the behavior of Non-Player Characters (NPCs) is predominantly dictated by predefined\nscripts crafted by developers. These scripts encompass a range of reactions and interactions based on various triggers or\nplayer actions within the gaming environment. However, this scripted nature often results in predictable or repetitive\nNPC behavior which fails to evolve in response to player\u2019s actions or the dynamic environment of the game.", "start_char_idx": 0, "end_char_idx": 4419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29a81eb8-17df-48d1-86d2-40e941b92255": {"__data__": {"id_": "29a81eb8-17df-48d1-86d2-40e941b92255", "embedding": null, "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a06d8914-305f-4ad4-b739-9c2375dcdc40", "node_type": "4", "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "573f60436561a1e87332483180e5647fcb0c4904d3dbd93f68a8ac50efdccc7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8302bc0-81ee-40f1-bdbf-caccdab84cda", "node_type": "1", "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "80be00e7cacd94a19e434df536cf1abd6cb7708770b589b2d0e9b4e1289860ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96e353a9-4908-481d-8e5d-fd05a52f2ef9", "node_type": "1", "metadata": {}, "hash": "98bc8d5ef43ae724749ff22af4cd353363e3a2b3e2ab823e1c983028b1788171", "class_name": "RelatedNodeInfo"}}, "text": "6 Agent AI Application Tasks\n6.1 Agents for Gaming\nGames provide a unique sandbox to test the agentic behavior of LLMs and VLMs, pushing the boundaries of their\ncollaborative and decision-making abilities. We describe three areas in particular that highlight agent\u2019s abilities to\ninteract with human players and other agents, as well as their ability to take meaningful actions within an environment.\n6.1.1 NPC Behavior\nIn modern gaming systems, the behavior of Non-Player Characters (NPCs) is predominantly dictated by predefined\nscripts crafted by developers. These scripts encompass a range of reactions and interactions based on various triggers or\nplayer actions within the gaming environment. However, this scripted nature often results in predictable or repetitive\nNPC behavior which fails to evolve in response to player\u2019s actions or the dynamic environment of the game. This rigidity\nhampers the immersive experience intended in a dynamic gaming environment. Therefore, there is a burgeoning interest\nin leveraging LLMs to induce autonomy and adaptability in NPC behavior, making interactions more nuanced and\nengaging. AI-driven NPCs can learn from player behavior, adapt to varying strategies, and provide a more challenging\n24", "start_char_idx": 3541, "end_char_idx": 4779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96e353a9-4908-481d-8e5d-fd05a52f2ef9": {"__data__": {"id_": "96e353a9-4908-481d-8e5d-fd05a52f2ef9", "embedding": null, "metadata": {"page_label": "25", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a623e6a9-e397-44f9-86c9-8f17f27ee947", "node_type": "4", "metadata": {"page_label": "25", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0346afcfbba1fd3d235118b8cd1b6126d28d844f9aca27b433e83c9e68c7cd0d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29a81eb8-17df-48d1-86d2-40e941b92255", "node_type": "1", "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "98e272c682ad39ccce822a33fa723a1426d3748c07dc6095a407b5d1cde6dcee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0914fd40-165c-47b1-a081-58f401544be5", "node_type": "1", "metadata": {}, "hash": "ef6579ef2c82e24d9f4f6176708f6c0cda983bc5242641f4e1437d196b708c26", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 8: The embodied agent for user interactive gaming action prediction and interactive editing with Minecraft\nDungeons gaming sense simulation and generation via GPT-4V .\nand less predictable gameplay experience. Large Language Models (LLMs) can significantly contribute to evolving\nNPC behavior in games. By processing vast amounts of text, LLMs can learn patterns and generate responses that\nare more varied and human-like. They can be utilized to create dynamic dialogue systems, making interactions with\nNPCs more engaging and less predictable. Furthermore, LLMs can be trained on player feedback and in-game data to\ncontinually refine NPC behaviors, making them more attuned to player expectations and game dynamics.\n6.1.2 Human-NPC Interaction\nThe interaction between human players and NPCs is a crucial aspect of the gaming experience. The conventional\ninteraction paradigm is primarily one-dimensional, with NPCs reacting in a preset manner to player inputs. This\nlimitation stifles the potential for a more organic and enriching interaction, akin to human-human interaction within\nthe virtual realm. The advent of LLM and VLM technologies holds the promise of transforming this paradigm. By\nemploying these technologies, gaming systems can analyze and learn from human behavior to provide more human-like\ninteractions. This not only enhances the realism and engagement of the game but also provides a platform for exploring\nand understanding human-machine interaction in a controlled yet complex setting.\n6.1.3 Agent-based Analysis of Gaming\nGaming is an integral part of daily life, estimated to engage half of the world\u2019s population4. Additionally, it exhibits\na positive impact on mental health5. However, contemporary game systems exhibit a deficiency in interactions with\nhuman players since their behaviors are primarily hand-crafted by game developers. These pre-programmed behaviors\nfrequently fail to adapt to players\u2019 needs. Consequently, there exists a need for new AI systems in games that can analyze\nplayer behaviors and furnish appropriate support when necessary. Intelligent interactive systems bear the potential\nto revolutionize how gamers interact with gaming systems in general. NPCs\u2019 interactions with gamers are no longer\nconfined by the restricted rule sets designed by game developers. They have the potential to adapt seamlessly to gamers\u2019\nexperiences, providing timely feedback to enrich the gaming experience and elevate the synergy of human-machine\ninteraction.\n4https://www.dfcint.com/global-video-game-audience-reaches-3-7-billion/\n5https://news.microsoft.com/source/features/work-life/mind-games-how-gaming-can-play-a-positive-role-in-mental-health/\n25", "start_char_idx": 0, "end_char_idx": 2767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0914fd40-165c-47b1-a081-58f401544be5": {"__data__": {"id_": "0914fd40-165c-47b1-a081-58f401544be5", "embedding": null, "metadata": {"page_label": "26", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a7ca561-9225-4e39-82b9-88d6c29a01a4", "node_type": "4", "metadata": {"page_label": "26", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "f3a0c5fc31d71f1f0e8ee210fd53b3dcfd7c677af7d36ec50f9f9d11e256d926", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96e353a9-4908-481d-8e5d-fd05a52f2ef9", "node_type": "1", "metadata": {"page_label": "25", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0346afcfbba1fd3d235118b8cd1b6126d28d844f9aca27b433e83c9e68c7cd0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "808c86eb-44f5-4ce9-a86f-91129c1aa6d8", "node_type": "1", "metadata": {}, "hash": "2071eba55088d48fd3b7059b9507d6eb0567c19df68e40a4ff8737b0f473d1d6", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nSystem prompt:  You are playing minecraft, please generate \nfuture actions. \nUser prompt:  The current action is moving to the right, and \nmoving forward. The goal is to craft a crafting table. Please \ndescribe the future actions \nVideo input: \nOutput: \nSince you\u2019ve collected wood, the next steps to craft a crafting \ntable in Minecraft are as follows: \n1. **Open your inventory** (`E` key by default) to bring up the \ncrafting screen. \n2. **Place the wood logs** you\u2019ve collected into one of the \ncrafting slots to turn them into wooden planks. Each log will \ncreate 4 wooden planks. \n3. **Collect the wooden planks** that you\u2019ve created by dragging \nthem into your inventory. \n4. **Fill all four crafting slots** in the 2x2 crafting grid \n(available in your inventory screen) with wooden planks to \ncreate one crafting table. \n5. **Collect the crafting table** by dragging it into your \ninventory. \n6. Close your inventory and **select the crafting table** in your \nhotbar. \n7. **Right-click on the ground** to place the crafting table. \nOnce the crafting table is placed, you can interact with it to use \nthe 3x3 crafting grid for creating a wide array of more complex \nitems. That\u2019s the basic process for crafting a crafting table in \nMinecraft. Now you\u2019re ready to begin crafting more advanced items \nand tools. \nGPT-4-Vision \nFigure 9: GPT-4V can effectively predict the high-level next actions when given the \u201caction history\" and a \u201cgaming\ntarget\" in the prompt. Furthermore, GPT-4V accurately recognized that the player is holding wooden logs in their hand\nand can incorporate this perceived information into its plan for future actions. Although GPT-4V appears to be capable\nof predicting some low-level actions (such as pressing \u2018E\u2018 to open the inventory), the model\u2019s outputs are not inherently\nsuitable for raw low-level action prediction (including mouse movements) and likely requires supplemental modules for\nlow-level action control.\n26", "start_char_idx": 0, "end_char_idx": 2024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "808c86eb-44f5-4ce9-a86f-91129c1aa6d8": {"__data__": {"id_": "808c86eb-44f5-4ce9-a86f-91129c1aa6d8", "embedding": null, "metadata": {"page_label": "27", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b534062-2e70-4154-bf4b-f355754e47c2", "node_type": "4", "metadata": {"page_label": "27", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "4b9440ff7696eac3a64679cfbb7a4206b76e90b0d301d530002dc2ca37492daf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0914fd40-165c-47b1-a081-58f401544be5", "node_type": "1", "metadata": {"page_label": "26", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "f3a0c5fc31d71f1f0e8ee210fd53b3dcfd7c677af7d36ec50f9f9d11e256d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fba36423-407d-4f57-b200-151590f4a123", "node_type": "1", "metadata": {}, "hash": "9d5c70a02e3c958f7439736b8092ea2840aa4361f77ac445b0a5fb876b3a385e", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nLLMs can serve as a robust tool for analyzing in-game text data, including chat logs, player feedback, and narrative\ncontent. They can help in identifying patterns of player behavior, preferences, and interactions which can be invaluable\nfor game developers to improve game mechanics and narratives. Additionally, VLMs can parse through large quantities\nof image and video data from gaming sessions to help analyze user intent and actions within the game world. Moreover,\nLLMs and VLMs can facilitate the development of intelligent agents within games that can communicate with players\nand other agents in a sophisticated and human-like manner, enhancing the overall gaming experience. Beyond LLMs\nand VLMs, user input data, provides a promising avenue for creating game-playing agents that model perception, game\nplaying, and game understanding by imitating human players. By incorporating a combination of player interactions\nand feedback, pixel inputs, and natural language planning and understanding, agent models can assist in the continuous\nimprovement of game dynamics, driving a more player-centric evolution of the gaming environment.\n6.1.4 Scene Synthesis for Gaming\nFigure 10: Masked video prediction on unseen Minecraft\nvideos. From left to right: the original frame, the masked\nframe, the reconstructed frame, and the reconstructed frame\nwith patches.Scene synthesis is a vital component in the creation and\nenhancement of immersive gaming environments. It en-\ntails the automatic or semi-automatic generation of three-\ndimensional (3D) scenes and environments within a game.\nThis process includes the generation of terrain, placement\nof objects, creation of realistic lighting, and sometimes\neven dynamic weather systems.\nModern games often feature vast, open-world environ-\nments. Manually designing these landscapes can be in-\ncredibly time-consuming and resource-intensive. Au-\ntomated terrain generation, often leveraging procedural\nor AI-driven techniques, can produce complex, realistic\nlandscapes with less manual effort. LLMs and VLMs can\nutilize the internet scale knowledge to formulate rules to\ndesign non-repeating landscapes that are visually impres-\nsive and unique. Additionally, LLMs and VLMs can be\nused to ensure the semantic consistency and variability of\ngenerated assets. Placing objects such as buildings, vege-\ntation, and other elements within a scene in a realistic and\naesthetically pleasing manner is crucial for immersion.\nVLMs and LLMs can assist in object placement by ad-\nhering to predefined or learned rules and aesthetics, thus\nspeeding up the level design process. VLMs and LLMs can be further trained to understand the principles of design and\naesthetics, aiding in the procedural generation of content. They can help formulate rules or guidelines that procedural\nalgorithms can follow to generate objects, and scenes that are both visually appealing and contextually appropriate.\nRealistic lighting and atmospheric effects are fundamental for creating a believable and engaging gaming environment.\nAdvanced algorithms can simulate natural lighting conditions and dynamic weather effects, enhancing the realism and\nmood of the scene. LLMs can help develop systems to acheive more realistic lighting and atmospheric effects in several\ninnovative ways. VLMs can analyze vast datasets from real-world lighting and atmospheric conditions to help develop\nmore realistic algorithms for simulating these effects in games. By understanding the patterns and intricacies of natural\nlighting and weather, these models can contribute to the development of algorithms that mimic reality closely. LLMs\nand VLMs could also be used to develop systems that adjust lighting and atmospheric effects in real-time based on\nplayer actions, game states, or external inputs. They can process natural language commands from players to modify\nthe game environment, providing a more interactive and immersive experience.\n6.1.5 Experiments and Results\n27", "start_char_idx": 0, "end_char_idx": 4047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fba36423-407d-4f57-b200-151590f4a123": {"__data__": {"id_": "fba36423-407d-4f57-b200-151590f4a123", "embedding": null, "metadata": {"page_label": "28", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4410f48a-e4ab-4d02-a93c-ec923628a92a", "node_type": "4", "metadata": {"page_label": "28", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b57ab1fd0a94841f0d66c4e0c380047b89e0615ebd6e451abda808261eb6368b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "808c86eb-44f5-4ce9-a86f-91129c1aa6d8", "node_type": "1", "metadata": {"page_label": "27", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "4b9440ff7696eac3a64679cfbb7a4206b76e90b0d301d530002dc2ca37492daf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7136962-c247-477a-84bb-f30c66e24127", "node_type": "1", "metadata": {}, "hash": "3b4a193125bdf1b50099567b899a2bdaabc6aa45a29bc1b8d32c786893b0e355", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nZero-shot/Few-shot Learning with LLM or LVM. As we showed in the Fig. 8 and Fig. 9, we used GPT-4V for\nhigh-level description and action prediction. Fig. 8 showed some qualitative examples of action description generation\nand editing with GPT-4V . Agent-enhanced text opens up a novel method of generating 3D scenes with game action\npriors to help improve the naturalness of the scene. Consequently, GPT-4V generates relevant high-level descriptions\nthat are appropriate for the gaming videos.\nFigure 11: The low-level next step action prediction with\nthe small agent pretraining model in gaming Minecraft\nscene.Small Agent Pretraining Model. To showcase our agent\nvision-language architecture, we first study its application in\na widely used domain for gaming agents by pretraining on\nMinecraft data. As shown in Fig. 7, given an input action\nagent, key frame of video, and corresponding text, a standard\nencoder-decoder can be employed to convert the agent ac-\ntion and image into action text token and image patch token\nand then use the agent-vision-language decoder to convert\nit into a action prediction sentence. The overall architecture\nis depicted in Fig. 7. We evaluate our approach with several\nMinecraft demonstrations. The Minecraft video data consists\nof 5min clips, and we use for pretraining contains 78K videos,\nand we used 5K videos (6% of pretraining data) for the first\nround pretraining. We train a 250M parameter model on 16\nNVIDIA v100 GPUs for one day and visualize our model out-\nputs in Fig. 10 and Fig. 11. Fig. 10 shows that our relatively\nsmall agent architecture can produce reasonable outputs for\nMinecraft scenes unseen during training. Fig. 11 showed\nthe model\u2019s predictions compared to the ground truth human\nplayer actions indicating potential low-level understanding\nfor our small agent model.\nMulti-Agent Infrastructure. As showed in the agent\nparadigm in Fig. 5, we designed a novel infrastructure for a new gaming scenario called \u201cCuisineWorld\" (Gong\net al., 2023a). We detail our approach in Fig. 12. Our infrastructure allows for multi-agent collaboration by leveraging\nGPT-4 as a central planner and works across multiple gaming domains. We investigated our system\u2019s multi-agent\nplanning capabilities, and we deployed the infrastructure into real-world video games to demonstrate its multi-agent\nand human-AI collaboration effectiveness. Additionally, we presented \u201cCuisineworld\", a text-based multi-agent\ncollaboration benchmark that provides a new auto-metric Collaboration Score (CoS) to quantify collaboration efficiency.\nPlease refer to the Appendix for more examples and details for gaming description, high-level action prediction, and\nGPT-4V prompting. We show examples for Bleeding Edge in Fig. 32 and Appendix B, Microsoft Flight Simulator in\nFig. 33 and Appendix C, ASSASSIN\u2019s CREED ODYSSEY in Fig. 34 and Appendix D, GEARS of WAR 4 in Fig. 35\nand Appendix E, and Starfield in Fig. 36 and Appendix F. We also provide a detailed screenshot of the prompting\nprocess for GPT4V used to generate Minecraft examples with Fig. 31 in Appendix A.\n6.2 Robotics\nRobots are representative agents that necessitate effective interaction with their environment. In this section, we will\nintroduce key elements essential for efficient robotic operation, review research topics where the latest LLM/VLM\ntechnologies have been applied, and share findings from our most recent studies.\nVisual Motor Control. Visual Motor Control refers to the integration of visual perception and motor action to execute\ntasks effectively in a robotic system. This integration is paramount as it enables robots to interpret the visual data from\ntheir environment and accordingly adjust their motor actions to interact with the environment accurately. For instance,\nin an assembly line, a robot equipped with visual motor control can perceive the position and orientation of objects and\naccurately align its manipulator to interact with these objects. This capability is essential for ensuring the precision and\neffectiveness of robotic operations across a myriad of applications, ranging from industrial automation to assisting the\nelderly in their daily chores. Moreover, visual motor control facilitates robots in adapting to dynamic environments\n28", "start_char_idx": 0, "end_char_idx": 4338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7136962-c247-477a-84bb-f30c66e24127": {"__data__": {"id_": "a7136962-c247-477a-84bb-f30c66e24127", "embedding": null, "metadata": {"page_label": "29", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94d46a4a-e6d9-469f-9b88-0297c7e3d5e8", "node_type": "4", "metadata": {"page_label": "29", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5559e77bc4f25f4e627ef90c4fa53fac590e4afb2290b2f09663b9162f553f28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fba36423-407d-4f57-b200-151590f4a123", "node_type": "1", "metadata": {"page_label": "28", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b57ab1fd0a94841f0d66c4e0c380047b89e0615ebd6e451abda808261eb6368b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "374b597f-258e-41d9-8109-5fe078eb56b4", "node_type": "1", "metadata": {}, "hash": "4e2d61366831fef1d79ae45c216393e6ab88c146a1aa8fb59f1c7ea72dac2e21", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 12: The MindAgent of in-context learning gaming Infrastructure. Planning Skill and Tool Use: The game\nenvironment requires diverse planning skills and tool use to complete tasks. It generates relevant game information\nand converts the game data into a structured text format that the LLMs can process. LLM: The main workhorse of\nour infrastructure makes decisions, thus serving as a dispatcher for the multi-agent system. Memory History: A\nstorage utility for relevant information. Action Module: Extracts actions from text inputs and convertd them into\ndomain-specific language and validates DSLs so that they cause no errors during execution.\nwhere the state of the environment may change rapidly, requiring real-time adjustments to motor actions based on visual\nfeedback.\nAdditionally, within the context of safe operation, visual information is crucial for detecting execution errors and\nconfirming the pre- and post-conditions of each robot action. In uncontrolled environments, such as unknown domestic\nsettings, robots are more likely to face unexpected outcomes due to unpredictable factors like changing furniture shapes,\nvaried lighting, and slippage. Executing a pre-planned action plan solely in a feedforward manner can pose significant\nrisks in these settings. Therefore, utilizing visual feedback to continually verify outcomes at each step is key to ensuring\nrobust and reliable operation of robotic systems.\nLanguage Conditioned Manipulation. Language Conditioned Manipulation entails the ability of a robotic system to\ninterpret and execute tasks based on language instructions. This aspect is particularly crucial for creating intuitive and\nuser-friendly interfaces for human-robot interaction. Through natural language commands, users can specify goals\nand tasks to robots in a manner similar to human-human communication, thereby lowering the barrier to operating\nrobotic systems. In a practical scenario, for instance, a user could instruct a service robot to \u201cpick up the red apple\nfrom the table,\u201d and the robot would parse this instruction, identify the referred object and execute the task of picking it\nup (Wake et al., 2023c). The core challenge lies in developing robust natural language processing and understanding\nalgorithms that can accurately interpret a wide array of instructions, ranging from direct commands to more abstract\ndirectives, and enable the robot to convert these instructions into actionable tasks. Furthermore, ensuring that robots can\ngeneralize these instructions across diverse tasks and environments is critical for enhancing their versatility and utility\nin real-world applications. The use of language input to guide robot\u2019s task planning has gained attention in the context\nof a robot framework called Task and Motion Planning (Garrett et al., 2021).\nSkill Optimization. Recent studies highlight the effectiveness of LLMs in robotic task planning. However the optimal\nexecution of tasks, especially those involving physical interactions like grasping, requires a deeper understanding of the\nenvironment that goes beyond simply interpreting human instructions. For example, robot grasping necessitates precise\ncontact points (Wake et al., 2023e) and arm posture (Sasabuchi et al., 2021) to efficiently execute subsequent actions.\n29", "start_char_idx": 0, "end_char_idx": 3367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "374b597f-258e-41d9-8109-5fe078eb56b4": {"__data__": {"id_": "374b597f-258e-41d9-8109-5fe078eb56b4", "embedding": null, "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8ac5fe3-8b1f-445f-a17c-8126f838aa7c", "node_type": "4", "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "8f2684842a35830d85182d9f9174b9da87211d7dad67d145f994e681e3ea91a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7136962-c247-477a-84bb-f30c66e24127", "node_type": "1", "metadata": {"page_label": "29", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5559e77bc4f25f4e627ef90c4fa53fac590e4afb2290b2f09663b9162f553f28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7837387-ba7d-41b5-baa1-ed78eef23b3a", "node_type": "1", "metadata": {}, "hash": "3e958107ef0eb51aa243cb3287e1020b9a84f39a44a58e3817b00ebb4e5b4697", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nWhile these elements\u2014precise contact points and arm posture\u2014are intuitive for humans, articulating them through\nlanguage is challenging. Despite advances in internet-scale VLMs, capturing these nuanced indirect cues from scenes\nand translating them effectively into robotic skills remains a significant challenge. In response, the robotics community\nis increasingly focusing on collecting enhanced datasets(e.g., (Wang et al., 2023d; Padalkar et al., 2023)) or developing\nmethodologies for direct skill acquisition from human demonstrations (Wake et al., 2021a). Frameworks including\nLearning-from-Demonstration and Imitation Learning are leading these developments, playing a crucial role in the\noptimization of physical skills.\n6.2.1 LLM/VLM Agent for Robotics.\nRecent research has demonstrated the potential of LLM/VLMs for robotic agents that involve interactions with humans\nin an environment. Research topics that aim to leverage latest LLM/VLM technologies include:\nMultimodal Systems : Recent research has been actively focusing on developing end-to-end systems that incorporate\nthe latest LLM and VLM technologies as encoders for input information. Particularly, there is a significant trend\ntowards modifying these foundation models to process multimodal information. (Jiang et al., 2022; Brohan et al., 2023,\n2022; Li et al., 2023d; Ahn et al., 2022b; Shah et al., 2023b; Li et al., 2023e). This adaptation aims to guide robotic\nactions based on both linguistic instructions and visual cues, thus achieving an effective embodiment.\nTask Planning and Skill Training : In contrast to end-to-end systems, Task And Motion Planning (TAMP) based\nsystems first compute a high-level task plan and then achieve them with low-level robot control, known as skills.\nThe advanced language processing abilities of LLMs have demonstrated the capability to interpret instructions and\ndecompose them into robot action steps, greatly advancing task planning technologies (Ni et al., 2023; Li et al., 2023b;\nParakh et al., 2023; Wake et al., 2023c). For skill training, several studies have explored the use of LLMs/VLMs\nfor designing reward functions (Yu et al., 2023a; Katara et al., 2023; Ma et al., 2023), generating data to facilitate\npolicy learning (Kumar et al., 2023; Du et al., 2023), or serving as part of a reward function (Sontakke et al., 2023).\nTogether with training frameworks such as RL and IL, these efforts will contribute to the development of efficient robot\ncontrollers.\nOn-site Optimization : Executing long task steps in robotics can be difficult due to unexpected and unpredictable\nenvironmental conditions. Therefore, a significant challenge in the field of robotics involves dynamically adapting and\nrefining robotic skills by integrating task plans with real-time environmental data. For instance, (Ahn et al., 2022b)\nproposed an approach that calculates the feasibility of actions (i.e., affordance) from visual information and compares it\nwith planned tasks. Additionally, there are approaches that focus on enabling LLMs to output the pre-conditions and\npost-conditions (e.g., states of objects and their interrelationships) of task steps to optimize their execution (Zhou et al.,\n2023c) and detect pre-condition errors for necessary revisions to the task plan (Raman et al., 2023). These strategies\nseek to achieve environment-grounded robot execution by integrating environmental information and adjusting the\nrobot\u2019s actions at the task plan or controller level.\nConversation Agents : In creating conversational robots, LLMs can contribute to natural, context-sensitive interactions\nwith humans (Ye et al., 2023a; Wake et al., 2023f). These models process and generate responses that mimic human\nconversation, allowing robots to participate in meaningful dialogues. Additionally, LLMs play a significant role in\nthe estimation of conceptual (Hensel et al., 2023; Teshima et al., 2022) and emotional attributes (Zhao et al., 2023;\nYang et al., 2023b; Wake et al., 2023d) of utterances. Those attributes facilitate the understanding of human intent and\nmeaningful gesture generation, thus contributing to the naturalness and efficacy of human-robot communication.", "start_char_idx": 0, "end_char_idx": 4259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7837387-ba7d-41b5-baa1-ed78eef23b3a": {"__data__": {"id_": "a7837387-ba7d-41b5-baa1-ed78eef23b3a", "embedding": null, "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8ac5fe3-8b1f-445f-a17c-8126f838aa7c", "node_type": "4", "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "8f2684842a35830d85182d9f9174b9da87211d7dad67d145f994e681e3ea91a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "374b597f-258e-41d9-8109-5fe078eb56b4", "node_type": "1", "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "cefa1a62b50be57c5684b2e8269cec7004de17d5bd5d5ad4375529f1cf51fc53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "063ec42a-d491-4d3b-af99-fc71da0fef33", "node_type": "1", "metadata": {}, "hash": "7de35f87f94b2c03428f34c054f2d6762f084e1abbef2bf69fb508ffb12c0c81", "class_name": "RelatedNodeInfo"}}, "text": "These strategies\nseek to achieve environment-grounded robot execution by integrating environmental information and adjusting the\nrobot\u2019s actions at the task plan or controller level.\nConversation Agents : In creating conversational robots, LLMs can contribute to natural, context-sensitive interactions\nwith humans (Ye et al., 2023a; Wake et al., 2023f). These models process and generate responses that mimic human\nconversation, allowing robots to participate in meaningful dialogues. Additionally, LLMs play a significant role in\nthe estimation of conceptual (Hensel et al., 2023; Teshima et al., 2022) and emotional attributes (Zhao et al., 2023;\nYang et al., 2023b; Wake et al., 2023d) of utterances. Those attributes facilitate the understanding of human intent and\nmeaningful gesture generation, thus contributing to the naturalness and efficacy of human-robot communication.\nNavigation Agents: Robot navigation has a long history of research, focusing on core aspects such as map-based path\nplanning and Simultaneous Localization and Mapping (SLAM) for creating environmental maps. These functionalities\nhave become standard in widely used robot middleware like the Robot Operating System (ROS) (Guimar\u00e3es et al.,\n2016).\nWhile classic navigation techniques remain prevalent in many robotics applications, they typically rely on static or\npre-created maps. Recently, there has been an increased interest in advanced technologies that enable robots to navigate\nin more challenging environments, leveraging breakthroughs in fields like computer vision and natural language\nprocessing. One representative task is object navigation (Chaplot et al., 2020a; Batra et al., 2020; Gervet et al., 2023;\nRamakrishnan et al., 2022; Zhang et al., 2021), where robots use object names for navigation instead of map coordinates,\nrequiring the visual grounding of object names in the environment. Furthermore, recent attention has been given to\n30", "start_char_idx": 3378, "end_char_idx": 5315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "063ec42a-d491-4d3b-af99-fc71da0fef33": {"__data__": {"id_": "063ec42a-d491-4d3b-af99-fc71da0fef33", "embedding": null, "metadata": {"page_label": "31", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04f3fca9-1f41-4558-a9d7-78676f89e1bf", "node_type": "4", "metadata": {"page_label": "31", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c7abbef0de02121a1be93b6648b3036d2cc6bc7c72ed22e05dc40b0ba018de7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7837387-ba7d-41b5-baa1-ed78eef23b3a", "node_type": "1", "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "94618e13610a01460db1d8db8b688d324baa5e48973916057545a6813c55de61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcce4ea6-d681-43e3-a336-5ea57dc78c55", "node_type": "1", "metadata": {}, "hash": "0599eedfa9a62314ae71c0108b028c6b90d4c0b313631777fe40d551ea4f8043", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 13: Overview of the robot teaching system that integrates a ChatGPT-empowered task planner. The process\ninvolves two steps: Task planning, where the user employs the task planner to create an action sequence and adjusts the\nresult through feedback as necessary, and Demonstration, where the user visually demonstrates the action sequence to\nprovide information needed for robot operation. The vision system collects visual parameters that will be used for robot\nexecution.\ntechnologies that navigate robots in entirely unfamiliar new environments on a zero-shot basis, on top of foundation\nmodels, so-called zero-shot object navigation (Gadre et al., 2023; Dorbala et al., 2023; Cai et al., 2023). Additionally,\nVision-Language Navigation (VLN) (Anderson et al., 2018a) is a representative task, where the task involves navigating\nan agent by natural language instructions in previously unseen, real-world environments (Shah et al., 2023a; Zhou et al.,\n2023a; Dorbala et al., 2022; Liang et al., 2023; Huang et al., 2023b). VLN interprets sentences rather than object names,\nsuch as \u201cgo to the bathroom on your left.,\u201d thus it requires a higher functionality to parse input text (Wang et al., 2019).\nThe advent of foundation models contributes to the development of such adaptive, on-the-fly navigation technologies by\nenhancing the understanding of human language instructions and the visual interpretation of environmental information.\nMore detailed explanations of representative VLN research are provided in 6.2.2.\n6.2.2 Experiments and Results.\nAn accumulating body of evidence suggests that recent VLMs and LLMs have promising capabilities for symbolic task\nplanning (e.g., what-to-do). However, each task requires low-level control policy (e.g., how-to-do) to achieve successful\ninteraction between the environment. While reinforcement learning and imitation learning are promising approach\nto learn policies in a data-driven manner, another promising approach is to obtain the strategy directly from humans\nthrough on-site demonstration, an approach called Learning-from-Observation (Wake et al., 2021a; Ikeuchi et al., 0). In\nthis section, we introduce a study where we employ ChatGPT for task planning and enrich the plan by parameterizing it\nwith affordance information to facilitate effective and precise execution (Fig. 13).\nThe pipeline was composed of two modules: task planning and parameterization. In task planning, the system is fed\nwith language instructions and the description of the working environment. These instructions, along with a predefined\nset of robot actions and output specifications, are compiled into a comprehensive prompt provided to ChatGPT, which\nthen generates a sequence of decomposed tasks with their textual descriptions (Fig. 13; left pane). Notably, we employ\na few-shot approach, meaning ChatGPT is not trained on this task, offering an advantage in applicability as it eliminates\nthe need for hardware-dependent data collection and model training. Additionally, the textual descriptions in the output\n31", "start_char_idx": 0, "end_char_idx": 3129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcce4ea6-d681-43e3-a336-5ea57dc78c55": {"__data__": {"id_": "fcce4ea6-d681-43e3-a336-5ea57dc78c55", "embedding": null, "metadata": {"page_label": "32", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e65f4928-49af-4ee6-bcb2-bf1517b76326", "node_type": "4", "metadata": {"page_label": "32", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "706a2850d7ff2a13bcb71760f51ed371872fbd3123ac55dc98d50935c18d38e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "063ec42a-d491-4d3b-af99-fc71da0fef33", "node_type": "1", "metadata": {"page_label": "31", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c7abbef0de02121a1be93b6648b3036d2cc6bc7c72ed22e05dc40b0ba018de7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "604c584c-ea8d-4dab-bd4c-7804f0945aa7", "node_type": "1", "metadata": {}, "hash": "d3deba74ed8e067eb3b5218656c19edeb794afdae4dd2c98000a49c2ec26ead2", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 14: Example of adjusting an output sequence through auto-generated feedback. We use an open-sourced\nsimulator, VirtualHome for the experiment. Given an instruction \u201cTake the pie on the table and warm it using the stove.,\u201d\nthe task planner plans a sequence of functions that are provided in VirtualHome. If an error in execution is detected, the\ntask planner correct its output based on the auto-generated error message.\nenable the user to check and adjust the results as necessary, which is a crucial feature for a safe and robust operation.\nFig. 14 shows the qualitative results conducted for an agentic simulation on top of VirtualHome (Puig et al., 2018). The\nresults demonstrate a reasonable task plan and its flexibility in adjusting outputs, indicating the broad applicability of\nour approach.\nWhile the task planner guarantees coherency between the task sequences, successful operation in reality requires\ndetailed parameters. For example, grasp type is crucial for carrying a container while spilling out the content, such\na parameter is often ignored in a simulators (see Fig. 14 in grasping a pie). In our robot system, therefore, users are\nasked to demonstrate each action visually (Fig. 13; right pane). The tasks had predefined parameters necessary for\nexecution, which our vision system extracts from the videos (Wake et al., 2021b). Notably, our robotic system is not\ndesigned for exact replication of human motions (i.e., teleoperation) but rather to handle varying real-world conditions,\nsuch as changes in object locations. Hence, the parameters extracted from human demonstrations encompass not\nprecise motion paths but affordance information that dictates effective environmental movement (e.g., waypoints for\ncollision avoidance (Wake et al., 2023a), grasp types (Wake et al., 2023e), and upper-limbs postures (Sasabuchi et al.,\n2021; Wake et al., 2021a)). The posture of the upper limbs is critical in robots with high degrees of freedom and is\ndesigned to assume predictable postures for humans coexisting with the operational robot. The task sequence endowed\nwith affordances is transformed into a sequence of reusable robot skills acquired through reinforcement learning and\nexecuted by the robot (Takamatsu et al., 2022).\nLLM-empowered task planning can be extended to a more versatile robotic system by integrating it with VLMs. Here,\nwe show an example where we use the GPT-4V(ision) to broaden the aforementioned task planner in a multimodal input\ncontext (Fig. 15), a human performs actions that are intended to be replicated by the robot. In this paper, only part of\nthe prompt is shown. The whole prompt is available at microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts.\nThis pipeline takes demonstration videos and text, then outputs a sequence of robot actions. A vision analyzer aims\nto understand the actions performed by humans in the video. We used GPT-4V and provided a prompt to generate\ntext instructions in a style typical of human-to-human communication.Fig. 16 demonstrates how the usage of text\n32", "start_char_idx": 0, "end_char_idx": 3128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "604c584c-ea8d-4dab-bd4c-7804f0945aa7": {"__data__": {"id_": "604c584c-ea8d-4dab-bd4c-7804f0945aa7", "embedding": null, "metadata": {"page_label": "33", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d67bf198-a220-4934-8ea7-57b9a7c9ecdc", "node_type": "4", "metadata": {"page_label": "33", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e95684ec5355375757ca1c2bad6d0615678397660ff5661ff2edee400e7826b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcce4ea6-d681-43e3-a336-5ea57dc78c55", "node_type": "1", "metadata": {"page_label": "32", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "706a2850d7ff2a13bcb71760f51ed371872fbd3123ac55dc98d50935c18d38e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bb2238f-9999-4421-9590-421a863d9ac9", "node_type": "1", "metadata": {}, "hash": "1124550fac849faea5b03d2ab10b151fe7037374b7107c779267a130f7f64492", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 15: Overview of the multimodal task planner that leverages GPT-4V and GPT-4. The system processes video\ndemonstrations and text instructions, generating task plans for robotic execution.\nFigure 16: Examples of the output of the video analyzer. The five frames are extracted at regular intervals and fed into\nGPT-4V . We describe the entire pipeline in Section 6.2.2.\ninput allows user to give feedback on GPT-4V\u2019s recognition results for correction purposes. Such a feature, aiming at\nimproving the accuracy of the recognition results, also enables more robust operation.\nNext, the scene analyzer compiles the expected work environment into the text information based on the instructions\nand the first frame of the video data (or an image of the environment). This environmental information includes a\nlist of object names recognized by GPT-4V , the graspable properties of objects, and the spatial relationships between\nobjects. Although these computational processes are a black box within GPT-4V , the information is output based on\nthe knowledge of GPT-4V and the image/text input. Fig. 17 shows the example outputs of our scene analyzer. As\nshown in the figure, GPT-4V successfully selects the objects that are related to the manipulation. For example, a table is\nincluded in the output when the human is relocating a spam container on the table, while the table is ignored for the\nfridge opening task. These results suggest that the scene analyzer encodes the scene information with respect to the\nhuman\u2019s actions. We prompted GPT-4V to explain the results of the object selection process and the reasons behind\nthose choices. In practice, we found this approach resulted in reasonable outputs. Finally, based on the given text\ninstructions and environmental information, the task planner outputs a sequence of tasks (Wake et al., 2023c).\n33", "start_char_idx": 0, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bb2238f-9999-4421-9590-421a863d9ac9": {"__data__": {"id_": "6bb2238f-9999-4421-9590-421a863d9ac9", "embedding": null, "metadata": {"page_label": "34", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1b36c56-23a4-4da0-888e-b5698d51aad7", "node_type": "4", "metadata": {"page_label": "34", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e81d64838f5c06f1cfdbecca6b67ef06d790c940b50b5ba615d0efda2672aaba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "604c584c-ea8d-4dab-bd4c-7804f0945aa7", "node_type": "1", "metadata": {"page_label": "33", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e95684ec5355375757ca1c2bad6d0615678397660ff5661ff2edee400e7826b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a20b837b-4842-4885-95cb-bcdb8f1ebdab", "node_type": "1", "metadata": {}, "hash": "83150f712c2b101a4c03c12e157da7e1d8d4a2ad67eb35b349b560d266849e84", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 17: Examples of the outputs of the scene analyzer that leverages GPT-4V . We describe our entire pipeline in\nSection 6.2.2.\nEmbodied Agents for Robotics Navigation. Vision-language navigation (VLN) is the task of navigating an embodied\nagent to carry out natural language instructions inside real 3D environments. Navigation in 3D environments (Zhu\net al., 2017a; Mirowski et al., 2016; Mousavian et al., 2018; Hemachandra et al., 2015) is an essential capability of a\nmobile intelligent system that functions in the physical world. In the past few years, a plethora of tasks and evaluation\nprotocols (Savva et al., 2017; Kolve et al., 2017; Song et al., 2017; Xia et al., 2018; Anderson et al., 2018a) have been\nproposed as summarized in (Anderson et al., 2018b). VLN (Anderson et al., 2018a) focuses on language-grounded\nnavigation in the real 3D environment. In order to solve the VLN task, (Anderson et al., 2018a) set up an attention-based\nsequence-to-sequence baseline model. Then (Wang et al., 2018) introduced a hybrid approach that combines model-free\nand model-based reinforcement learning (RL) to improve the model\u2019s generalizability. Lastly, (Fried et al., 2018)\nproposed a speaker-follower model that adopts data augmentation, a panoramic action space and modified beam search\nfor VLN, establishing the current state-of-the-art performance on the Room-to-Room dataset. Extending prior work, we\npropose a Reinforced Cross-Modal Matching (RCM) for VLN in (Wang et al., 2019). The RCM model is built upon\n(Fried et al., 2018) but differs in many significant aspects: (1) RCM combines a novel multi-reward RL with imitation\nlearning for VLN while Speaker-Follower models (Fried et al., 2018) only uses supervised learning as in (Anderson\net al., 2018a). (2) The RCM reasoning navigator performs cross-modal grounding rather than the temporal attention\nmechanism on single-modality input. (3) The RCM matching critic is similar to the Speaker in terms of the architecture\ndesign, but the former is used to provide the cycle-reconstruction intrinsic reward for both RL and SIL training while\nthe latter is used to augment training data for supervised learning. In (Wang et al., 2019), we study how to address three\ncritical leader-board for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problem.\nAs shown in Fig. 18, we propose a novel Reinforced Cross-Modal Matching approach that enforces cross-modal\n34", "start_char_idx": 0, "end_char_idx": 2529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a20b837b-4842-4885-95cb-bcdb8f1ebdab": {"__data__": {"id_": "a20b837b-4842-4885-95cb-bcdb8f1ebdab", "embedding": null, "metadata": {"page_label": "35", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5bd20468-a0d4-46cd-a56e-60f1dcea63c4", "node_type": "4", "metadata": {"page_label": "35", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0e70c33193fc985d952f55d72a5f83c5984e0b06ecf3a57edd263eee922272ff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bb2238f-9999-4421-9590-421a863d9ac9", "node_type": "1", "metadata": {"page_label": "34", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e81d64838f5c06f1cfdbecca6b67ef06d790c940b50b5ba615d0efda2672aaba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc550ec8-ce7c-46dc-87df-4f97e580b1ce", "node_type": "1", "metadata": {}, "hash": "30990a5b4f523d78c909a5a9e02c5af81ea35411797d6dfcbcc2caeb80be8ba9", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nTurnrightandheadtowardsthekitchen.Thenturnleft,passatableandenterthehallway.Walkdownthehallwayandturnintotheentrywaytoyourrightwithoutdoors.Stopinfrontofthetoilet.\nLocal\tvisual\tscene\tInstruction\nDemonstration\tPath\tAExecuted\tPath\tBExecuted\tPath\tC\tInitial\tPositionTarget\tPositionGlobal\ttrajectories\tin\ttop-down\tview\t\nFigure 18: Demonstration of embodied agent for the VLN task (Wang et al., 2019). The instruction, the local visual\nscene, and the global trajectories in a top-down view is shown. The agent does not have access to the top-down view.\nPath A is the demonstration path following the instruction. Path B and C are two different paths executed by the agent.\ngrounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide\nan intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is\nemployed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows\nthat our RCM model significantly outperforms previous methods by 10% on SPL and achieved a new state-of-the-art\nperformance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation\nLearning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that\nSIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance\ngap between seen and unseen environments (from 30.7% to 11.7%). Moreover, in (Wang et al., 2019) we introduce a\nself-supervised imitation learning method for exploration in order to explicitly address the generalization issue, which\nis a problem not well-studied in prior work. Concurrent to the work, (Thomason et al., 2018; Ke et al., 2019; Ma et al.,\n2019a,b) studies the VLN tasks from various aspects, and (Nguyen et al., 2018) introduces a variant of the VLN task to\nfind objects by requesting language assistance when needed. Note that we are the first to propose to explore unseen\nenvironments for the VLN task.\n6.3 Healthcare\nIn healthcare, LLMs and VLMs can act as diagnostic agents, patient care assistants, or even therapy aids, but they come\nwith unique leader-board and responsibilities. With the tremendous potential for AI agents to improve patient care and\nsave lives comes an equally dangerous possibility that their misuse or hasty deployment could endanger thousands or\nmillions of people worldwide. We discuss some of the promising routes for AI agents within the context of healthcare\nand also discuss some of the key leader-board faced.\nDiagnostic Agents. Using LLMs as medical chatbots for patient diagnosis has recently attracted great attention due to\nthe high-demand for medical experts and the potential for LLMs to help triage and diagnose patients (Lee et al., 2023).\nDialogue agents, especially those that can effectively communicate important medical information to a broad range\nof people from diverse patient populations, have the potential to provide equitable healthcare access to historically\ndisadvantaged or marginalized groups. Furthermore, doctors and healthcare systems across the world are largely\nover-burdened and under-resourced, resulting in insufficient access to medical care for hundreds of millions of people\nworldwide (World Health Organization and World Bank, 2015). Diagnostic agents provide a particularly advantageous\npathway to improve healthcare for millions since they have they can be built with the capability to understand a variety\n35", "start_char_idx": 0, "end_char_idx": 3644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc550ec8-ce7c-46dc-87df-4f97e580b1ce": {"__data__": {"id_": "fc550ec8-ce7c-46dc-87df-4f97e580b1ce", "embedding": null, "metadata": {"page_label": "36", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54ebdb15-9f28-41b0-806d-a5c4f2a1f802", "node_type": "4", "metadata": {"page_label": "36", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "492b9b24027e03763858ecedcb798d1667207f7642573cede0392127a68ede29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a20b837b-4842-4885-95cb-bcdb8f1ebdab", "node_type": "1", "metadata": {"page_label": "35", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0e70c33193fc985d952f55d72a5f83c5984e0b06ecf3a57edd263eee922272ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ad53d7f-65a4-4999-97c9-eb593ff33d46", "node_type": "1", "metadata": {}, "hash": "fd4c93f4309f73c27ce8ba5f8ef4e943e8c573ebaa4d6c4ee0bc19da6cf58bfc", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nof languages, cultures, and health conditions. Initial results have shown that healthcare-knowledgeable LMMs can be\ntrained by utilizing large-scale web data (Li et al., 2023f). Although an exciting direction, the promise of diagnostic\nagents does not come without risks. We highlight the risks of hallucination within medical contexts, as well as potential\npathways for solutions in the following section.\nKnowledge Retrieval Agents. Within the medical context, model hallucinations are particularly dangerous and may\neven result in serious patient harm or death, depending on the severity of the error. For instance, if a patient mistakenly\nreceives a diagnosis suggesting they are free of a condition they actually have, it can lead to catastrophic outcomes.\nThese include postponed or inappropriate treatments, or in some cases, a total lack of necessary medical intervention.\nThe gravity of undiagnosed or misdiagnosed conditions can lead to escalated healthcare expenses, extended therapies\ncausing further physical strain, and in extreme scenarios, severe harm or even death. Thus, approaches that can use\nagents to more reliably retrieve knowledge (Peng et al., 2023) or generate text in a retrieval-based manner (Guu et al.,\n2020) are promising directions. Pairing a diagnostic agent with a medical knowledge retrieval agent has the potential to\nsignificantly reduce hallucinations while simultaneously improving the quality and preciseness of the responses of the\ndiagnostic dialogue agent.\nTelemedicine and Remote Monitoring. Agent-based AI also has great potential within the world of Telemedicine and\nRemote Monitoring by improving the access to healthcare, improving communications between healthcare providers\nand patients, as well as improving the efficiency and reducing the costs of frequent doctor-patient interactions (Amjad\net al., 2023). Primary care clinicians spend significant amounts of time sifting through patient messages, reports, and\nemails that are often irrelevant or unnecessary for them to view. There is significant potential to allow for support agents\nto help triage messages from doctors, patients, and other healthcare providers and to help highlight important messages\nfor all parties. By enabling agentic AI systems to coordinate with patients, clinicians, and other AI agents, there is a\nmassive potential to revolutionize the remote healthcare and digital health industry.\n6.3.1 Current Healthcare Capabilities\nImage understanding. We demonstrate the current capabilities and limitations of modern multimodal agents such as\nGPT-4V within the context of healthcare in Fig. 19. We can see that although GPT-4V possesses significant internal\nknowledge of the equipment and procedures involved in hospital care, it does not always respond to more prescriptive\nor diagnostic queries by the user.\nVideo understanding. We investigate the performance of VLM agents for medical video understanding in two\ncontexts. First, we investigate the ability for VLM agents to identify important patient care activities in clinical spaces.\nSecondly, we explore the usage of of VLMs for more technical videos such as ultrasounds. Specifically, in Figure 20,\nwe demonstrate some of the current capabilities and limitations of GPT-4V for hospital care and medical video analysis.\n6.4 Multimodal Agents\nThe integration of visual and linguistic understanding is crucial for developing sophisticated multimodal AI agents. This\nincludes tasks such as image captioning, visual question answering, video language generation, and video understanding,\namongst others. We aim to delve into these visual-language tasks, exploring the leader-board and opportunities they\npresent in the context of AI agents.\n6.4.1 Image-Language Understanding and Generation\nImage-language understanding is a task that involves the interpretation of visual content in a given image with language\nand the generation of associated linguistic descriptions. This task is critical to the development of AI agents that can\ninteract with the world in a more human-like manner. Some of most popular ones are image captioning (Lin et al., 2014;\nSharma et al., 2018; Young et al., 2014; Krishna et al., 2016), referring expression (Yu et al., 2016; Karpathy et al.,\n2014), and visual question answering (Antol et al., 2015; Ren et al., 2015; Singh et al., 2019).\nMore recently, knowledge-intensive Visual Question Answering tasks such as OKVQA (Marino et al., 2019), KB-\nVQA (Wang et al., 2015), FVQA (Wang et al., 2017), and WebQA (Chang et al., 2021) have been introduced. Multimodal\n36", "start_char_idx": 0, "end_char_idx": 4643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ad53d7f-65a4-4999-97c9-eb593ff33d46": {"__data__": {"id_": "3ad53d7f-65a4-4999-97c9-eb593ff33d46", "embedding": null, "metadata": {"page_label": "37", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15dac06e-2c16-4bcb-8f39-bf2e67c2c0e7", "node_type": "4", "metadata": {"page_label": "37", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "193703c36af24f1c803c8f1b60e4554a5c688a90453412a8d43342f1db2701b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc550ec8-ce7c-46dc-87df-4f97e580b1ce", "node_type": "1", "metadata": {"page_label": "36", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "492b9b24027e03763858ecedcb798d1667207f7642573cede0392127a68ede29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cc1aac5-103f-463c-9e0b-2f4fd891f4a2", "node_type": "1", "metadata": {}, "hash": "0dba0b4574ee6964eac13589fd76acc00948426aed7a5fce7764860fca37162f", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 19: Example prompts and responses when using GPT-4V within the domain of healthcare image understanding.\nFrom left to right: (1) an image of a nurse and doctor conducting a CT scan, (2) a synthetic image of an irregular EKG\nscan, and (3) an image from the ISIC (Codella et al., 2018) skin lesion dataset. We can see that GPT-4V possesses\nsignificant medical knowledge and is able to reason about medical images. However, due to safety training, it is unable\nto make diagnoses for some medical images.\nagents should capable of identifying objects in an image, comprehending their spatial relationships, generating accurate\ndescriptive sentences about the scene, and utilizing reasoning skills to handle knowledge-intensive visual reasoning.\nThis requires not just object recognition capabilities, but also a deep understanding of spatial relationships, visual\nsemantics, and the ability to map these visual elements to linguistic constructs with integration of the world knowledge.\n6.4.2 Video and Language Understanding and Generation\nVideo-language generation. Video captioning or video storytelling is the task of generating a sequence of coherent\nsentences for a stream of video frames. Inspired by the successful use of recurrent large foundation models employed\nin video and language tasks, variants of agent driven enhanced models have shown promising results on the task of\nvideo-lanaguage generation. The fundamental challenge is that the strong performance of neural encoder-decoder\nmodels does not generalize well for visual storytelling, because the task requires a full understanding of the content of\neach image as well as the relation among different frames. One important goal for the field is to create an agent-aware\ntext-synthesis model that can efficiently encode the sequence of frames and generate a topically coherent multi-sentence\nparagraph.\nVideo Understanding. Video understanding extends the scope of image understanding to dynamic visual content. This\ninvolves interpretation and reasoning about the sequence of frames in a video, often in conjunction with accompanying\naudio or textual information. An agent should be able interact with various modalities from visual, text, and also\naudio modalities to demonstrate their advanced comprehension of video content. Tasks in this domain include video\ncaptioning, video question answering, and activity recognition, amongst others. The leader-board in video understanding\nare manifold. They include the temporal alignment of visual and linguistic content, the handling of long sequences of\nframes, and the interpretation of complex activities that unfold over time. Regarding audio, the agent could process\nspoken words, background noises, music, and tone of voice to comprehend the mood, setting, and subtleties of the\nvideo content.\n37", "start_char_idx": 0, "end_char_idx": 2890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cc1aac5-103f-463c-9e0b-2f4fd891f4a2": {"__data__": {"id_": "1cc1aac5-103f-463c-9e0b-2f4fd891f4a2", "embedding": null, "metadata": {"page_label": "38", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0446c95b-6313-4390-94b6-e09f81e40077", "node_type": "4", "metadata": {"page_label": "38", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "249373e07cda350d98a30d352cd8d5181e7e3bb5a185be503c9013ca28f199a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ad53d7f-65a4-4999-97c9-eb593ff33d46", "node_type": "1", "metadata": {"page_label": "37", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "193703c36af24f1c803c8f1b60e4554a5c688a90453412a8d43342f1db2701b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d980368-43e5-4a48-84c1-a9cd2a4879fa", "node_type": "1", "metadata": {}, "hash": "18173043eafb753678681830cef9d010c69ce757249f7b2ff813892b7572e973", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nWhich clinical bedside activity is \noccurring in the video? Which clinical bedside activity is \noccurring in the video? \nWhat is happening in the video? \nThe images you've shared appear \nto be frames from a video that \ndemonstrates the process of \nturning a patient in bed , likely to \nprevent bedsores or for comfort. \nThis is a common procedure in \nhospital settings, where patients \nmay be immobile or bedridden for \nextended periods. The activity is \ntypically done by healthcare \nprofessionals to maintain the \npatient's skin integrity and to \npromote circulation. It involves \nmoving a patient from one side to \nthe other and adjusting their \nposition in bed. \nThe sequence of images appears \nto show a healthcare provider \nassisting a patient with a standing \npivot transfer from a hospital \nbed to a chair or wheelchair . \nThis is a common bedside activity \nin healthcare settings, where a \npatient who has limited mobility \nneeds assistance to be safely \nmoved from the bed to a chair\u2026 \nThe image you've uploaded \nappears to be a compilation of \nfour frames from an ultrasound \nvideo , showing the progression of \nframes as indicated by the labels \nFrame 0, Frame 1, Frame 2, and \nFrame 3. Ultrasound imaging is a \ncommon diagnostic tool used in \nmedicine to visualize internal\u2026 \nThis is an echocardiogram, do you \nnotice anything abnormal? \nAs an AI, I'm not quali\ufb01ed to \ninterpret medical images such as \nechocardiograms\u2026 \nFigure 20: Example prompts and responses when using GPT-4V within the domain of healthcare video understanding.\nWe input the example videos as 2x2 grids with overlaid text indicating the order of frames. In the first two examples, we\nprompt GPT-4V to examine the frames in the video to detect the clinical bedside activities performed on the volunteer\npatients. For the final example, we attempt to prompt GPT-4V to assess an echocardiogram video, however due to\nGPT-4V\u2019s safety training, it does not provide a detailed response. For clarity, we bold text that describes the activity of\ninterest, and abbreviate model responses that are unnecessary. We gray-out faces from the individuals to preserve their\nprivacy.\nFigure 21: Interactive multimodal agents include four main pillars: Interaction, Speech, Vision, and Language. Co-pilot\nagents are made up of different services. 1) Interaction services help make a unified platform for automated actions,\ncognition, and decision-making. 2) Audio services integrate audio and speech processing into apps and services. 3)\nVision services identify and analyze content within images, videos, and digital ink. 4) Language services extract\nmeaning from structured and unstructured text.\n38", "start_char_idx": 0, "end_char_idx": 2737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d980368-43e5-4a48-84c1-a9cd2a4879fa": {"__data__": {"id_": "6d980368-43e5-4a48-84c1-a9cd2a4879fa", "embedding": null, "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f35eb9bf-0ac9-47f3-837c-3c7d846a7236", "node_type": "4", "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "130bcdb9454f373b0fa3a88489e76359637c2224c2bd7ceaef6464be5f51253c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc1aac5-103f-463c-9e0b-2f4fd891f4a2", "node_type": "1", "metadata": {"page_label": "38", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "249373e07cda350d98a30d352cd8d5181e7e3bb5a185be503c9013ca28f199a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c607c463-e403-41d9-8627-ade64ae4bed2", "node_type": "1", "metadata": {}, "hash": "a9d723006a91ec9d58d0e1269ea5a0c56d282d38e85f54138fb057404a8c653a", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nPrevious works have focused on employing existing video-language training data available online for establishing video\nfoundational models (Li et al., 2020, 2021b; Fu et al., 2022; Bain et al., 2021; Zellers et al., 2021, 2022; Fu et al., 2023).\nSupporting such training pipelines and functionalities is, however, difficult due to the limited and often inconsistent\nnature of these datasets. Video foundational models are designed with masked and contrastive pretraining objectives\nand later tuned on their respective tasks. Despite showing remarkable results in multimodal benchmarks, these models\nencounter difficulties in video-only tasks such as action recognition due to their dependency on limited video-text data\nbuilt from noisy audio transcriptions. This limitation also leads to the lack of robustness and fine-grained reasoning\nskills that large language models generally possess.\nOther methods, similar to those used in image-language understanding, have drawn on the strong reasoning skills\nand broad knowledge of large language models to improve different facets of video interpretation. The task of video\nunderstanding is simplified by language only models like ChatGPT and GPT4 or image-language models like GPT4-V ,\nwhich treat the audio, video, and language modalities as individual interpretable input data types and position the agents\nas strong open-source models. For example, (Huang et al., 2023c; Li et al., 2023g) transformed video understanding into\na natural language processing (NLP) question-answering formulation by textualizing video content with open-source\nvision classification/detection/caption models. (Lin et al., 2023) integrated GPT4-V with specialized tools in vision,\naudio, and speech, to facilitate complex video understanding tasks, such as scripting character movements and actions\nin long-form videos.\nParallel research explores generating scaled datasets from large models, then applying visual instruction tuning (Liu\net al., 2023c; Li et al., 2023c; Zhu et al., 2023) on the generated data. Considerable audio, speech, and visual expert\nperception models are subsequently used to verbalize videos. Speech is transcribed with automatic speech recognition\ntools, and video descriptions and related data are produced with various tagging, grounding, and captioning models (Li\net al., 2023g; Maaz et al., 2023; Chen et al., 2023; Wang et al., 2023f). These techniques demonstrate how instruction\ntuning video-language models on generated datasets may lead to enhanced video-reasoning and communication abilities.\n6.4.3 Experiments and Results\n\u2022Knowledge-Intensive Models: As introduced in INK (Park et al., 2022), and KAT (Gui et al., 2022a),\nan intensive neural knowledge task that incorporates required knowledge annotated by humans to support\nknowledge-intensive retrieval task.\n\u2022Multimodal-Agents: There has been a growing interest in multimodal language models like Chameleon (Lu\net al., 2023) and MM-React (Yang et al., 2023c).\n\u2022Visual Instruction Tuning: VCL(Gui et al., 2022b), Mini-GPT4 (Zhu et al., 2023), MPLUG-OWL (Ye et al.,\n2023b), LSKD (Park et al., 2023c) generate image-level instruction tuning dataset.\nKnowledge-Intensive Agent. As showed in Fig. 22 and Fig. 23, Knowledge-based visual question answering and\nvision-language retrieval tasks are challenging tasks in multi-modal machine learning that requires outside knowledge\nbeyond image contents. Recent studies on large-scale transformers have primarily focused on maximizing the efficiency\nof the model\u2019s parameters to store information. This line of research explores a different aspect: whether multimodal\ntransformers can use explicit knowledge in their decision-making process. Pretraining methods based on transformers\nhave shown remarkable success in implicitly learning knowledge representations across multiple modalities. However,\ntraditional methods, mainly unimodal, have investigated knowledge retrieval and subsequent answer prediction, raising\nquestions about the quality and relevance of the knowledge retrieved and the integration of reasoning processes using\nboth implicit and explicit knowledge. To tackle these issues, we introduce the Knowledge Augmented Transformer\n(KAT), which outperforms others by 6% on the 2022 OK-VQA open-domain multimodal task.", "start_char_idx": 0, "end_char_idx": 4362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c607c463-e403-41d9-8627-ade64ae4bed2": {"__data__": {"id_": "c607c463-e403-41d9-8627-ade64ae4bed2", "embedding": null, "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f35eb9bf-0ac9-47f3-837c-3c7d846a7236", "node_type": "4", "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "130bcdb9454f373b0fa3a88489e76359637c2224c2bd7ceaef6464be5f51253c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d980368-43e5-4a48-84c1-a9cd2a4879fa", "node_type": "1", "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "4c563898ac91faa7f1cbff628d82fa1a2f7714f3746403418c7a91186b4fc8c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f607a44-8299-4331-9afe-f4fb156e6ce9", "node_type": "1", "metadata": {}, "hash": "9cc033e937584b9ff61b796d8265c2e77690359faa504f576d299b770ee7baf9", "class_name": "RelatedNodeInfo"}}, "text": "22 and Fig. 23, Knowledge-based visual question answering and\nvision-language retrieval tasks are challenging tasks in multi-modal machine learning that requires outside knowledge\nbeyond image contents. Recent studies on large-scale transformers have primarily focused on maximizing the efficiency\nof the model\u2019s parameters to store information. This line of research explores a different aspect: whether multimodal\ntransformers can use explicit knowledge in their decision-making process. Pretraining methods based on transformers\nhave shown remarkable success in implicitly learning knowledge representations across multiple modalities. However,\ntraditional methods, mainly unimodal, have investigated knowledge retrieval and subsequent answer prediction, raising\nquestions about the quality and relevance of the knowledge retrieved and the integration of reasoning processes using\nboth implicit and explicit knowledge. To tackle these issues, we introduce the Knowledge Augmented Transformer\n(KAT), which outperforms others by 6% on the 2022 OK-VQA open-domain multimodal task. KAT combines implicit\nknowledge from GPT3 with explicit knowledge from websites using an encoder-decoder structure, and allows for\nconcurrent reasoning with both knowledge types during answer generation. Furthermore, incorporating explicit\nknowledge enhances the interpretability of the model\u2019s predictions. The code and pre-trained models are available at\nhttps://github.com/guilk/KAT .\nVision-language Transformer Agent. Next, we introduce the \"Training Vision-Language Transformers from Cap-\ntions\" (VLC) model (Gui et al., 2022b), a transformer that has been pretrained exclusively with image-caption pairs.\nDespite using just a simple linear projection layer for image embeddings, VLC attains competitive results across various\nvision-language tasks, in contrast to other methods that depend on object detectors or supervised CNN/ViT networks.\n39", "start_char_idx": 3282, "end_char_idx": 5214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f607a44-8299-4331-9afe-f4fb156e6ce9": {"__data__": {"id_": "1f607a44-8299-4331-9afe-f4fb156e6ce9", "embedding": null, "metadata": {"page_label": "40", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d0789f8-f0c1-4acd-8c77-319eb8b019c4", "node_type": "4", "metadata": {"page_label": "40", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "07fe2f01d9a0cd76ce749894c9ed0e34b347f21f4b7dd177e332f87ebec776e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c607c463-e403-41d9-8627-ade64ae4bed2", "node_type": "1", "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "10ff40982610c52c3afcc3b0f58cf7b152bb6293ace1b0ba161fefeab27e0b92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c3e12b0-2d14-4d79-8562-f0656b34b158", "node_type": "1", "metadata": {}, "hash": "adebd436af7297c0e229946236201e813a3d1d3e960915a57a219a02f3dcb968", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 22: Example of Intensive Neural Knowledge (INK) (Park et al., 2022) task that uses knowledge to identify text\nrelevant to the image from a set of text candidates. Our task involves leveraging visual and text knowledge retrieved\nfrom web and human-annotated knowledge.\nFigure 23: The KAT model (Gui et al., 2022a) uses a contrastive-learning-based module to retrieve knowledge entries\nfrom an explicit knowledge base and uses GPT-3 to retrieve implicit knowledge with supporting evidence. The\nintegration of knowledge is processed by the respective encoder transformer and jointly with reasoning module and the\ndecoder transformer via end-to-end training for answer generation.\n40", "start_char_idx": 0, "end_char_idx": 757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c3e12b0-2d14-4d79-8562-f0656b34b158": {"__data__": {"id_": "1c3e12b0-2d14-4d79-8562-f0656b34b158", "embedding": null, "metadata": {"page_label": "41", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08828317-a4c9-4175-bb99-72783dd7978e", "node_type": "4", "metadata": {"page_label": "41", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "34e8660971f4b353853202944454c529d855bd97187fb38a713ddeba562de9ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f607a44-8299-4331-9afe-f4fb156e6ce9", "node_type": "1", "metadata": {"page_label": "40", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "07fe2f01d9a0cd76ce749894c9ed0e34b347f21f4b7dd177e332f87ebec776e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85835b48-0796-44f6-bb10-7a0464242cb3", "node_type": "1", "metadata": {}, "hash": "459cc371b4cabf359568c84932076d9c7f1c339a2b6c4f54d9d85c39f521a567", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nMulti-layerTransformerMLMheadMIMheadITMhead\nTheyellowandbluebirdisstandingonabranch\u2026a[MASK]The[CLS]Wordembedding\n\u2026\n\u2026\n\u202601\u2026T12\u2026K00001111LinearProjectionMatchornotbranch\nPatchmasks[MASK]TokenmasksTokenpositionembeddingModality-specificembeddingPatchpositionembedding\nModality-specificProjectionMultimodalEncoderTask-specificDecoder\nFigure 24: The overall architecture of the VLC model (Gui et al., 2022b). Our model consists of three modules: (1)\nModality-specific projection. We use a simple linear projection to embed patched images and a word embedding\nlayer to embed tokenized text; (2) Multi-modal encoder. We use a 12-layer ViT (Dosovitskiy et al., 2021) initialized\nfrom MAE (He et al., 2022) (ImageNet-1K without labels) as our backbone; (3) Task-specific decoder. We learn our\nmulti-modal representations by masked image/language modeling and image-text matching which are only used during\npre-training. We use a 2-layer MLP to fine-tune our multi-modal encoder for downstream tasks. Importantly, we find\nthat the masked image modeling objective is important throughout second-stage pre-training, not only for initialization\nof the visual transformer.\nThrough extensive analysis, we explore the potential of VLC as a vision-language transformer agent. For instance,\nwe show that VLC\u2019s visual representations are highly effective for ImageNet-1K classification, and our visualizations\nconfirm that VLC can accurately match image patches to corresponding text tokens. The scalability of performance\nwith more training data highlights the promising potential for developing large-scale, weakly-supervised, open-domain\nvision-language models.\n6.5 Video-language Experiments\nTo understand the practicality of converting pre-trained image-LLMs for video understanding, we temporally expand\nand fine-tune InstructBLIP (Dai et al., 2023) for video captioning. Specifically, we expand the visual encoder of\nInstructBLIP (EV A-CLIP-G (Sun et al., 2023b)) using the same divided space-time attention scheme as Frozen in Time\n(Bain et al., 2021) and keep the Q-former and LLM (Flan-T5-XL (Chung et al., 2022)) frozen during training. We\nfreeze all spatial layers of the visual encoder, while keeping the temporal layers unfrozen during captioning training.\nThis allows for our model to take image and videos as input (matching the image-level performance of InstructBLIP).\nWe train on a 5 million video-caption subset of WebVid10M (Bain et al., 2021). We visualize two example outputs in\nFigure 25. However, existing agents fail to fully comprehend precise, fine-grained visual details in the video content. A\nsimilar limitation is seen by visual instruction tuning methods, where they lack the general, human-level perception\nabilities that are remain to be solved by multimodal models and agents.\nThe instruction-tuned models show promise in accurately summarizing visible actions within videos and identifying\nactions like \"person sitting on a bench\" effectively in Fig. 25. However, they sometimes add incorrect details, such as\n\"person smiling to the camera,\" revealing a shortfall in capturing conversation topics or the video\u2019s ambiance, elements\nthat are readily apparent to human observers. This shortfall underscores another key limitation: the omission of audio\nand speech modalities that would enrich the video understanding with context, aiding in more accurate interpretation and\npreventing such misrepresentations. Bridging this gap requires a holistic integration of available modalities, allowing\nmultimodal agents to reach a level of comprehension akin to human perception and ensuring a fully multimodal\napproach to video interpretation.\n41", "start_char_idx": 0, "end_char_idx": 3724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85835b48-0796-44f6-bb10-7a0464242cb3": {"__data__": {"id_": "85835b48-0796-44f6-bb10-7a0464242cb3", "embedding": null, "metadata": {"page_label": "42", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00455c0b-f906-40f7-b5b7-c7dee663cb48", "node_type": "4", "metadata": {"page_label": "42", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c7fd3ea27492311d031e751d1d29c1d4182bfac2f9f22ddfccee0992deb15d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c3e12b0-2d14-4d79-8562-f0656b34b158", "node_type": "1", "metadata": {"page_label": "41", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "34e8660971f4b353853202944454c529d855bd97187fb38a713ddeba562de9ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84112dac-86dd-459a-a869-64404be4bb1b", "node_type": "1", "metadata": {}, "hash": "07cac5ac0d0918386ef8af59e63354640991a06b9c0c4cecf72468e961cfa572", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nWhat would you recommend for this person to do to \nfeel better? \nIn the image, there is a person sitting on a bench in a \npark with a friend. The person is wearing a blue shirt and \na pair of blue jeans. The person is holding a bottle of \nwater in one hand and a cup of coffee in the other . The \nperson is looking at the camera and smiling to the \ncamera. The person may be feeling depressed, anxious, \nor stressed due to a recent event or situation. The person \nmay want to take a break from their busy schedule and \nrelax for a while. They may want to take a walk, read a \nbook, listen to music, or take a nap to help them feel \nbetter . The person may also want to talk to a trusted \nfriend or family member to share their feelings and \nprovide emotional support. \nProvide a short description of the scene \n Red fox running in the snow \nWhat does the fox do at the end? \nJump. \nFigure 25: Example prompts and responses when using a video fine-tuned variant of InstructBLIP (method described\nin Section 6.5). Our model is able to produce long-form textual responses that describe scenes and is able to answer\nquestions related to the temporality of events in the videos.\nFigure 26: The audio-multimodal agent described in Section 6.5. Hallucinated content are highlighted in red. We use\nGPT-4V to generate 1) the videochat summary with video frames; 2) the video summary with the frame captions; 3) the\nvideo summary with frame captioning and audio information.\n42", "start_char_idx": 0, "end_char_idx": 1538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84112dac-86dd-459a-a869-64404be4bb1b": {"__data__": {"id_": "84112dac-86dd-459a-a869-64404be4bb1b", "embedding": null, "metadata": {"page_label": "43", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a10e64c-0e2b-4c1b-b28b-2f06df29a4b7", "node_type": "4", "metadata": {"page_label": "43", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1c669c412c99924a36daf88c3b70b7a186ee05fd4ff647d33d7abc6247619837", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85835b48-0796-44f6-bb10-7a0464242cb3", "node_type": "1", "metadata": {"page_label": "42", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c7fd3ea27492311d031e751d1d29c1d4182bfac2f9f22ddfccee0992deb15d4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49e765db-71cd-465e-861a-1f0d282ec453", "node_type": "1", "metadata": {}, "hash": "3bf281924e0abcfdd9e3f62b0a2c2ff95111308f09d36924170f9ece3e45f427", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 27: An interactive multimodal agent that incorporates visual, audio, and text modalities for video understanding.\nOur pipeline mines hard negative hallucinations to produce difficult queries for the VideoAnalytica challenge. More the\nrelated details of interactive audio-video-language agent dataset are described in Section 9.2.\nAudio-Video-Language Agents with GPT-4V . We then evaluate the capabilities of GPT-4V as a multimodal agent\nthat integrates vision, audio, and speech for a nuanced and precise understanding of videos, following the methodology\noutlined in (Lin et al., 2023). Results depicted in Fig. 26 compare the performance of various video agents on the task\nof video summarization. The video-instruction tuned model (Li et al., 2023g) provides accurate content but falls short\non comprehensiveness and detail, missing specific actions like the methodical use of a broomstick to measure a tree\u2019s\nheight.\nTo enhance the accuracy of video descriptions, we employ GPT-4V to caption frames, while audio and its transcriptions\nare sourced from the OpenAI Whisper model. We then prompt GPT-4V to create video summaries using only frame\ncaptions and then using both frame captions and audio transcriptions. Initially, we observe that frame captions alone\ncan lead to fabricated events, such as a person biting down on a stick in the third segment. These inaccuracies persist in\nthe video summary, with descriptions like \"in a playful twist, he bites down on it while holding it horizontally.\" Without\naudio input, the agent cannot correct these captioning errors, resulting in descriptions that are semantically correct but\nvisually misleading.\n43", "start_char_idx": 0, "end_char_idx": 1736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49e765db-71cd-465e-861a-1f0d282ec453": {"__data__": {"id_": "49e765db-71cd-465e-861a-1f0d282ec453", "embedding": null, "metadata": {"page_label": "44", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7007df59-703f-4894-b8e7-1db035832e86", "node_type": "4", "metadata": {"page_label": "44", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "efe9eeb707b5c4af056d669952eff77b255f78a2fb60f00abfc28103ab38c8ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84112dac-86dd-459a-a869-64404be4bb1b", "node_type": "1", "metadata": {"page_label": "43", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1c669c412c99924a36daf88c3b70b7a186ee05fd4ff647d33d7abc6247619837", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6091070b-ea62-4766-95df-7cf31091e64d", "node_type": "1", "metadata": {}, "hash": "6388042ca457e791422bd789a4d22168bb0279c1fb39dfdd5e8d43760354d19f", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nHowever, when we provide the audio transcriptions to the agent, it manages to accurately depict the content, even\ncapturing detailed physical actions like \"holding the broomstick perpendicular to the body and rotating it downwards.\"\nThis level of detail is significantly more informative and gives viewers a clearer understanding of the video\u2019s purpose\nand key details. These findings highlight the importance of integrating audio, video, and language interactions to\ndevelop high-quality multimodal agents. GPT-4V emerges as a promising foundation for such advanced multimodal\nunderstanding and interaction.\nEmbodied Multi-modal Agents with GPT-4V . As shown in Fig. 27, We mainly used StackOverflow to get the initial\nQuestion, then we used the \u201cBing search\" API to retrieve a related video and audio corresponding to the question. Next,\nwe mainly use GPT-4V to get the relevant text information and high-level video description. On the other hand, we\ntransfer the key frame audio to a low-level segment description of the key frames via ASR. Finally, we use GPT-4V\nto generate convincing \"hallucinations\" that serve as hard negative queries for video-question and answer tasks. We\nsupport interactions and question answering in the current frame of the video, as well as summarization for the overall\nhigh-level video description. During inference, we also combine external knowledge information via web search to\nimprove answering capapbilities.\nThe main prompt information for GPT-4V is described as below. The entire prompt is indented for clarity; it is over one\npage long.\nGPT-4V are an assistant to provide descriptive, informative, and full comprehensive details in the\nvideo for the visually impaired who can hear the video but cannot see. The job is to create high-quality,\ndense descriptions of the video by synthesizing the given annotations and output them as JSON.\nSpecifically, GPT-4V will be given original query used to search the video, the video title, description,\naudio transcription, and potentially noisy descriptions for specific time in the video. Different\nsegments of same video is annotated as \"[time start - time end (in seconds)] \u2019text\u2019 \". Utilize the\ntranscriptions and descriptions all together to reason about the exact detail and visual demonstration\nthat might be happening in the video. GPT-4V will to combine or segment the timestamps as necessary\nto provide the best segmentation of the video.\nExpectations for GPT-4V Output:\n1. Action-Oriented Descriptions: Prioritize plausible actions, motions, and physical demonstrations\nthat the audio implies, enriching your narrative with dynamic visual cues.\n2. Complete Video Coverage: Provide a continuous and consistent audio-descriptive experience that\ncovers every moment of the video\u2019s duration, ensuring no content is left undescribed.\n3. Concise Segmentation: Construct your descriptions in focused, succinct segments of 1-2 sentences\neach to effectively communicate visual actions without overwhelming detail.\n4. Contextual Audio-Visual Synthesis: Seamlessly blend the spoken audio content with inferred\nvisual elements to form a narrative that reflects potential onscreen activities.\n5. Imaginative and Plausible Speculation: Infuse your descriptions with creative yet believable visual\ndetails that correspond with the audio, enhancing scene comprehension.\n6. Accurate Timecode Correspondence: Align your descriptive segments with corresponding time-\ncodes, ensuring that speculative visual details synchronize with the audio narrative\u2019s timeline.\n7. Confident Narrative Delivery: Present the descriptions with assurance, as though the speculated\nvisuals are occurring, to instill confidence in the listener.\n8. Omit Implausible Details: Exclude descriptions of objects or events that do not reasonably fit\nwithin the context established by the audio and visual information provided.\nThe final output should be structured in a JSON format containing a list of dictionaries, each detailing\na segment of the video.\nThe final output should be structured in a JSON format containing a list of dictionaries, each detailing\na segment of the video.\n[ \u2018start\u2019: <start-time-in-seconds>, \u2018end\u2019: <end-time-in-seconds>, \u2018text\u2019: \u201c<Your detailed\nsingle-sentence, audio-visual description here>\" ]\nFor MC Creation: our task is to create multiple-choice questions for video-to-text retrieval tasks\nthat is trivially solved by looking at the title and reading through audio transcriptions. To do so, we\nwill be given original query to get the video, description, audio transcription, and potentially noisy\ndescriptions for specific time in the video.\n44", "start_char_idx": 0, "end_char_idx": 4700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6091070b-ea62-4766-95df-7cf31091e64d": {"__data__": {"id_": "6091070b-ea62-4766-95df-7cf31091e64d", "embedding": null, "metadata": {"page_label": "45", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14310310-b79d-4265-9928-b3aa27ee2f70", "node_type": "4", "metadata": {"page_label": "45", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c557e47ee8ce2f5db2b6322c5169cb0173df141388d36e75cfeaddd6cb618382", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49e765db-71cd-465e-861a-1f0d282ec453", "node_type": "1", "metadata": {"page_label": "44", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "efe9eeb707b5c4af056d669952eff77b255f78a2fb60f00abfc28103ab38c8ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e848314c-ff29-4279-b64f-dd76484ba2f7", "node_type": "1", "metadata": {}, "hash": "9874f3c8fa9e945fe787bc79613f8e7ef74d0dc9378e64aea2833bb6f9d04863", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n\u2022Format of audio transcription: -[start-end time in seconds] \u201ctranscription\"\n\u2022Format of noisy description: - [time in seconds] \u201cdescription\"\nWe kindly ask GPT-4V to generate four queries, where the primary query is aligned with the video\ncontent, and the other three negatives are subtly different from our primary one. Selecting the\nprimary one should not simply involve listening to audio transcriptions e.g. the text original query is\ncontained in audio transcriptions. The negatives should be closely related but not fully aligned with\nthe video content, requiring visual understanding of the video to differentiate. For example, modify\nthe semantics in nuanced way so that one needs to watch the video than just listening to select the\noriginal query. Compile four queries in caption-like statement, with the first one being the rephrased\noriginal.\nThink step by step how you can come up with negative statements using the information from the\nvideo. And justify the negative queries are incorrect but still compelling choices that demand nuanced\nunderstanding of the video. And how humans would not accidentally choose the negatives over the\noriginal query.\nFinally, we present the work in the following format of analyses and 4 queries. No need to generate\nhow you translated the original query.\n\u2022 Video Analysis: xxx\n\u2022 Queries: [query1, query2, query3, query4]\n\u2022 Justification: xxx\n6.6 Agent for NLP\n6.6.1 LLM agent\nRecognizing task directives and taking action has been a fundamental challenge in interactive AI and natural language\nprocessing for decades. With the recent advances in deep learning, there is a growing interest in studying these\nareas jointly to improve human-agent collaboration. We identify three specific directions, among others, to improve\nlanguage-grounded agents:\n\u2022Tool use and querying from knowledge bases. This direction emphasizes the importance of integrating\nexternal knowledge bases, web search, or other helpful tools into the reasoning processes of AI agents. By\nleveraging structured and unstructured data from various sources, agents can enhance their understanding and\nprovide more accurate and context-aware responses. Furthermore, it fosters the agent\u2019s ability to proactively\nseek out information when faced with unfamiliar scenarios or queries, ensuring more comprehensive and\ninformed responses. Examples include Toolformer (Schick et al., 2023) and Retrieve What You Need (Wang\net al., 2023g).\n\u2022Improved agent reasoning and planning. Enhancing the agent\u2019s ability to reason and plan is pivotal for\neffective human-agent collaboration. This involves the development of models that can understand complex\ninstructions, infer user intentions, and predict potential future scenarios. This can be accomplished by asking\nthe agent to reflect on past actions and failures as in ReAct (Yao et al., 2023a), or by structuring the agent\nthought process as a form of search (Yao et al., 2023b). By simulating different outcomes and assessing the\nramifications of various actions, agents can make more informed context-aware decisions.\n\u2022Incorporating system and human feedback. AI agents can frequently operate in two primary contexts:\nenvironments that provide explicit signals about the effectiveness of their actions (system feedback), and\nsettings where they collaborate with humans who can offer verbal critiques (human feedback). This direction\nunderscores the need for adaptive learning mechanisms that allow agents to refine their strategies and rectify\nmistakes, such as in AutoGen (Wu et al., 2023). The ability to continuously learn and adapt from diverse\nfeedback sources ensures that agents remain helpful and aligned for user needs.\n6.6.2 General LLM agent\nRecognizing and understanding agent content and natural language has been a fundamental challenge in interactive AI\nand natural language processing for decades. With the recent advances in deep learning, there is a growing interest\n45", "start_char_idx": 0, "end_char_idx": 4014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e848314c-ff29-4279-b64f-dd76484ba2f7": {"__data__": {"id_": "e848314c-ff29-4279-b64f-dd76484ba2f7", "embedding": null, "metadata": {"page_label": "46", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8850023-e7f3-41c6-a310-c00f090aaa7d", "node_type": "4", "metadata": {"page_label": "46", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "eac444011a34d69f9336e65dcec48641b5a520b74f0b298427c905131e23a2b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6091070b-ea62-4766-95df-7cf31091e64d", "node_type": "1", "metadata": {"page_label": "45", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c557e47ee8ce2f5db2b6322c5169cb0173df141388d36e75cfeaddd6cb618382", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67a74e0f-e3c6-472a-b2ee-15cfd7f42f87", "node_type": "1", "metadata": {}, "hash": "83928cc83e4049725e1b8a00d1686d05993ccfbb038a3ba5e9791abed7b08fa6", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nSelf-Instruct seed tasks \nExample: \nInstruction: Brainstorm a list of good \ntropical island vacation destinations. \nOutput: \n- Hawaii \n- Mauritius \n- Seychelles Larger, proprietary or \nclosed-source LLM \nExample Generated Task: \nInstruction: Describe ways you can \nplay in the snow.. \nOutput: \nYou could have a snowball fight, go \nsledding, build a snowman \u2026Instruction-following \nexamples Pre-trained \nOpen-source LLM \n Alpaca General Alpaca Training Framework \nModified Self-instruct \nInstruction Generation Supervised \nFine-tuning \nFigure 28: The training recipe used to train the Alpaca model (Taori et al., 2023). At a high level, existing LLMs\nare used to generate a large pool of instruction-following examples from a smaller set of seed tasks. The generated\ninstruction-following examples are then used to instruction-tune an LLM where the underlying model weights are\navailable.\nin studying these two areas jointly for deep understanding of both agent planning or human feedback for knowledge-\ninference and natural language generation. These are the key components of many human-machine-interaction agents,\nsuch as \u201cAutoGen\"(Wu et al., 2023) and \u201cRetrieve What You Need\"(Wang et al., 2023g).\n6.6.3 Instruction-following LLM agents\nFurthermore, the creation of LLM Agents that can be trained to effectively follow human instructions has become\nan important area of research. Initial models used human feedback to train a proxy reward model to simulate human\npreferences, through a process known as Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022).\nThis process produced models such as InstructGPT and ChatGPT. In order to more efficiently train instruction-following\nLLM agents without needing human labels, researchers developed a more efficient method for instruction-tuning that\ntrains the LLM agent directly on instruction/response pairs, either generated by humans like Dolly 2.06or automatically\nfrom LLMs like Alpaca (Taori et al., 2023). We show the overall Alpaca training pipeline in Figure 28.\n6.6.4 Experiments and Results\nDespite the growing adoption of conversational and self-feedback systems, these forms of AI still do not perform\nwell with regard to generating factually correct responses from their own implicit knowledge and therefore often use\nexternal tools like web search and knowledge retrieval mechanisms at inference-time to augment their response as a\nconsequence. Addressing this would help create more engaging experiences for users in many real-life applications. In\nsocial conversations (such as those on social media platforms like Instagram and Facebook), or with Q+A websites\n(such as Ask or Quora), people usually engage with others through a series of comments and by web-searching for\ninformation and knowledge relevant to the discussion. Thus, the task of generating conversational turns in this context\nis not to simply bootstrap upon traditional NLP models and tasks, but to use agents to generate dialogue through\nintelligent behaviors that reflect knowledge search and acquisition (Peng et al., 2023). In this way, intelligent agents for\n6Dolly 2.0 blogpost link\n46", "start_char_idx": 0, "end_char_idx": 3211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67a74e0f-e3c6-472a-b2ee-15cfd7f42f87": {"__data__": {"id_": "67a74e0f-e3c6-472a-b2ee-15cfd7f42f87", "embedding": null, "metadata": {"page_label": "47", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cedbdaba-b3a4-4ac2-aa20-3208b1c78c81", "node_type": "4", "metadata": {"page_label": "47", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5c6d5fedfdb54c58c43e4e7f40b0839f7bc696705af9319bdb4d891dec7769be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e848314c-ff29-4279-b64f-dd76484ba2f7", "node_type": "1", "metadata": {"page_label": "46", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "eac444011a34d69f9336e65dcec48641b5a520b74f0b298427c905131e23a2b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e62965f-9708-4b8e-9ea5-d03c111572e8", "node_type": "1", "metadata": {}, "hash": "1cddf3697f3781a7662c45b8b1d5d20d8722e6b7f49821c1caa896b80e8f8dca", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 29: The logic transformer agent model (Wang et al., 2023e). We integrate a logical reasoning module into the\ntransformer-based abstractive summarization model in order to endow the logic agent the ability to reason over text and\ndialogue logic, so that it can generate better-quality abstractive summarizations and reduce factuality errors.\nNLP tasks extends the task description and improves upon the interpretability of the response by adding an explicit\nknowledge search and retrieval step during dialogue. Incorporating these web search and retrieval agents as feedback\nduring dialogue will help to engage further and deeper the social interactions between humans and agents (Wang\net al., 2023e). As the Fig 29 showed, we introduced a new modeling paradigm for transformer language models that\ndetects and extracts important logical structures and information from input texts and then integrates them into the input\nembeddings through carefully designed multi-layer hierarchical logical projections to infuse logical structures into\npre-trained language models as one kind of NLP agent. (Wang et al., 2023e) propose a novel approach to construct\nlogic-aware input embeddings for transformer language models through a combination of logic detection, logic mapping\nand hierarchical logical projections, and then develop a corresponding new modeling paradigm that can upgrade all\nexisting transformer language models into logical transformers to consistently boost their performance. The proposed\nlogical transformer agent consistently achieve superior performance over their baseline transformer models through\na deeper understanding of the logical structures of texts. To human users, it is often these aspects that are more\nimportant for delivering a meaningful and interesting conversation via a agent-based coordination between dialogue\nand information retrieval. Delving deep into natural language processing, this topic will discuss the advancements and\nleader-board in making LLMs more agentic and better suited for various language-centered tasks.\nAn open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever\nis used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved\npassages and the original question. In (Wang et al., 2023g), we propose a simple and novel mutual learning framework\nto improve the performance of retrieve-then-read -style models via an intermediate module named the knowledge\nselector agent, which we train with reinforcement learning. The fine-grained knowledge selector into the retrieve-then-\nreader paradigm, whose goal is to construct a small subset of passages which retain question-relevant information. As\nshowed in Figure 30, The knowledge selector agent is trained as a component of our novel mutual learning framework,\nwhich iteratively trains the knowledge selector and the reader. We adopt a simple and novel approach employing policy\ngradients to optimize the knowledge selector agnet, using feedback from the reader to train it to select a small and\n47", "start_char_idx": 0, "end_char_idx": 3187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e62965f-9708-4b8e-9ea5-d03c111572e8": {"__data__": {"id_": "8e62965f-9708-4b8e-9ea5-d03c111572e8", "embedding": null, "metadata": {"page_label": "48", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2cb18a4-f1c9-4f1d-953c-41f259daee3e", "node_type": "4", "metadata": {"page_label": "48", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ed866641d5c3c75658021e4658cb8ca4f6939481b894cb37ab05413da6d1eb07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67a74e0f-e3c6-472a-b2ee-15cfd7f42f87", "node_type": "1", "metadata": {"page_label": "47", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5c6d5fedfdb54c58c43e4e7f40b0839f7bc696705af9319bdb4d891dec7769be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a861fc75-c1ae-4d50-844b-1d547be3f285", "node_type": "1", "metadata": {}, "hash": "37ada4469149039d33ffd523e3277829438e5f6577e3dd1d3eeaba834a6b561e", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 30: Architecture of one proposed NLP agent (Wang et al., 2023g) mutual learning framework. In each epoch,\nPhase 1 and Phase 2 are executed alternately. During Phase 1, the parameters of the reader model remain fixed, and\nonly the weights of the knowledge selector are updated. Conversely, during Phase 2, the reader model\u2019s parameters are\nadjusted, while the knowledge selector\u2019s weights remain frozen.\ninformative set of passages. This approach avoids brute-force search or manually-designed heuristics, without requiring\nany annotated query-document pairs for supervision. We show that iteratively training the reader and the knowledge\nselector agent leads to better predictive performance on some public open-domain question answering benchmarks.\n7 Agent AI Across Modalities, Domains, and Realities\n7.1 Agents for Cross-modal Understanding\nMulti-modal understanding is a significant challenge for creating generalist AI agents due to the lack of large-scale\ndatasets that contain vision, language, and agent behavior. More generally, training data for AI agents is often modality\nspecific. This results in most modern multi-modal systems using a combination of frozen submodules. Some notable\nexamples are Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023c), and LLaV A (Liu et al., 2023c), all of which\nutilize a frozen LLM and frozen visual encoder. These submodules are trained individually on separate datasets, and\nthen adaptation layers are trained to encode the visual encoder into the LLM embedding space. In order to make further\nprogress for cross-modal understanding for AI agents, it is likely that the strategy of using frozen LLMs and visual\nencoders will need to change. Indeed, RT-2, a recent visual-language model that is capable of taking actions within the\ndomain of robotics showed significantly improved performance when jointly tuning the visual encoder and LLM for\nrobotics and visual-language tasks (Brohan et al., 2023).\n7.2 Agents for Cross-domain Understanding\nA key challenge for creating generalist agents is the distinctive visual appearance and disparate action spaces across\ndifferent domains. Humans possess the capability to interpret images and videos from various sources, including the\nreal world, video games, and specialized domains such as robotics and healthcare, once they become familiar with the\nspecific details of these areas. However, existing LLMs and VLMs often demonstrate significant differences between\nthe data they were trained on and the varied domains in which they are applied. And notably, training agent models to\npredict specific actions presents a considerable challenge when trying to develop a single policy that can effectively\nlearn multiple control systems across domains. Generally, the approach most modern works take when applying systems\nwithin specific domains is to start from a pretrained foundation model and then finetune a separate model for each\nspecific domain. This fails to capture any commonalities between domains and results in a smaller total set of data used\nfor training instead of leveraging each domain\u2019s data.\n48", "start_char_idx": 0, "end_char_idx": 3190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a861fc75-c1ae-4d50-844b-1d547be3f285": {"__data__": {"id_": "a861fc75-c1ae-4d50-844b-1d547be3f285", "embedding": null, "metadata": {"page_label": "49", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be488adb-e601-4510-a159-4e96ed46ab3f", "node_type": "4", "metadata": {"page_label": "49", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "caa02aa31958c2d589f4fca6a2735b1b53f7b26c26a0a1cb187ad212331d8925", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e62965f-9708-4b8e-9ea5-d03c111572e8", "node_type": "1", "metadata": {"page_label": "48", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ed866641d5c3c75658021e4658cb8ca4f6939481b894cb37ab05413da6d1eb07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3adb0812-80a6-4cc7-a963-84f2e047dbf0", "node_type": "1", "metadata": {}, "hash": "67fd6216c733eccbd49389754a9cd7a9b62dc9402de441318ed778bc479760cc", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n7.3 Interactive agent for cross-modality and cross-reality\nDeveloping AI agents that can successfully understand and perform tasks across different realities is an on-going\nchallenge that has seen some recent success for image and scene generation (Huang et al., 2023a). In particular, it is\nchallenging for agents to simultaneously understand real-world and virtual reality environments due to their visual\ndissimilarities and separate environment physics. Within the context of cross-reality, Sim to Real transfer is a particularly\nimportant problem when using simulation-trained policies for real-world data, which we discuss in the next section.\n7.4 Sim to Real Transfer\nTechniques which enable models trained in simulation to be deployed in the real world. Embodied agents, especially\none based on RL policies, are typically trained in simulated environments. These simulations do not fully replicate the\ncharacteristics of the real world (e.g., disturbances, light, gravity, and other physical properties). Due to this discrepancy\nbetween simulation and reality, models trained in simulation often struggle to perform well when applied in the real\nworld. This issue is known as the \u201csim-to-real\u201d problem. To solve this problem, several approaches can be taken:\n\u2022Domain randomization : domain randomization is a technique that trains a model while randomly varying\nparameters within a simulation environment (e.g., object appearance, sensor noise, and optical properties)\nin anticipation of the uncertainties and variations of the real world (Tobin et al., 2017). For instance, in the\ncontext of training a RL-based grasping skills, introducing randomness in the shapes of objects can lead to a\npolicy capable of adapting to objects with somewhat different shapes (Saito et al., 2022).\n\u2022Domain adaptation : Domain adaptation, or domain transfer is a technique that bridges the gap between\nsimulated and real-world domains by training models with a large number of simulated images and a smaller\nset of real-world images. In practical settings, unpaired image-to-image translation methods such as Cy-\ncleGAN (Zhu et al., 2017b) are employed due to the difficulty in preparing paired images across domains.\nSeveral enhanced versions exist for reinforcement learning, including RL-CycleGAN (Rao et al., 2020), and\nfor imitation learning, such as RetinaGAN (Ho et al., 2021).\n\u2022Improvement of simulation : Realistic simulation is a key for sim-to-real transfer. Part of this effort is\nachieved by a system identification techniques (Zhu et al., 2017c; Allevato et al., 2020), which aims to identify\nsimulation parameters to mimic the real-world environments. Additionally, use of photorealistic simulators\nwould be effective in image-based reinforcement learning (Martinez-Gonzalez et al., 2020; M\u00fcller et al., 2018;\nShah et al., 2018; Sasabuchi et al., 2023).\nThe sim-to-real transfer remains a central challenge in the study of Embodied Agents, as approaches keep evolving.\nBoth theoretical and empirical research are essential to advance these technologies further.\n8 Continuous and Self-improvement for Agent AI\nCurrently, foundation model based AI agents have the capacity to learn from multiple different data sources, which\nallow for more flexible sources for data for training. Two key consequences of this are (1) user and human-based\ninteraction data can be used to further refine and improve the agent and (2) existing foundation models and model\nartifacts can be used to generate training data. We discuss each of these in more detail in the following sections, but we\nnote that since current AI Agents are largely tied to existing pretrained foundation models, they generally do not learn\nfrom continuous interaction with their environments. We think this is an exciting future direction, and initial work by\nBousmalis et al. has shown that self-improving agents for robotic control are able to continuous learn and improve\nthrough environmental interactions without supervision (Bousmalis et al., 2023).\n8.1 Human-based Interaction Data\nThe core idea behind using human-based interaction data is to leverage a large number of of agent-human interactions to\ntrain and improve future iterations of the agent. There are several strategies used to improve agents from human-agent\ninteractions.\n\u2022Additional training data Perhaps the simplest usage of human-agent interactions is to use the interaction\nexamples themselves as training data for a future iteration of the agent. This generally requires filtering\n49", "start_char_idx": 0, "end_char_idx": 4592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3adb0812-80a6-4cc7-a963-84f2e047dbf0": {"__data__": {"id_": "3adb0812-80a6-4cc7-a963-84f2e047dbf0", "embedding": null, "metadata": {"page_label": "50", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e6efbdb-6889-4f2e-a21c-6acc585ab376", "node_type": "4", "metadata": {"page_label": "50", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "20d65827c3f3c23fbc5c12e90d5a19ee735333305066d652535cb571ab83a8cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a861fc75-c1ae-4d50-844b-1d547be3f285", "node_type": "1", "metadata": {"page_label": "49", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "caa02aa31958c2d589f4fca6a2735b1b53f7b26c26a0a1cb187ad212331d8925", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2b87b05-baa6-4afb-92a3-07dc0c8b9c8c", "node_type": "1", "metadata": {}, "hash": "e7cbf138db456936b56522051c5119d2d5f3a6535320fc2910b42b10c6cfe8f9", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nstrategies to differentiate successful agent examples from unsuccessful interaction examples. Filtering can\nbe rules-based (e.g., reaching some desired end goal state), model-based (e.g., classifying successful vs\nunsuccessful interactions), or manually selected after a posthoc inspection and/or modification of the interaction\nexamples.\n\u2022Human preference learning During interaction with the user, the agent system can prompt the user with\nseveral different model outputs and allow for the user to select the best output. This is commonly used by\nLLMs like ChatGPT and GPT-4, whereby users can select one output (out of several) that aligns best with\ntheir preferences.\n\u2022Safety training (red-teaming) Red-teaming within the context of Agent AI refers to having a dedicated team\nof adversaries (either human or computer) that seek to exploit and expose weaknesses and vulnerabilities\nwithin the Agent AI system. Although adversarial in nature, red-teaming is commonly used as a means for\nunderstanding how to improve AI safety measures and reduce the occurrence of harmful outputs. The core\nprinciple is to discover consistent methods for inducing unwanted agent outputs so that the model can be\ntrained on data that explicitly corrects this behavior.\n8.2 Foundation Model Generated Data\nWith the advent of powerful foundation model artifacts produced by academia and industry, there have been a variety of\nmethods developed to extract and generate meaningful training data from these artifacts using a variety of prompting\nand data-pairing techniques.\n\u2022LLM Instruction-tuning Methods for generating instruction-following training data from LLMs have allowed\nfor the finetuning of smaller, open-source models based on the outputs of larger proprietary LLMs (Wang\net al., 2022b). For example, Alpaca (Taori et al., 2023) and Vicuna (Zheng et al., 2023) are LLMs based on the\nopen-source LLaMA family (Touvron et al., 2023) that have been tuned on various outputs from ChatGPT\nand human participants. This method of instruction tuning can be viewed as a form of knowledge distillation,\nwhere the larger LLM serves as a teacher model to a smaller student model. Importantly, although LLM\ninstruction-tuning has been shown to transfer the writing style and some instruction-following capabilities of\nthe teacher model to the student model, significant gaps still exist between the factuality and capabilities of the\nteacher and student models (Gudibande et al., 2023).\n\u2022Vision-language pairs A number of recent works have sought to increase the number of diversity of pretraining\ndata available to visual-language models by automatically generating captions and other text for visual content.\nFor example, LLaV A (Liu et al., 2023c) uses 150,000 examples of instruction-following behavior from textual\nand visual inputs that are mainly LLM-generated. Other work has shown that using VLMs to re-caption images\ncan improve the training data and subsequent quality of image generation models (Segalis et al., 2023). Within\nthe realm of video understanding, using VLMs and LLMs to recaption videos has been shown to improve the\nperformance and quality of subsequent VLMs trained on the recaptioned videos (Wang et al., 2023f; Zhao\net al., 2022).\n9 Agent Dataset and Leaderboard\nTo accelerate research in this domain, we propose two benchmarks respectively for multi-agent gaming and agentic\nvisual language tasks. We will release two new datasets - \u201cCuisineWorld\u201d and \u201cVideoAnalytica\u201d - and a set of baseline\nmodels, encouraging participants to explore new models, systems, and submit their results on the test set of our\nleaderboard.\n9.1 \u201cCuisineWorld\u201d Dataset for Multi-agent Gaming\nCuisineWorld is a text-based game reminiscent of Overcooked! It offers a platform for AI-powered agents to cooperate\nand play in tandem. This dataset will test the collaboration efficiency of multi-agent systems, offering insights into\nhow well LLMs and other systems can work together in dynamic scenarios. In particular, the dataset will focus on how\nwell the agents understand goals, and how well the agents can coordinate among themselves. Two types of modes are\nsupported in this dataset: a centralized dispatcher mode and a decentralized mode. Participants can choose a play mode\nand make a submission to our leaderboard.\n50", "start_char_idx": 0, "end_char_idx": 4379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2b87b05-baa6-4afb-92a3-07dc0c8b9c8c": {"__data__": {"id_": "a2b87b05-baa6-4afb-92a3-07dc0c8b9c8c", "embedding": null, "metadata": {"page_label": "51", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96182cd8-481e-4d67-8e85-c35156048fae", "node_type": "4", "metadata": {"page_label": "51", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b7a61f5ebc94d52945a9380f9fd0b94069a389004349af3cc07580f9012c6700", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3adb0812-80a6-4cc7-a963-84f2e047dbf0", "node_type": "1", "metadata": {"page_label": "50", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "20d65827c3f3c23fbc5c12e90d5a19ee735333305066d652535cb571ab83a8cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b1e329a-ec59-4717-a8e0-7c396687ab66", "node_type": "1", "metadata": {}, "hash": "47f202263db47e0377e1385aad3cc7832c2b19d9de3489076ddd2ba2d8ca7f93", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\n9.1.1 Benchmark\nFor our competition, we will release a benchmark, the CuisineWorld benchmark, which includes a text interface that\nincludes extendable task definition files, and an interface for multi-agent interaction, and human-machine interactions.\nWe introduce the gaming interaction task in which the goal is to generate relevant, appropriate, multi-agent collaboration\nstrategies that can maximize collaboration efficiency. We evaluate the collaboration efficiency with the proposed\nevaluation metric: CoS.\nThe \u201cCuisineWorld\" dataset was collected by Microsoft, UCLA, and Stanford University. The goal of the competition\nis to explore how different, existing and novel, grounded-LLM and interactive techniques perform with this benchmark\nand establish strong baselines for the task of multi-agent gaming infrastructure.\nThe dataset of CuisineWorld includes:\n- A selection of well-defined multi-agent collaboration tasks.\n- An API system to facilitate agent interactions.\n- An automatic evaluation system.\n(The link for downloading the dataset will soon be made available and this article will be updated to include it here.)\n9.1.2 Task\n\u2022We provide a dataset and related the benchmark, called Microsoft MindAgent and and correspondingly release\na dataset \u201cCuisineWorld\u201d to the to the research community.\n\u2022We will provide benchmarks to evaluate and rank the submitted \u201cMindAgent\" algorithms. We will also provide\nbaseline results generated using popular infrastructures.\n9.1.3 Metrics and Judging\nThe quality of multi-agent collaboration efficiency is determined by the new \u201ccos\" auto-metric (from MindAgent (Gong\net al., 2023a)). The final rating of out metric is calculated as an average over the evaluated collaboration efficiency\nmetrics of the multi-agent system on all tasks. Human evaluators will be asked to rate individual responses as well\nas provide subjective judgement of the engagement, breadth and an overall quality of the users\u2019 interactions with the\nagents.\n9.1.4 Evaluation\n\u2022Automated Evaluation . We plan to release a leaderboard, starting on the release date (TBA), registered\nparticipants will be asked to submit their results on the task associated with the dataset \u201cCuisineWorld\" (our\npublicly released dataset for the leaderboard). Submission of results will be closed on the end date (TBA).\nEach team will be required to submit their generated results on the testing set for automated evaluation of the\n\u201ccos\" metric.\n\u2022Human Evaluation on our leaderboard . The leaderboard participants will need to provide a submission\nfile generated by evaluation scripts locally. We will use the evalAI system to check the submission file and\noptionally rerun the code for top challenge contenders. Therefore, teams must also submit their code with a\nReadme file on how to run their code. Human evaluation will be performed by the organization team.\n\u2022Winner Announcement. We will make an announcement of the winners and post the final ratings of the\nsubmissions on our leaderboard.\n9.2 Audio-Video-Language Pre-training Dataset.\nWe introduce VideoAnalytica: a new benchmark for analytical video demonstration comprehension. VideoAnalytica\nfocuses on leveraging video demonstrations as aids to better understand complex, high-level reasoning embedded\nwithin long-formed instructional videos. The objective is to evaluate the cognitive reasoning abilities of video language\nmodels, pushing them beyond mere recognition tasks and basic comprehension, towards a more sophisticated and\nnuanced understanding of videos. Crucially, VideoAnalytica emphasizes the integration of multiple modalities, such as\n51", "start_char_idx": 0, "end_char_idx": 3687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b1e329a-ec59-4717-a8e0-7c396687ab66": {"__data__": {"id_": "0b1e329a-ec59-4717-a8e0-7c396687ab66", "embedding": null, "metadata": {"page_label": "52", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1535bf5-47b2-4404-b642-9bbb965494ba", "node_type": "4", "metadata": {"page_label": "52", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9b003fe614803faa9c1fd974a418e2b5dd3b2dbb77e33730fd19787610d6e16e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2b87b05-baa6-4afb-92a3-07dc0c8b9c8c", "node_type": "1", "metadata": {"page_label": "51", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b7a61f5ebc94d52945a9380f9fd0b94069a389004349af3cc07580f9012c6700", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a672486-b062-4bf6-b1ce-76a87991fbc0", "node_type": "1", "metadata": {}, "hash": "3d29bf7aac3910c854a835df40e6ce6f97846cfe6d26b85736581b250315abe7", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\naudio, video, and language, as well as the ability of models to apply domain-specific knowledge, to contextualize and\ninterpret the information presented in the videos. Specifically, VideoAnalytica involves two primary tasks:\n1.Video Text Retrieval: This task involves accurately retrieving relevant text from the instructional videos.\nThe challenge lies in distinguishing between relevant and irrelevant information, thus requiring a deep\nunderstanding of the video content, and analysis of the demonstration to retrieve the correct query. To further\nincrease the complexity of these tasks, we introduce hard negatives into our datasets generated by large\nlanguage models. We run human validation on the generated negatives and remove instances that make the\ntask invalid and unfair (e.g. negatives being valid).\n2.Video Assisted Informative Question Answering: This task requires the model to answer questions based on\nthe information extracted from the videos. The focus is on complex questions that require analytical reasoning\nand a thorough comprehension of the video demonstration.\nTo facilitate the development of an audio-video-language agent for analytical video understanding, we introduce a\nbenchmark leaderboard for the two tasks from VideoAnalytica.\n\u2022The leaderboard participants will need to submit their solutions for evaluation. The evaluation will be based on\nthe model\u2019s performance on the two tasks, and the results will be displayed on the leaderboard. Participants\nare required to submit their code, along with a detailed explanation of their approach and methodology.\n\u2022Ethical considerations: The leaderboard focuses on understanding and interpreting video content, which could\npotentially be used in surveillance or other privacy-invasive applications. Therefore, it\u2019s crucial to consider\nthe ethical implications and potential misuse of the technology. We encourage participants to consider these\naspects in their submissions and promote the ethical use of AI.\n10 Broader Impact Statement\nThis article and our associated forum7aim to be a catalyst for innovative research, fostering collaborations that will\ndrive the next wave of AI applications. By focusing on multimodal agents, we emphasize the future direction of\nhuman-AI interactions, leader-board, and solutions. We detail three ways in which we make significant contributions to\nthe broader community.\nFirstly, we hope our forum grounds AI researchers to develop solutions motivated by real-world problems in gaming,\nrobotics, healthcare, and long-video understanding. Specifically, the development of multimodal agents in gaming\ncould lead to more immersive and personalized gaming experiences, thereby transforming the gaming industry. In\nrobotics, the development of adaptive robotic systems could revolutionize industries ranging from manufacturing\nto agriculture, potentially addressing labor shortages and improving efficiency. In healthcare, the use of LLMs and\nVLMs as diagnostic agents or patient care assistants could lead to more accurate diagnoses, improved patient care,\nand increased accessibility to medical services, particularly in underserved areas. Furthermore, the ability of these\nmodels to interpret long-form videos could have far-reaching applications, from enhancing online learning to improving\ntechnical support services. In general, the topics covered in our forum will have significant downstream effects on a\nwide range of industries and humans across the world.\nSecondly, we hope our forum stands as a valuable resource for AI practitioners and researchers alike, serving as a\nplatform to explore and deeply comprehend the diverse and complex leader-board that come with implementing AI\nagents across a wide variety of environments and situations. This exploration includes, for instance, understanding the\nspecific limitations and potential hazards linked to Agentic AI systems when they are developed for specialized sectors\nsuch as healthcare diagnostics. In this domain, issues like dangerous hallucinations in AI behavior can pose significant\nrisks, highlighting the critical need for meticulous design and testing. However, these specific leader-board may not be\nequally relevant or noticeable when considering AI agents crafted for the gaming industry. In such recreational fields,\ndevelopers might instead prioritize tackling different hurdles, such as the need for AI to perform more open-ended\ngeneration and exhibit creativity, adapting dynamically to unpredictable gameplay scenarios and player interactions.\nBy attending the forum, participants will gain insights into how these varied environments dictate the focus and\ndirection of AI development, and how best to tailor AI solutions to meet these distinct needs and overcome the pertinent\nleader-board.\n7https://multimodalagentai.github.io\n52", "start_char_idx": 0, "end_char_idx": 4894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a672486-b062-4bf6-b1ce-76a87991fbc0": {"__data__": {"id_": "2a672486-b062-4bf6-b1ce-76a87991fbc0", "embedding": null, "metadata": {"page_label": "53", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a916cabb-d677-4c41-9884-fa54c13efdb4", "node_type": "4", "metadata": {"page_label": "53", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9ba40227678629d444623bbb526080f27b2b0d7f722eda21390b59764e0ccfda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b1e329a-ec59-4717-a8e0-7c396687ab66", "node_type": "1", "metadata": {"page_label": "52", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9b003fe614803faa9c1fd974a418e2b5dd3b2dbb77e33730fd19787610d6e16e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9f77a88-be87-4576-832f-a125d49ae211", "node_type": "1", "metadata": {}, "hash": "5cd91a7fb01b618e1659cee65137ae11e38b9ac0696b4e904bf8db53a7769550", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nThirdly, the various elements of our event, including the expert presentations, informative posters, and notably the\nwinners of our two leader-board, are set to offer a substantive yet succinct overview of the latest and significant trends,\nresearch directions, and innovative concepts in the realm of multimodal agents. These presentations will encapsulate\npivotal findings and developments, shining a light on new systems, ideas, and technologies in the field of mulitmodal\nagent AI. This assortment of knowledge is not only beneficial for the attendees of our forum, who are looking to\ndeepen their understanding and expertise in this domain, but it also serves as a dynamic and rich resource board. Those\nvisiting our forum\u2019s website can tap into this reservoir of information to discover and understand the cutting-edge\nadvancements and creative ideas steering the future of multimodal agent AI. We strive to serve as a useful knowledge\nbase for both newcomers and veterans in the field. By engaging with these resources, we hope participants and online\nvisitors alike can remain informed of the transformative changes and novel approaches that are shaping the exciting\nlandscape surrounding multimodal agent AI.\n11 Ethical Considerations\nMultimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models\ncould help drive content generation for bots and AI agents, and assist in productivity applications, helping to re-play,\nparaphrase, action prediction or synthesize 3D or 2D scenario. Fundamental advances in agent AI help contribute\ntowards these goals and many would benefit from a greater understanding of how to model embodied and empathetic in\na simulate reality or a real world. Arguably many of these applications could have positive benefits.\nHowever, this technology could also be used by bad actors. Agent AI systems that generate content can be used to\nmanipulate or deceive people. Therefore, it is very important that this technology is developed in accordance with\nresponsible AI guidelines. For example, explicitly communicating to users that content is generated by an AI system\nand providing the user with controls in order to customize such a system. It is possible the Agent AI could be used to\ndevelop new methods to detect manipulative content - partly because it is rich with hallucination performance of large\nfoundation model - and thus help address another real world problem.\nFor examples, 1) in health topic, ethical deployment of LLM and VLM agents, especially in sensitive domains like\nhealthcare, is paramount. AI agents trained on biased data could potentially worsen health disparities by providing\ninaccurate diagnoses for underrepresented groups. Moreover, the handling of sensitive patient data by AI agents raises\nsignificant privacy and confidentiality concerns. 2) In the gaming industry, AI agents could transform the role of\ndevelopers, shifting their focus from scripting non-player characters to refining agent learning processes. Similarly,\nadaptive robotic systems could redefine manufacturing roles, necessitating new skill sets rather than replacing human\nworkers. Navigating these transitions responsibly is vital to minimize potential socio-economic disruptions.\nFurthermore, the agent AI focuses on learning collaboration policy in simulation and there is some risk if directly apply-\ning the policy to the real world due to the distribution shift. Robust testing and continual safety monitoring mechanisms\nshould be put in place to minimize risks of unpredictable behaviors in real-world scenarios. Our \u201cVideoAnalytica\"\ndataset is collected from the Internet and considering which is not a fully representative source, so we already go\nthrough-ed the ethical review and legal process from both Microsoft and University Washington. Be that as it may, we\nalso need to understand biases that might exist in this corpus. Data distributions can be characterized in many ways. In\nthis workshop, we have captured how the agent level distribution in our dataset is different from other existing datasets.\nHowever, there is much more than could be included in a single dataset or workshop. We would argue that there is a\nneed for more approaches or discussion linked to real tasks or topics and that by making these data or system available.\nWe will dedicate a segment of our project to discussing these ethical issues, exploring potential mitigation strategies,\nand deploying a responsible multi-modal AI agent. We hope to help more researchers answer these questions together\nvia this paper.\n12 Diversity Statement\nBy examining the adaptability of AI agent models in various domains, we inherently embrace a diversity of leader-board,\nperspectives, and solutions. In this vein, our project aims to build a diverse community by exploring the wide array of\nsubjects in multimodal and agentic AI.\n53", "start_char_idx": 0, "end_char_idx": 4963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9f77a88-be87-4576-832f-a125d49ae211": {"__data__": {"id_": "d9f77a88-be87-4576-832f-a125d49ae211", "embedding": null, "metadata": {"page_label": "54", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c6e7d93-8978-41b4-95e4-6ee94320b0dc", "node_type": "4", "metadata": {"page_label": "54", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1c45dd6e5d6945a052b23557fbe3c984c33ca67282379a9d5f828908c87a9e7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a672486-b062-4bf6-b1ce-76a87991fbc0", "node_type": "1", "metadata": {"page_label": "53", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9ba40227678629d444623bbb526080f27b2b0d7f722eda21390b59764e0ccfda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4190b0d-afac-4b87-9202-3254270dfb09", "node_type": "1", "metadata": {}, "hash": "7c0b401be705dc25b3805a2e875f5a9f49c4ea4b9644f3c9981319f10c158fbd", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nWith these principles in mind, this project focuses on advanced multimodal systems that interact effectively within\nboth physical and virtual environments and facilitate effective interaction with humans. As such, we intend to engage a\nbroad range of experts and practitioners across a wide-range of technical specialities, cultures, countries, and scholarly\nfields to discuss important topics, including but not limited to:\n\u2022Application of foundation models: the development of agents with integrated modalities (audio, image, text,\nsensor inputs), aiming to enhance their recognition and response capabilities for a wide variety of applications.\n\u2022General-purpose end-to-end systems: the development of end-to-end models that are trained with large-scale\ndata, seeking to create versatile and adaptable AI solutions.\n\u2022Methodologies for grounding modalities: integrating information across various modalities, enhancing the\ncoherence and efficacy of data processing.\n\u2022Intuitive human interface: the development of effective and meaningful interaction between humans and\nagents.\n\u2022Taming LLM/VLMs: exploring new approaches to address common issues in large-scale models, such as\nhallucinations and biases in their outputs.\nWe aspire to broaden our collective understanding of the potential and limitations of agentic AI by leveraging our\nunique and diverse perspectives. We strongly believe that this approach will not only enrich individual perspectives,\nbut will also enhance the community\u2019s collective knowledge and promote a holistic view that is more inclusive of the\nwide-ranging leader-board faced by multimodal AI agents.\n54", "start_char_idx": 0, "end_char_idx": 1701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4190b0d-afac-4b87-9202-3254270dfb09": {"__data__": {"id_": "d4190b0d-afac-4b87-9202-3254270dfb09", "embedding": null, "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61d95ae2-f09d-4399-9da7-ea9bef5476b9", "node_type": "4", "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9d691511ac4ebac5263e186ffc095f5d8e2f0582798c5e07f91ee974662d0e4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9f77a88-be87-4576-832f-a125d49ae211", "node_type": "1", "metadata": {"page_label": "54", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1c45dd6e5d6945a052b23557fbe3c984c33ca67282379a9d5f828908c87a9e7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c531495-f655-4d17-b0be-6dbf8dcc6ef0", "node_type": "1", "metadata": {}, "hash": "1439f8abf3f5d3e52b2c9736bacce0e6b01759aef820061aae2c7d13bd54476e", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nReferences\nM. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\nA. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi,\nR. Julian, D. Kalashnikov, Y . Kuang, K.-H. Lee, S. Levine, Y . Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao,\nJ. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V . Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu,\nM. Yan, and A. Zeng, \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint\narXiv:2204.01691 , 2022.\nM. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog\net al. , \u201cDo as i can, not as i say: Grounding language in robotic affordances,\u201d arXiv preprint arXiv:2204.01691 , 2022.\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al. ,\n\u201cFlamingo: a visual language model for few-shot learning,\u201d Advances in Neural Information Processing Systems ,\nvol. 35, pp. 23 716\u201323 736, 2022.\nA. Allevato, E. S. Short, M. Pryor, and A. Thomaz, \u201cTunenet: One-shot residual tuning for system identification and\nsim-to-real robot task transfer,\u201d in Conference on Robot Learning . PMLR, 2020, pp. 445\u2013455.\nA. Amjad, P. Kordel, and G. Fernandes, \u201cA review on innovation in healthcare sector (telehealth) through artificial\nintelligence,\u201d Sustainability , vol. 15, no. 8, p. 6655, 2023.\nS. An, Z. Lin, Q. Fu, B. Chen, N. Zheng, J.-G. Lou, and D. Zhang, \u201cHow do in-context examples affect compositional\ngeneralization?\u201d arXiv preprint arXiv:2305.04835 , 2023.\nP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V . Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva\net al. , \u201cOn evaluation of embodied navigation agents,\u201d arXiv preprint arXiv:1807.06757 , 2018.\nP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. Van Den Hengel,\n\u201cVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,\u201d in\nProceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp. 3674\u20133683.\nS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \u201cVqa: Visual question answering,\u201d in\nProceedings of the IEEE international conference on computer vision , 2015, pp. 2425\u20132433.", "start_char_idx": 0, "end_char_idx": 2491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c531495-f655-4d17-b0be-6dbf8dcc6ef0": {"__data__": {"id_": "4c531495-f655-4d17-b0be-6dbf8dcc6ef0", "embedding": null, "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61d95ae2-f09d-4399-9da7-ea9bef5476b9", "node_type": "4", "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9d691511ac4ebac5263e186ffc095f5d8e2f0582798c5e07f91ee974662d0e4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4190b0d-afac-4b87-9202-3254270dfb09", "node_type": "1", "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2607c2dcbf57676cb69f2a007d14033c012336190169520638fc7b11e62c859a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ce55b2b-206c-4ec4-a7a0-0b1bb0f01ae3", "node_type": "1", "metadata": {}, "hash": "9c6a95f43009be05eb677311f6be310d2c93a95b3752a07b54e76bf4426f84e5", "class_name": "RelatedNodeInfo"}}, "text": ", \u201cOn evaluation of embodied navigation agents,\u201d arXiv preprint arXiv:1807.06757 , 2018.\nP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. Van Den Hengel,\n\u201cVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,\u201d in\nProceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp. 3674\u20133683.\nS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \u201cVqa: Visual question answering,\u201d in\nProceedings of the IEEE international conference on computer vision , 2015, pp. 2425\u20132433.\nM. Bain, A. Nagrani, G. Varol, and A. Zisserman, \u201cFrozen in time: A joint video and image encoder for end-to-end\nretrieval,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 1728\u20131738.\nB. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune, \u201cVideo\npretraining (vpt): Learning to act by watching unlabeled online videos,\u201d Advances in Neural Information Processing\nSystems , vol. 35, pp. 24 639\u201324 654, 2022.\nD. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva, A. Toshev, and E. Wijmans, \u201cObjectnav\nrevisited: On evaluation of embodied agents navigating to objects,\u201d arXiv preprint arXiv:2006.13171 , 2020.\nK. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine, \u201cZero-shot robotic manipulation with\npretrained image-editing diffusion models,\u201d arXiv preprint arXiv:2310.10639 , 2023.\nA. Blair-Stanek, N. Holzenberger, and B. Van Durme, \u201cCan gpt-3 perform statutory reasoning?\u201d arXiv preprint\narXiv:2302.06100 , 2023.\nK. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y . Zhou, A. Gupta, A. Raju et al. ,\n\u201cRobocat: A self-improving foundation agent for robotic manipulation,\u201d arXiv preprint arXiv:2306.11706 , 2023.\nA. Brohan, N. Brown, J. Carbajal, Y . Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu\net al. , \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817 , 2022.\nA. Brohan, N. Brown, J. Carbajal, Y . Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn et al. ,\n\u201cRt-2: Vision-language-action models transfer web knowledge to robotic control,\u201d arXiv preprint arXiv:2307.15818 ,\n2023.\n55", "start_char_idx": 1869, "end_char_idx": 4233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ce55b2b-206c-4ec4-a7a0-0b1bb0f01ae3": {"__data__": {"id_": "7ce55b2b-206c-4ec4-a7a0-0b1bb0f01ae3", "embedding": null, "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95933316-7a71-427e-b9c0-e06efcad6ae1", "node_type": "4", "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "73b735facbb38bfd038e6db420c175609f3c1e59ed635b0778aa4993d6d9ee06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c531495-f655-4d17-b0be-6dbf8dcc6ef0", "node_type": "1", "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "46680d6a6d92f822914eb4c93338cee7e5393d8ff75434b5d05a0d9f7cc5429e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf96fb4b-4b3d-4713-966b-3a468d45b7ed", "node_type": "1", "metadata": {}, "hash": "596233e7c439a9fbb0b9e6e8245aa9c74d75967fc76cdec34d50c681f0cc9491", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell\net al. , \u201cLanguage models are few-shot learners,\u201d Advances in neural information processing systems , vol. 33, pp.\n1877\u20131901, 2020.\nS. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y . T. Lee, Y . Li, S. Lundberg et al. ,\n\u201cSparks of artificial general intelligence: Early experiments with gpt-4,\u201d arXiv preprint arXiv:2303.12712 , 2023.\nW. Cai, S. Huang, G. Cheng, Y . Long, P. Gao, C. Sun, and H. Dong, \u201cBridging zero-shot object navigation and\nfoundation models through pixel-guided navigation skill,\u201d arXiv preprint arXiv:2309.10309 , 2023.\nM. Carroll, R. Shah, M. K. Ho, T. Griffiths, S. Seshia, P. Abbeel, and A. Dragan, \u201cOn the utility of learning about\nhumans for human-ai coordination,\u201d Advances in neural information processing systems , vol. 32, 2019.\nY . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y . Bisk, \u201cWebQA: Multihop and Multimodal QA,\u201d arXiv preprint\narXiv:2109.00590 , 2021.\nD. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, \u201cObject goal navigation using goal-oriented semantic\nexploration,\u201d Advances in Neural Information Processing Systems , vol. 33, pp. 4247\u20134258, 2020.\nD. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, \u201cNeural topological slam for visual navigation,\u201d in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 12 875\u201312 884.\nG. Chen, Y .-D. Zheng, J. Wang, J. Xu, Y . Huang, J. Pan, Y . Wang, Y . Wang, Y . Qiao, T. Lu, and L. Wang, \u201cVideollm:\nModeling video sequence with large language models,\u201d 2023.\nK. Chen, Q. Huang, H. Palangi, P. Smolensky, K. D. Forbus, and J. Gao, \u201cMapping natural-language problems to\nformal-language solutions using structured neural representations,\u201d in ICML 2020 , July 2020.\nK. Chen, Q. Huang, D. McDuff, X. Gao, H. Palangi, J. Wang, K. Forbus, and J. Gao, \u201cNice:\nNeural image commenting with empathy,\u201d in EMNLP 2021 , October 2021. [Online]. Available: https:\n//www.microsoft.com/en-us/research/publication/nice-neural-image-commenting-with-empathy/\nJ. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, \u201cChatgpt goes to law school,\u201d Available at SSRN , 2023.\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li, X. Wang, M. Dehghani, S. Brahma et al. , \u201cScaling\ninstruction-finetuned language models,\u201d arXiv preprint arXiv:2210.11416 , 2022.", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf96fb4b-4b3d-4713-966b-3a468d45b7ed": {"__data__": {"id_": "cf96fb4b-4b3d-4713-966b-3a468d45b7ed", "embedding": null, "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95933316-7a71-427e-b9c0-e06efcad6ae1", "node_type": "4", "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "73b735facbb38bfd038e6db420c175609f3c1e59ed635b0778aa4993d6d9ee06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ce55b2b-206c-4ec4-a7a0-0b1bb0f01ae3", "node_type": "1", "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "50f4119da7731d5872c1903fbab9529aa37ba665d62ecec4ae97b4ccafb16fba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b9769aa-b3b6-4f55-b50a-7aa3ccbbfb42", "node_type": "1", "metadata": {}, "hash": "32b65e54226be4740ae094d028bc647f2fd301d85e7e8e948524266ec0ee0397", "class_name": "RelatedNodeInfo"}}, "text": "[Online]. Available: https:\n//www.microsoft.com/en-us/research/publication/nice-neural-image-commenting-with-empathy/\nJ. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, \u201cChatgpt goes to law school,\u201d Available at SSRN , 2023.\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li, X. Wang, M. Dehghani, S. Brahma et al. , \u201cScaling\ninstruction-finetuned language models,\u201d arXiv preprint arXiv:2210.11416 , 2022.\nN. C. F. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra,\nH. Kittler, and A. Halpern, \u201cSkin lesion analysis toward melanoma detection: A challenge at the 2017 international\nsymposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic),\u201d in 2018\nIEEE 15th International Symposium on Biomedical Imaging (ISBI 2018) , 2018, pp. 168\u2013172.\nA. Creswell, M. Shanahan, and I. Higgins, \u201cSelection-inference: Exploiting large language models for interpretable\nlogical reasoning,\u201d arXiv preprint arXiv:2205.09712 , 2022.\nB. Cui, A. Lupu, S. Sokota, H. Hu, D. J. Wu, and J. N. Foerster, \u201cAdversarial diversity in\nhanabi,\u201d in The Eleventh International Conference on Learning Representations , 2023. [Online]. Available:\nhttps://openreview.net/forum?id=uLE3WF3-H_5\nG. Dagan, F. Keller, and A. Lascarides, \u201cDynamic planning with a llm,\u201d arXiv preprint arXiv:2308.06391 , 2023.\nW. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, \u201cInstructblip: Towards general-\npurpose vision-language models with instruction tuning,\u201d 2023.\nA. d\u2019Avila Garcez and L. C. Lamb, \u201cNeurosymbolic ai: The 3rd wave,\u201d 2020.\nM. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. VanderBilt,\nM. Wallingford et al. , \u201cRobothor: An open simulation-to-real embodied ai platform,\u201d in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , 2020, pp. 3164\u20133174.\n56", "start_char_idx": 2091, "end_char_idx": 4038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b9769aa-b3b6-4f55-b50a-7aa3ccbbfb42": {"__data__": {"id_": "2b9769aa-b3b6-4f55-b50a-7aa3ccbbfb42", "embedding": null, "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c19c292d-8da0-48de-b26c-aa5c123a337c", "node_type": "4", "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "85ec628562f91f2e6e301947f00c65e8d71cd2db81b7aaf2b47e8687cf65c7a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf96fb4b-4b3d-4713-966b-3a468d45b7ed", "node_type": "1", "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "7bbd79a728a31157bc1bbc4c38f0d2667240a200f4298197d41a168ce6ea61c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8b65c63-ad79-40bf-9e1d-7143ca333727", "node_type": "1", "metadata": {}, "hash": "84f030c281e81275795e21b305fa0ac64609347be2a4e05aeb22fffadcbea16a", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, \u201cA survey for in-context learning,\u201d arXiv\npreprint arXiv:2301.00234 , 2022.\nV . S. Dorbala, G. Sigurdsson, R. Piramuthu, J. Thomason, and G. S. Sukhatme, \u201cClip-nav: Using clip for zero-shot\nvision-and-language navigation,\u201d arXiv preprint arXiv:2211.16649 , 2022.\nV . S. Dorbala, J. F. Mullen Jr, and D. Manocha, \u201cCan an embodied agent find your\" cat-shaped mug\"? llm-based\nzero-shot object navigation,\u201d arXiv preprint arXiv:2303.03480 , 2023.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly et al. , \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d ICLR ,\n2021.\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al. ,\n\u201cPalm-e: An embodied multimodal language model,\u201d arXiv preprint arXiv:2303.03378 , 2023.\nY . Du, M. Yang, P. Florence, F. Xia, A. Wahid, B. Ichter, P. Sermanet, T. Yu, P. Abbeel, J. B. Tenenbaum et al. , \u201cVideo\nlanguage planning,\u201d arXiv preprint arXiv:2310.10625 , 2023.\nN. Dziri, A. Madotto, O. Zaiane, and A. J. Bose, \u201cNeural path hunter: Reducing hallucination in dialogue systems via\npath grounding,\u201d arXiv preprint arXiv:2104.08455 , 2021.\nK. Ehsani, W. Han, A. Herrasti, E. VanderBilt, L. Weihs, E. Kolve, A. Kembhavi, and R. Mottaghi, \u201cManipulathor: A\nframework for visual object manipulation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , 2021, pp. 4497\u20134506.\nD. Fried, R. Hu, V . Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and\nT. Darrell, \u201cSpeaker-follower models for vision-and-language navigation,\u201d in Advances in Neural Information\nProcessing Systems (NIPS) , 2018.\nT.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y . Wang, L. Wang, and Z. Liu, \u201cViolet : End-to-end video-language transformers\nwith masked visual-token modeling,\u201d 2022.\n\u2014\u2014, \u201cAn empirical study of end-to-end video-language transformers with masked visual modeling,\u201d 2023.\nS. Y . Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, \u201cCows on pasture: Baselines and benchmarks for\nlanguage-driven zero-shot object navigation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2023, pp. 23 171\u201323 181.\nJ. Gao, B. Peng, C. Li, J. Li, S. Shayandeh, L. Liden, and H.-Y . Shum, \u201cRobust conversational ai with grounded text\ngeneration,\u201d arXiv preprint arXiv:2009.03457 , 2020.", "start_char_idx": 0, "end_char_idx": 2601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8b65c63-ad79-40bf-9e1d-7143ca333727": {"__data__": {"id_": "e8b65c63-ad79-40bf-9e1d-7143ca333727", "embedding": null, "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c19c292d-8da0-48de-b26c-aa5c123a337c", "node_type": "4", "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "85ec628562f91f2e6e301947f00c65e8d71cd2db81b7aaf2b47e8687cf65c7a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b9769aa-b3b6-4f55-b50a-7aa3ccbbfb42", "node_type": "1", "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "767dbf7640d71c72d8d48dda3940528649950c6d966e81fbb8d413d94efea04e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e42970f-1f45-4073-bef1-75fa29a6f6ba", "node_type": "1", "metadata": {}, "hash": "ecea74453ac365a991943e95edd5fe377a56231cb0093e93b479e720f03c7b74", "class_name": "RelatedNodeInfo"}}, "text": "\u2014\u2014, \u201cAn empirical study of end-to-end video-language transformers with masked visual modeling,\u201d 2023.\nS. Y . Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, \u201cCows on pasture: Baselines and benchmarks for\nlanguage-driven zero-shot object navigation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2023, pp. 23 171\u201323 181.\nJ. Gao, B. Peng, C. Li, J. Li, S. Shayandeh, L. Liden, and H.-Y . Shum, \u201cRobust conversational ai with grounded text\ngeneration,\u201d arXiv preprint arXiv:2009.03457 , 2020.\nJ. Gao, C. Xiong, P. Bennett, and N. Craswell, \u201cNeural approaches to conversational information retrieval,\u201d arXiv\npreprint arXiv:2201.05176 , 2022.\nC. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-P\u00e9rez, \u201cIntegrated task and\nmotion planning,\u201d Annual review of control, robotics, and autonomous systems , vol. 4, pp. 265\u2013293, 2021.\nT. Gervet, S. Chintala, D. Batra, J. Malik, and D. S. Chaplot, \u201cNavigating to objects in the real world,\u201d Science Robotics ,\nvol. 8, no. 79, p. eadf6991, 2023.\nR. Gong, J. Huang, Y . Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S.-C. Zhu et al. , \u201cArnold: A\nbenchmark for language-grounded task learning with continuous states in realistic 3d scenes,\u201d in Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV) , 2023.\nR. Gong, Q. Huang, X. Ma, H. V o, Z. Durante, Y . Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei et al. ,\n\u201cMindagent: Emergent gaming interaction,\u201d arXiv preprint arXiv:2309.09971 , 2023.\nA. Goyal, J. Xu, Y . Guo, V . Blukis, Y .-W. Chao, and D. Fox, \u201cRvt: Robotic view transformer for 3d object manipulation,\u201d\narXiv preprint arXiv:2306.14896 , 2023.\n57", "start_char_idx": 2061, "end_char_idx": 3793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e42970f-1f45-4073-bef1-75fa29a6f6ba": {"__data__": {"id_": "2e42970f-1f45-4073-bef1-75fa29a6f6ba", "embedding": null, "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "058bd4d8-a556-4dcb-ad25-dfac46c57434", "node_type": "4", "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "50302bbe55f9523d6eea09ecc626c2ddf05278c2c98fab9dcbb222e1fe0101a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8b65c63-ad79-40bf-9e1d-7143ca333727", "node_type": "1", "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a4080e067a9a401190c8d9dac1cc067d7d3c66e1e3cce49483e5934bcb7d1195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16151346-725c-44ec-a945-27f9f0a7c6e2", "node_type": "1", "metadata": {}, "hash": "46f88dda9e1459bcfdd550e0370d45835a7343926e3d9cd1eb896c3a1522ea11", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nM. Gramopadhye and D. Szafir, \u201cGenerating executable action plans with environmentally-aware language models,\u201d\narXiv preprint arXiv:2210.04964 , 2022.\nA. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine, and D. Song, \u201cThe false promise of imitating\nproprietary llms,\u201d arXiv preprint arXiv:2305.15717 , 2023.\nL. Gui, Q. Huang, A. Hauptmann, Y . Bisk, and J. Gao, \u201cVlc: Training vision-language transformers from captions,\u201d\nMay 2022.\nL. Gui, B. Wang, Q. Huang, A. Hauptmann, Y . Bisk, and J. Gao, \u201cKat: A knowledge augmented transformer for\nvision-and-language,\u201d in NAACL 2022. Long paper, Oral. , January 2022.\nR. L. Guimar\u00e3es, A. S. de Oliveira, J. A. Fabro, T. Becker, and V . A. Brenner, \u201cRos navigation: Concepts and tutorial,\u201d\nRobot Operating System (ROS) The Complete Reference (Volume 1) , pp. 121\u2013160, 2016.\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, \u201cRetrieval augmented language model pre-training,\u201d in International\nconference on machine learning . PMLR, 2020, pp. 3929\u20133938.\nH. Ha, P. Florence, and S. Song, \u201cScaling up and distilling down: Language-guided robot skill acquisition,\u201d arXiv\npreprint arXiv:2307.14535 , 2023.\nT. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, M. Wulfmeier, J. Humplik, S. Tunyasuvunakool, N. Y .\nSiegel, R. Hafner et al. , \u201cLearning agile soccer skills for a bipedal robot with deep reinforcement learning,\u201d arXiv\npreprint arXiv:2304.13653 , 2023.\nK. He, X. Chen, S. Xie, Y . Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d CVPR ,\n2022.\nS. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, \u201cLearning models for following\nnatural language directions in unknown environments,\u201d arXiv preprint arXiv:1503.05079 , 2015.\nJ. Henrich, S. J. Heine, and A. Norenzayan, \u201cThe weirdest people in the world?\u201d Behavioral and Brain Sciences ,\nvol. 33, no. 2-3, p. 61\u201383, 2010.\nL. B. Hensel, N. Yongsatianchot, P. Torshizi, E. Minucci, and S. Marsella, \u201cLarge language models in textual analysis\nfor gesture selection,\u201d in INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION , 2023, pp. 378\u2013387.\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y . Bai, \u201cRetinagan: An object-aware approach to sim-to-real transfer,\u201d\nin2021 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 2021, pp. 10 920\u201310 926.\nC. Huang, O. Mees, A. Zeng, and W. Burgard, \u201cVisual language maps for robot navigation,\u201d in 2023 IEEE International\nConference on Robotics and Automation (ICRA) . IEEE, 2023, pp. 10 608\u201310 615.\nQ. Huang, J. S. Park, A. Gupta, P. Bennett, R. Gong, S. Som, B. Peng, O. K. Mohammed, C. Pal, Y . Choi et al.", "start_char_idx": 0, "end_char_idx": 2719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16151346-725c-44ec-a945-27f9f0a7c6e2": {"__data__": {"id_": "16151346-725c-44ec-a945-27f9f0a7c6e2", "embedding": null, "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "058bd4d8-a556-4dcb-ad25-dfac46c57434", "node_type": "4", "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "50302bbe55f9523d6eea09ecc626c2ddf05278c2c98fab9dcbb222e1fe0101a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e42970f-1f45-4073-bef1-75fa29a6f6ba", "node_type": "1", "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1408b24c1c7d3aaca571da74749f13fd482a5816ec1bc4d86cc05eade94ba3d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d088a73-aae6-4f4b-98a5-eee8d2397450", "node_type": "1", "metadata": {}, "hash": "49a7fe4932b4edf44a9aa2e90db373267b25ffe03bcf39478ea37ae3d07f5206", "class_name": "RelatedNodeInfo"}}, "text": "378\u2013387.\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y . Bai, \u201cRetinagan: An object-aware approach to sim-to-real transfer,\u201d\nin2021 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 2021, pp. 10 920\u201310 926.\nC. Huang, O. Mees, A. Zeng, and W. Burgard, \u201cVisual language maps for robot navigation,\u201d in 2023 IEEE International\nConference on Robotics and Automation (ICRA) . IEEE, 2023, pp. 10 608\u201310 615.\nQ. Huang, J. S. Park, A. Gupta, P. Bennett, R. Gong, S. Som, B. Peng, O. K. Mohammed, C. Pal, Y . Choi et al. , \u201cArk:\nAugmented reality with knowledge interactive emergent ability,\u201d arXiv preprint arXiv:2305.00970 , 2023.\nW. Huang, P. Abbeel, D. Pathak, and I. Mordatch, \u201cLanguage models as zero-shot planners: Extracting\nactionable knowledge for embodied agents,\u201d in Proceedings of the 39th International Conference on Machine\nLearning , ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato, Eds., vol. 162. PMLR, 17\u201323 Jul 2022, pp. 9118\u20139147. [Online]. Available:\nhttps://proceedings.mlr.press/v162/huang22a.html\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y . Chebotar, P. Sermanet,\nN. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, \u201cInner monologue: Embodied reasoning\nthrough planning with language models,\u201d in arXiv preprint arXiv:2207.05608 , 2022.\nZ. Huang, H. Feng, Z. Chongzhi, L. Sheng, L. Ziwei, and J. Shao, \u201cDolphin: General video interaction platform based\non llms,\u201d 2023, https://github.com/kaleido-lab/dolphin.\n58", "start_char_idx": 2183, "end_char_idx": 3774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d088a73-aae6-4f4b-98a5-eee8d2397450": {"__data__": {"id_": "1d088a73-aae6-4f4b-98a5-eee8d2397450", "embedding": null, "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2ac4098-cd18-4001-8ffa-9ad26be07692", "node_type": "4", "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ae595b954aac4b4f825966dbb3b05f5bcba9c52958b3ed3add4b65ffa7228ffe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16151346-725c-44ec-a945-27f9f0a7c6e2", "node_type": "1", "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9b47aa9ee5f8ab01eff4f23ad2ee23048ab98619b43897f17bd5fd6b9478801c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "790b28cd-c953-4dfe-b43d-685a8c4e0c45", "node_type": "1", "metadata": {}, "hash": "50dc008cc868a60e5bffcb7fdcc28bb987ac51fcf8bd05a33f71ccd762026618", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nK. Ikeuchi, N. Wake, K. Sasabuchi, and J. Takamatsu, \u201cSemantic constraints to represent common sense required in\nhousehold actions for multimodal learning-from-observation robot,\u201d The International Journal of Robotics Research ,\nvol. 0, no. 0, p. 02783649231212929, 0.\nK. Ikeuchi, J. Takamatsu, K. Sasabuchi, N. Wake, and A. Kanehiro, \u201cApplying learning-from-observation to household\nservice robots: three common-sense formulation,\u201d arXiv preprint arXiv:2304.09966 , 2023.\nS. Imani, L. Du, and H. Shrivastava, \u201cMathprompter: Mathematical reasoning using large language models,\u201d arXiv\npreprint arXiv:2303.05398 , 2023.\nS. James and A. J. Davison, \u201cQ-attention: Enabling efficient learning for vision-based robotic manipulation,\u201d IEEE\nRobotics and Automation Letters , vol. 7, no. 2, pp. 1612\u20131619, 2022.\nE. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, \u201cBc-z: Zero-shot task\ngeneralization with robotic imitation learning,\u201d in Conference on Robot Learning . PMLR, 2022, pp. 991\u20131002.\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J. Bang, A. Madotto, and P. Fung, \u201cSurvey of hallucination in\nnatural language generation,\u201d ACM Computing Surveys , vol. 55, no. 12, pp. 1\u201338, 2023.\nY . Jiang, A. Gupta, Z. Zhang, G. Wang, Y . Dou, Y . Chen, L. Fei-Fei, A. Anandkumar, Y . Zhu, and L. Fan, \u201cVima:\nGeneral robot manipulation with multimodal prompts,\u201d arXiv , 2022.\nD. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V . Vanhoucke\net al. , \u201cScalable deep reinforcement learning for vision-based robotic manipulation,\u201d in Conference on Robot\nLearning . PMLR, 2018, pp. 651\u2013673.\nA. Karpathy, A. Joulin, and L. F. Fei-Fei, \u201cDeep fragment embeddings for bidirectional image sentence mapping,\u201d\nAdvances in neural information processing systems , vol. 27, 2014.\nP. Katara, Z. Xian, and K. Fragkiadaki, \u201cGen2sim: Scaling up robot learning in simulation with generative models,\u201d\narXiv preprint arXiv:2310.18308 , 2023.\nL. Ke, X. Li, B. Yonatan, A. Holtzman, Z. Gan, J. Liu, J. Gao, Y . Choi, and S. Srinivasa, \u201cTactical rewind: Self-\ncorrection via backtracking in vision-and-language navigation,\u201d in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2019.\nJ. Kim, J. Kim, and S. Choi, \u201cFlame: Free-form language-based motion synthesis & editing,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 37, no. 7, 2023, pp. 8255\u20138263.\nP.-C. Ko, J. Mao, Y . Du, S.-H. Sun, and J. B. Tenenbaum, \u201cLearning to act from actionless videos through dense\ncorrespondences,\u201d arXiv preprint arXiv:2310.08576 , 2023.\nE. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke, K. Ehsani, D. Gordon, Y . Zhu et al.", "start_char_idx": 0, "end_char_idx": 2842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "790b28cd-c953-4dfe-b43d-685a8c4e0c45": {"__data__": {"id_": "790b28cd-c953-4dfe-b43d-685a8c4e0c45", "embedding": null, "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2ac4098-cd18-4001-8ffa-9ad26be07692", "node_type": "4", "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ae595b954aac4b4f825966dbb3b05f5bcba9c52958b3ed3add4b65ffa7228ffe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d088a73-aae6-4f4b-98a5-eee8d2397450", "node_type": "1", "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9dac96691acc3705eea7358bd755c9994edcadb89d690a3ae3374c17a1618f48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e23fbeb-575d-489f-a39d-a385ef36cb11", "node_type": "1", "metadata": {}, "hash": "f897d7fdf4c0560e8af78abaefda23cf90797e366adf3981d78d4a460c02b940", "class_name": "RelatedNodeInfo"}}, "text": "J. Kim, J. Kim, and S. Choi, \u201cFlame: Free-form language-based motion synthesis & editing,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 37, no. 7, 2023, pp. 8255\u20138263.\nP.-C. Ko, J. Mao, Y . Du, S.-H. Sun, and J. B. Tenenbaum, \u201cLearning to act from actionless videos through dense\ncorrespondences,\u201d arXiv preprint arXiv:2310.08576 , 2023.\nE. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke, K. Ehsani, D. Gordon, Y . Zhu et al. ,\n\u201cAi2-thor: An interactive 3d environment for visual ai,\u201d arXiv preprint arXiv:1712.05474 , 2017.\nR. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y . Kalantidis, L.-J. Li, D. A. Shamma,\nM. Bernstein, and L. Fei-Fei, \u201cVisual genome: Connecting language and vision using crowdsourced dense image\nannotations,\u201d in arXiv:1602.07332 , 2016.\nK. N. Kumar, I. Essa, and S. Ha, \u201cWords into action: Learning diverse humanoid robot behaviors using language guided\niterative motion refinement,\u201d arXiv preprint arXiv:2310.06226 , 2023.\nP. Lee, S. Bubeck, and J. Petro, \u201cBenefits, limits, and risks of gpt-4 as an ai chatbot for medicine,\u201d New England Journal\nof Medicine , vol. 388, no. 13, pp. 1233\u20131239, 2023.\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W.-t. Yih, T. Rockt\u00e4schel et al. ,\n\u201cRetrieval-augmented generation for knowledge-intensive nlp tasks,\u201d Advances in Neural Information Processing\nSystems , vol. 33, pp. 9459\u20139474, 2020.\nB. Li, P. Wu, P. Abbeel, and J. Malik, \u201cInteractive task planning with language models,\u201d arXiv preprint arXiv:2310.10645 ,\n2023.\n59", "start_char_idx": 2361, "end_char_idx": 3974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e23fbeb-575d-489f-a39d-a385ef36cb11": {"__data__": {"id_": "8e23fbeb-575d-489f-a39d-a385ef36cb11", "embedding": null, "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21f56c2-1414-46b0-bc4c-39912d033e8d", "node_type": "4", "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "4c0f7f19f1d616d8c53722a5ae6d0d20300bc5da03451bb22137cb9f44535bf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "790b28cd-c953-4dfe-b43d-685a8c4e0c45", "node_type": "1", "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "efe8798727fa0f4c7739de1c9b092c6fc2a1023468bfd4136075066f1604df36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e49e7ae9-4792-4883-ac15-ceabc46dcf45", "node_type": "1", "metadata": {}, "hash": "8edc032de5fd5f9f10a2f2ae5e6efd9fead5b7e5817431a8fbbd22642f5b143e", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nC. Li, F. Xia, R. Mart\u00edn-Mart\u00edn, M. Lingelbach, S. Srivastava, B. Shen, K. Vainio, C. Gokmen, G. Dharan, T. Jain\net al. , \u201cigibson 2.0: Object-centric simulation for robot learning of everyday household tasks,\u201d arXiv preprint\narXiv:2108.03272 , 2021.\nC. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, \u201cLlava-med: Training a\nlarge language-and-vision assistant for biomedicine in one day,\u201d arXiv preprint arXiv:2306.00890 , 2023.\nG. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, \u201cCamel: Communicative agents for\" mind\"\nexploration of large scale language model society,\u201d arXiv preprint arXiv:2303.17760 , 2023.\nJ. Li, Q. Gao, M. Johnston, X. Gao, X. He, S. Shakiah, H. Shi, R. Ghanadan, and W. Y . Wang, \u201cMastering robot manip-\nulation with multimodal prompts through pretraining and multi-task fine-tuning,\u201d arXiv preprint arXiv:2310.09676 ,\n2023.\nJ. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping language-image pre-training with frozen image encoders\nand large language models,\u201d arXiv preprint arXiv:2301.12597 , 2023.\nK. Li, Y . He, W. Yi, Y . Li, W. Wang, P. Luo, Y . Wang, L. Wang, and Y . Qiao, \u201cVideochat: Chat-centric video\nunderstanding,\u201d arXiv preprint arXiv:2305.06355 , 2023.\nL. Li, Y .-C. Chen, Y . Cheng, Z. Gan, L. Yu, and J. Liu, \u201cHero: Hierarchical encoder for video+language omni-\nrepresentation pre-training,\u201d 2020.\nL. Li, J. Lei, Z. Gan, L. Yu, Y .-C. Chen, R. Pillai, Y . Cheng, L. Zhou, X. E. Wang, W. Y . Wang, T. L. Berg, M. Bansal,\nJ. Liu, L. Wang, and Z. Liu, \u201cValue: A multi-task benchmark for video-and-language understanding evaluation,\u201d\n2021.\nX. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y . Jing, W. Zhang, H. Liu et al. , \u201cVision-language foundation\nmodels as effective robot imitators,\u201d arXiv preprint arXiv:2311.01378 , 2023.\nJ. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, \u201cCode as policies: Language model\nprograms for embodied control,\u201d in arXiv preprint arXiv:2209.07753 , 2022.\nX. Liang, L. Ma, S. Guo, J. Han, H. Xu, S. Ma, and X. Liang, \u201cMo-vln: A multi-task benchmark for open-set zero-shot\nvision-and-language navigation,\u201d arXiv preprint arXiv:2306.10322 , 2023.\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith, \u201cSteve-1: A generative model for text-to-behavior in minecraft,\u201d\narXiv preprint arXiv:2306.00937 , 2023.\nK. Lin, F. Ahmed, L. Li, C.-C. Lin, E. Azarnasab, Z. Yang, J. Wang, L. Liang, Z. Liu, Y .", "start_char_idx": 0, "end_char_idx": 2546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e49e7ae9-4792-4883-ac15-ceabc46dcf45": {"__data__": {"id_": "e49e7ae9-4792-4883-ac15-ceabc46dcf45", "embedding": null, "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21f56c2-1414-46b0-bc4c-39912d033e8d", "node_type": "4", "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "4c0f7f19f1d616d8c53722a5ae6d0d20300bc5da03451bb22137cb9f44535bf6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e23fbeb-575d-489f-a39d-a385ef36cb11", "node_type": "1", "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b96fdb6b8a4e4949c703d32e30f3f27d8d84dd13b8345b0267e3747f5017d0ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3137545-1112-402a-b525-2938c73bdd79", "node_type": "1", "metadata": {}, "hash": "414d1eb3d3d3d1972280c09a017e18c136fe5731ff0a50eef042fc6981862575", "class_name": "RelatedNodeInfo"}}, "text": "X. Liang, L. Ma, S. Guo, J. Han, H. Xu, S. Ma, and X. Liang, \u201cMo-vln: A multi-task benchmark for open-set zero-shot\nvision-and-language navigation,\u201d arXiv preprint arXiv:2306.10322 , 2023.\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith, \u201cSteve-1: A generative model for text-to-behavior in minecraft,\u201d\narXiv preprint arXiv:2306.00937 , 2023.\nK. Lin, F. Ahmed, L. Li, C.-C. Lin, E. Azarnasab, Z. Yang, J. Wang, L. Liang, Z. Liu, Y . Lu, C. Liu, and L. Wang,\n\u201cMm-vid: Advancing video understanding with gpt-4v(ision),\u201d 2023.\nT.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Doll\u00e1r,\n\u201cMicrosoft coco: Common objects in context,\u201d Proceedings of ECCV , 2014.\nC. K. Liu and D. Negrut, \u201cThe role of physics-based simulators in robotics,\u201d Annual Review of Control, Robotics, and\nAutonomous Systems , vol. 4, pp. 35\u201358, 2021.\nH. Liu, C. Li, Q. Wu, and Y . J. Lee, \u201cVisual instruction tuning,\u201d 2023.\nH. Liu, A. Chen, Y . Zhu, A. Swaminathan, A. Kolobov, and C.-A. Cheng, \u201cInteractive robot learning from verbal\ncorrection,\u201d arXiv preprint arXiv:2310.17555 , 2023.\nY . Liu, W. Held, and D. Yang, \u201cDada: Dialect adaptation via dynamic aggregation of linguistic rules,\u201d in Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing , 2023.\nP. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y . N. Wu, S.-C. Zhu, and J. Gao, \u201cChameleon: Plug-and-play\ncompositional reasoning with large language models,\u201d 2023.\n60", "start_char_idx": 2105, "end_char_idx": 3605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3137545-1112-402a-b525-2938c73bdd79": {"__data__": {"id_": "e3137545-1112-402a-b525-2938c73bdd79", "embedding": null, "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8b769cb-8614-491e-b3df-8d1ae8a2c929", "node_type": "4", "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a7d5e38f4863b8e45836b018155dccf1f23ffdd3b7689609f81414e18831a838", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e49e7ae9-4792-4883-ac15-ceabc46dcf45", "node_type": "1", "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a6fbc9296885078bf759f83977a756e8bd09a182957620cb22243a235f0ecbbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e53680b-6d72-44cc-8fed-84d3b04b789a", "node_type": "1", "metadata": {}, "hash": "d55201ec179c029f12a209d6bad7e6e4848f5473d12ea2c7ae56cc6c0ff9b2fb", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nZ. Luo, Z. Durante, L. Li, W. Xie, R. Liu, E. Jin, Z. Huang, L. Y . Li, J. Wu, J. C. Niebles et al. , \u201cMoma-lrg: Language-\nrefined graphs for multi-object multi-actor activity parsing,\u201d Advances in Neural Information Processing Systems ,\nvol. 35, pp. 5282\u20135298, 2022.\nC.-Y . Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong, \u201cSelf-monitoring navigation agent via auxiliary\nprogress estimation,\u201d arXiv preprint arXiv:1901.03035 , 2019.\nC.-Y . Ma, Z. Wu, G. AlRegib, C. Xiong, and Z. Kira, \u201cThe regretful agent: Heuristic-aided navigation through progress\nestimation,\u201d arXiv preprint arXiv:1903.01602 , 2019.\nY . J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y . Zhu, L. Fan, and A. Anandkumar, \u201cEureka:\nHuman-level reward design via coding large language models,\u201d arXiv preprint arXiv:2310.12931 , 2023.\nM. Maaz, H. Rasheed, S. Khan, and F. S. Khan, \u201cVideo-chatgpt: Towards detailed video understanding via large vision\nand language models,\u201d 2023.\nR. Mao, Q. Liu, K. He, W. Li, and E. Cambria, \u201cThe biases of pre-trained language models: An empirical study on\nprompt-based sentiment analysis and emotion detection,\u201d IEEE Transactions on Affective Computing , 2022.\nG. Marcus, \u201cThe next decade in ai: four steps towards robust artificial intelligence,\u201d arXiv preprint arXiv:2002.06177 ,\n2020.\nG. Marcus and E. Davis, Rebooting AI: Building artificial intelligence we can trust . Pantheon, 2019.\nK. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, \u201cOk-vqa: A visual question answering benchmark requiring\nexternal knowledge,\u201d in CVPR , 2019.\nP. Martinez-Gonzalez, S. Oprea, A. Garcia-Garcia, A. Jover-Alvarez, S. Orts-Escolano, and J. Garcia-Rodriguez,\n\u201cUnrealrox: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data\ngeneration,\u201d Virtual Reality , vol. 24, pp. 271\u2013288, 2020.\nJ. Maynez, S. Narayan, B. Bohnet, and R. McDonald, \u201cOn faithfulness and factuality in abstractive summarization,\u201d in\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics , D. Jurafsky, J. Chai,\nN. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 1906\u20131919.\n[Online]. Available: https://aclanthology.org/2020.acl-main.173\nO. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, \u201cCalvin: A benchmark for language-conditioned policy learning\nfor long-horizon robot manipulation tasks,\u201d IEEE Robotics and Automation Letters , vol. 7, no. 3, pp. 7327\u20137334,\n2022.\nMeta Fundamental AI Research (FAIR) Diplomacy Team, A. Bakhtin, N. Brown, E. Dinan, G. Farina, C. Flaherty,\nD. Fried, A. Goff, J. Gray, H. Hu et al. , \u201cHuman-level play in the game of Diplomacy by combining language models\nwith strategic reasoning,\u201d Science , vol. 378, no. 6624, pp. 1067\u20131074, 2022.", "start_char_idx": 0, "end_char_idx": 2867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e53680b-6d72-44cc-8fed-84d3b04b789a": {"__data__": {"id_": "3e53680b-6d72-44cc-8fed-84d3b04b789a", "embedding": null, "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8b769cb-8614-491e-b3df-8d1ae8a2c929", "node_type": "4", "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "a7d5e38f4863b8e45836b018155dccf1f23ffdd3b7689609f81414e18831a838", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3137545-1112-402a-b525-2938c73bdd79", "node_type": "1", "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b684b69a3598dc2e8865ea933ac42e32040356990655c933b2ee718b46d1bf18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8263039-d6db-46a8-b374-df7f7d9a5a09", "node_type": "1", "metadata": {}, "hash": "6c0bb26136f30fab4f18155222a9b517d8709f57e4ff4fb21842160449bf4ecb", "class_name": "RelatedNodeInfo"}}, "text": "Available: https://aclanthology.org/2020.acl-main.173\nO. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, \u201cCalvin: A benchmark for language-conditioned policy learning\nfor long-horizon robot manipulation tasks,\u201d IEEE Robotics and Automation Letters , vol. 7, no. 3, pp. 7327\u20137334,\n2022.\nMeta Fundamental AI Research (FAIR) Diplomacy Team, A. Bakhtin, N. Brown, E. Dinan, G. Farina, C. Flaherty,\nD. Fried, A. Goff, J. Gray, H. Hu et al. , \u201cHuman-level play in the game of Diplomacy by combining language models\nwith strategic reasoning,\u201d Science , vol. 378, no. 6624, pp. 1067\u20131074, 2022.\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, \u201cRethinking the role of\ndemonstrations: What makes in-context learning work?\u201d arXiv preprint arXiv:2202.12837 , 2022.\nM. L. Minsky, \u201cMinsky\u2019s frame system theory,\u201d in Proceedings of the 1975 Workshop on Theoretical Issues in Natural\nLanguage Processing , ser. TINLAP \u201975. USA: Association for Computational Linguistics, 1975, p. 104\u2013116.\n[Online]. Available: https://doi.org/10.3115/980190.980222\nS. Mirchandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. G. Arenas, K. Rao, D. Sadigh, and A. Zeng, \u201cLarge\nlanguage models as general pattern machines,\u201d arXiv preprint arXiv:2307.04721 , 2023.\nP. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu\net al. , \u201cLearning to navigate in complex environments,\u201d arXiv preprint arXiv:1611.03673 , 2016.\nM. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, R. Singh, Y . Guo, H. Mazhar et al. , \u201cOrbit: A unified\nsimulation framework for interactive robot learning environments,\u201d IEEE Robotics and Automation Letters , 2023.\n61", "start_char_idx": 2279, "end_char_idx": 4002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8263039-d6db-46a8-b374-df7f7d9a5a09": {"__data__": {"id_": "f8263039-d6db-46a8-b374-df7f7d9a5a09", "embedding": null, "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8e43f67-dc65-49b6-aac1-822a40b660d2", "node_type": "4", "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2b131d05790c798c9d756e11b59d42f7bc39ebd834b49df34878263571363634", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e53680b-6d72-44cc-8fed-84d3b04b789a", "node_type": "1", "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0ba57ba1177aa94603799d18fc4bb8db77fc252dca107ae035830e76865a8b36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f137748-e707-4064-b7a0-df25fb6b9d61", "node_type": "1", "metadata": {}, "hash": "b9ca0ff7d1e33ce247ab123f96ece1383d2b132e56cdf1eab09e67792c6ecd9a", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nV . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,\nG. Ostrovski et al. , \u201cHuman-level control through deep reinforcement learning,\u201d nature , vol. 518, no. 7540, pp.\n529\u2013533, 2015.\nA. Mousavian, A. Toshev, M. Fiser, J. Kosecka, and J. Davidson, \u201cVisual representations for semantic target driven\nnavigation,\u201d arXiv preprint arXiv:1805.06066 , 2018.\nT. Mu, Z. Ling, F. Xiang, D. Yang, X. Li, S. Tao, Z. Huang, Z. Jia, and H. Su, \u201cManiskill: Generalizable manipulation\nskill benchmark with large-scale demonstrations,\u201d arXiv preprint arXiv:2107.14483 , 2021.\nM. M\u00fcller, V . Casser, J. Lahoud, N. Smith, and B. Ghanem, \u201cSim4cv: A photo-realistic simulator for computer vision\napplications,\u201d International Journal of Computer Vision , vol. 126, pp. 902\u2013919, 2018.\nJ. J. Nay, \u201cLaw informs code: A legal informatics approach to aligning artificial intelligence with humans,\u201d Nw. J. Tech.\n& Intell. Prop. , vol. 20, p. 309, 2022.\nK. Nguyen, D. Dey, C. Brockett, and B. Dolan, \u201cVision-based navigation with language-based assistance via imitation\nlearning with indirect intervention,\u201d arXiv preprint arXiv:1812.04155 , 2018.\nZ. Ni, X.-X. Deng, C. Tai, X.-Y . Zhu, X. Wu, Y .-J. Liu, and L. Zeng, \u201cGrid: Scene-graph-based instruction-driven\nrobotic task planning,\u201d arXiv preprint arXiv:2309.07726 , 2023.\nOpenAI, \u201cGPT-4 technical report,\u201d OpenAI, Tech. Rep., 2023.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al. ,\n\u201cTraining language models to follow instructions with human feedback,\u201d Advances in Neural Information Processing\nSystems , vol. 35, pp. 27 730\u201327 744, 2022.\nA. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan et al. ,\n\u201cOpen x-embodiment: Robotic learning datasets and rt-x models,\u201d arXiv preprint arXiv:2310.08864 , 2023.\nM. Parakh, A. Fong, A. Simeonov, A. Gupta, T. Chen, and P. Agrawal, \u201cHuman-assisted continual robot learning with\nfoundation models,\u201d arXiv preprint arXiv:2309.14321 , 2023.\nJ. S. Park, J. Hessel, K. Chandu, P. P. Liang, X. Lu, P. West, Q. Huang, J. Gao, A. Farhadi, and Y . Choi, \u201cMultimodal\nagent \u2013 localized symbolic knowledge distillation for visual commonsense models,\u201d in NeurIPS 2023 , October 2023.\nJ. S. Park, J. Hessel, K. Chandu, P. P. Liang, X. Lu, P. West, Y . Yu, Q. Huang, J. Gao, A. Farhadi, and Y . Choi,\n\u201cLocalized symbolic knowledge distillation for visual commonsense models,\u201d in Thirty-seventh Conference on\nNeural Information Processing Systems , 2023. [Online]. Available: https://openreview.net/forum?id=V5eG47pyVl\nJ. Park, Q. Huang, Y .", "start_char_idx": 0, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f137748-e707-4064-b7a0-df25fb6b9d61": {"__data__": {"id_": "7f137748-e707-4064-b7a0-df25fb6b9d61", "embedding": null, "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8e43f67-dc65-49b6-aac1-822a40b660d2", "node_type": "4", "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2b131d05790c798c9d756e11b59d42f7bc39ebd834b49df34878263571363634", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8263039-d6db-46a8-b374-df7f7d9a5a09", "node_type": "1", "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e0672f9a02e692e60b795b53e44f94c4631eac7dc3434000cac1cb57cc212295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acf58a5f-764d-4c3b-bf54-5f4eb150a5b9", "node_type": "1", "metadata": {}, "hash": "2a9bc941263f11b52bcd2ad3123ac34c0b387e3c116fb0628fb5c0e5b695918a", "class_name": "RelatedNodeInfo"}}, "text": "Choi, \u201cMultimodal\nagent \u2013 localized symbolic knowledge distillation for visual commonsense models,\u201d in NeurIPS 2023 , October 2023.\nJ. S. Park, J. Hessel, K. Chandu, P. P. Liang, X. Lu, P. West, Y . Yu, Q. Huang, J. Gao, A. Farhadi, and Y . Choi,\n\u201cLocalized symbolic knowledge distillation for visual commonsense models,\u201d in Thirty-seventh Conference on\nNeural Information Processing Systems , 2023. [Online]. Available: https://openreview.net/forum?id=V5eG47pyVl\nJ. Park, Q. Huang, Y . Bisk, J. Yang, S. Som, A. Farhadi, Y . Choi, and J. Gao, \u201cInk: Intensive neural knowledge,\u201d July\n2022.\nJ. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, \u201cGenerative agents: Interactive\nsimulacra of human behavior,\u201d arXiv preprint arXiv:2304.03442 , 2023.\nB. Peng, M. Galley, P. He, H. Cheng, Y . Xie, Y . Hu, Q. Huang, L. Liden, Z. Yu, W. Chen et al. , \u201cCheck your facts\nand try again: Improving large language models with external knowledge and automated feedback,\u201d arXiv preprint\narXiv:2302.12813 , 2023.\nX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, \u201cSim-to-real transfer of robotic control with dynamics\nrandomization,\u201d in 2018 IEEE international conference on robotics and automation (ICRA) . IEEE, 2018, pp.\n3803\u20133810.\nX. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba, \u201cVirtualhome: Simulating household activities via\nprograms,\u201d in 2018 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) , 2018, pp.\n8494\u20138502.\nX. Puig, E. Undersander, A. Szot, M. D. Cote, T.-Y . Yang, R. Partsey, R. Desai, A. W. Clegg, M. Hlavac, S. Y . Min\net al. , \u201cHabitat 3.0: A co-habitat for humans, avatars and robots,\u201d arXiv preprint arXiv:2310.13724 , 2023.\n62", "start_char_idx": 2262, "end_char_idx": 3986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acf58a5f-764d-4c3b-bf54-5f4eb150a5b9": {"__data__": {"id_": "acf58a5f-764d-4c3b-bf54-5f4eb150a5b9", "embedding": null, "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03dd72de-9ab8-4b8b-9111-2f3b82955706", "node_type": "4", "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9d192e8270f644643c191d7cbb6767b8756799828c2a32ce9d502bb5e69d5d5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f137748-e707-4064-b7a0-df25fb6b9d61", "node_type": "1", "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "6207fe27408aa1170c32a8c402a21ac851b846debc709582ebba5f1f78266d4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f84a2e94-2eff-47d5-a308-291c030f4895", "node_type": "1", "metadata": {}, "hash": "82b321c4d749d2b30b8cc211f3262e5d6504f64fbfe2624ed254bd6fe57a0442", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al. ,\n\u201cLearning transferable visual models from natural language supervision,\u201d in International conference on machine\nlearning . PMLR, 2021, pp. 8748\u20138763.\nS. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, \u201cPoni: Potential functions for objectgoal\nnavigation with interaction-free learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2022, pp. 18 890\u201318 900.\nS. S. Raman, V . Cohen, D. Paulius, I. Idrees, E. Rosen, R. Mooney, and S. Tellex, \u201cCape: Corrective actions from\nprecondition errors using large language models,\u201d in 2nd Workshop on Language and Robot Learning: Language as\nGrounding , 2023.\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, \u201cRl-cyclegan: Reinforcement learning aware\nsimulation-to-real,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020,\npp. 11 157\u201311 166.\nV . Raunak, A. Menezes, and M. Junczys-Dowmunt, \u201cThe curious case of hallucinations in neural machine translation,\u201d\narXiv preprint arXiv:2104.06683 , 2021.\nS. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y . Sulsky, J. Kay, J. T.\nSpringenberg et al. , \u201cA generalist agent,\u201d arXiv preprint arXiv:2205.06175 , 2022.\nM. Ren, R. Kiros, and R. Zemel, \u201cExploring models and data for image question answering,\u201d Advances in neural\ninformation processing systems , vol. 28, 2015.\nA. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, \u201cObject hallucination in image captioning,\u201d arXiv\npreprint arXiv:1809.02156 , 2018.\nA. Rosinol, J. J. Leonard, and L. Carlone, \u201cNerf-slam: Real-time dense monocular slam with neural radiance fields,\u201d\narXiv preprint arXiv:2210.13641 , 2022.\nF. Sadeghi and S. Levine, \u201cCad2rl: Real single-image flight without a single real image,\u201d arXiv preprint\narXiv:1611.04201 , 2016.\nD. Saito, K. Sasabuchi, N. Wake, J. Takamatsu, H. Koike, and K. Ikeuchi, \u201cTask-grasping from a demonstrated human\nstrategy,\u201d in 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids) , 2022, pp. 880\u2013887.\nD. Saito, K. Sasabuchi, N. Wake, A. Kanehira, J. Takamatsu, H. Koike, and K. Ikeuchi, \u201cConstraint-aware policy for\ncompliant manipulation,\u201d 2023.\nB. Sarkar, A. Shih, and D. Sadigh, \u201cDiverse conventions for human-AI collaboration,\u201d in Thirty-seventh Conference on\nNeural Information Processing Systems , 2023.\nK. Sasabuchi, N. Wake, and K. Ikeuchi, \u201cTask-oriented motion mapping on robots of various configuration using body\nrole division,\u201d IEEE Robotics and Automation Letters , vol. 6, no. 2, pp. 413\u2013420, 2021.", "start_char_idx": 0, "end_char_idx": 2784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f84a2e94-2eff-47d5-a308-291c030f4895": {"__data__": {"id_": "f84a2e94-2eff-47d5-a308-291c030f4895", "embedding": null, "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03dd72de-9ab8-4b8b-9111-2f3b82955706", "node_type": "4", "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9d192e8270f644643c191d7cbb6767b8756799828c2a32ce9d502bb5e69d5d5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acf58a5f-764d-4c3b-bf54-5f4eb150a5b9", "node_type": "1", "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "e6238570714fc9eeea7ce972f7f640df0404a1c8bcb004974c360de9e315026a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8879b566-f473-4f07-97ef-f66f9dfb8d0a", "node_type": "1", "metadata": {}, "hash": "11478dbd4d586a5147ae043de12479fa903fb28c6e16976c43e07298c6e20c41", "class_name": "RelatedNodeInfo"}}, "text": "880\u2013887.\nD. Saito, K. Sasabuchi, N. Wake, A. Kanehira, J. Takamatsu, H. Koike, and K. Ikeuchi, \u201cConstraint-aware policy for\ncompliant manipulation,\u201d 2023.\nB. Sarkar, A. Shih, and D. Sadigh, \u201cDiverse conventions for human-AI collaboration,\u201d in Thirty-seventh Conference on\nNeural Information Processing Systems , 2023.\nK. Sasabuchi, N. Wake, and K. Ikeuchi, \u201cTask-oriented motion mapping on robots of various configuration using body\nrole division,\u201d IEEE Robotics and Automation Letters , vol. 6, no. 2, pp. 413\u2013420, 2021.\nK. Sasabuchi, D. Saito, A. Kanehira, N. Wake, J. Takamatsu, and K. Ikeuchi, \u201cTask-sequencing simulator: Integrated\nmachine learning to execution simulation for robot manipulation,\u201d arXiv preprint arXiv:2301.01382 , 2023.\nM. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V . Koltun, \u201cMinos: Multimodal indoor simulator for\nnavigation in complex environments,\u201d arXiv preprint arXiv:1712.03931 , 2017.\nM. Savva, A. Kadian, O. Maksymets, Y . Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V . Koltun, J. Malik et al. , \u201cHabitat:\nA platform for embodied ai research,\u201d in Proceedings of the IEEE/CVF international conference on computer vision ,\n2019, pp. 9339\u20139347.\nT. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, \u201cToolformer:\nLanguage models can teach themselves to use tools,\u201d 2023.\nE. Segalis, D. Valevski, D. Lumen, Y . Matias, and Y . Leviathan, \u201cA picture is worth a thousand words: Principled\nrecaptioning improves image generation,\u201d arXiv preprint arXiv:2310.16656 , 2023.\n63", "start_char_idx": 2263, "end_char_idx": 3828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8879b566-f473-4f07-97ef-f66f9dfb8d0a": {"__data__": {"id_": "8879b566-f473-4f07-97ef-f66f9dfb8d0a", "embedding": null, "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61413c3c-f1cd-4f2d-ba03-e8d046214179", "node_type": "4", "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "538475d85608885d6b294f216dd92c8749bad97d1d8fa3b169c67a8d1ff16e99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f84a2e94-2eff-47d5-a308-291c030f4895", "node_type": "1", "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "945c0c408d2cffb4605893f70c5745b06a555fd4baab847f81bb2f27ce1c5a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "838d7d92-0e12-4c6a-9a7c-75a39f614939", "node_type": "1", "metadata": {}, "hash": "7a0f7ef7add3ebe1f11442a7867f5307f04e16215a614c1acec43b585c0f9126", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nB. Shacklett, L. G. Rosenzweig, Z. Xie, B. Sarkar, A. Szot, E. Wijmans, V . Koltun, D. Batra, and K. Fatahalian, \u201cAn\nextensible, data-oriented architecture for high-performance, many-world simulation,\u201d ACM Trans. Graph. , vol. 42,\nno. 4, 2023.\nD. Shah, B. Osi \u00b4nski, S. Levine et al. , \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision,\nand action,\u201d in Conference on Robot Learning . PMLR, 2023, pp. 492\u2013504.\nR. Shah, R. Mart\u00edn-Mart\u00edn, and Y . Zhu, \u201cMutex: Learning unified policies from multimodal task specifications,\u201d arXiv\npreprint arXiv:2309.14320 , 2023.\nS. Shah, D. Dey, C. Lovett, and A. Kapoor, \u201cAirsim: High-fidelity visual and physical simulation for autonomous\nvehicles,\u201d in Field and Service Robotics: Results of the 11th International Conference . Springer, 2018, pp. 621\u2013635.\nP. Sharma, N. Ding, S. Goodman, and R. Soricut, \u201cConceptual captions: A cleaned, hypernymed, image alt-text dataset\nfor automatic image captioning,\u201d Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics , 2018.\nR. Shi, Y . Liu, Y . Ze, S. S. Du, and H. Xu, \u201cUnleashing the power of pre-trained language models for offline\nreinforcement learning,\u201d arXiv preprint arXiv:2310.20587 , 2023.\nM. Shridhar, L. Manuelli, and D. Fox, \u201cPerceiver-actor: A multi-task transformer for robotic manipulation,\u201d in\nConference on Robot Learning . PMLR, 2023, pp. 785\u2013799.\nK. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, \u201cRetrieval augmentation reduces hallucination in conversation,\u201d\narXiv preprint arXiv:2104.07567 , 2021.\nA. Singh, V . Natarajan, M. Shah, Y . Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, \u201cTowards vqa models\nthat can read,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2019, pp.\n8317\u20138326.\nS. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, \u201cSemantic scene completion from a single depth\nimage,\u201d IEEE Conference on Computer Vision and Pattern Recognition , 2017.\nS. A. Sontakke, J. Zhang, S. M. Arnold, K. Pertsch, E. B\u0131y\u0131k, D. Sadigh, C. Finn, and L. Itti, \u201cRoboclip: One\ndemonstration is enough to learn robot policies,\u201d arXiv preprint arXiv:2310.07899 , 2023.\nS. Srivastava, C. Li, M. Lingelbach, R. Mart\u00edn-Mart\u00edn, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen, S. Buch, K. Liu et al. ,\n\u201cBehavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments,\u201d in\nConference on Robot Learning . PMLR, 2022, pp. 477\u2013490.\nJ. Sun, Q. Zhang, Y . Duan, X. Jiang, C. Cheng, and R. Xu, \u201cPrompt, plan, perform: Llm-based humanoid control via\nquantized imitation learning,\u201d arXiv preprint arXiv:2309.11359 , 2023.\nQ. Sun, Y . Fang, L. Wu, X. Wang, and Y . Cao, \u201cEva-clip: Improved training techniques for clip at scale,\u201d arXiv preprint\narXiv:2303.15389 , 2023.", "start_char_idx": 0, "end_char_idx": 2884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "838d7d92-0e12-4c6a-9a7c-75a39f614939": {"__data__": {"id_": "838d7d92-0e12-4c6a-9a7c-75a39f614939", "embedding": null, "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61413c3c-f1cd-4f2d-ba03-e8d046214179", "node_type": "4", "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "538475d85608885d6b294f216dd92c8749bad97d1d8fa3b169c67a8d1ff16e99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8879b566-f473-4f07-97ef-f66f9dfb8d0a", "node_type": "1", "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "859687a6122a4348e611eca4a6c3ec6e4380c5b7544ef59fc86961ad4ff4f087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44566039-a56e-4c9b-9e47-8a4a12716f68", "node_type": "1", "metadata": {}, "hash": "6c5aba29ed0e063b8f01e3aaf6fd15204f8b2b7461d7f2e191e3bc58b42337c6", "class_name": "RelatedNodeInfo"}}, "text": ",\n\u201cBehavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments,\u201d in\nConference on Robot Learning . PMLR, 2022, pp. 477\u2013490.\nJ. Sun, Q. Zhang, Y . Duan, X. Jiang, C. Cheng, and R. Xu, \u201cPrompt, plan, perform: Llm-based humanoid control via\nquantized imitation learning,\u201d arXiv preprint arXiv:2309.11359 , 2023.\nQ. Sun, Y . Fang, L. Wu, X. Wang, and Y . Cao, \u201cEva-clip: Improved training techniques for clip at scale,\u201d arXiv preprint\narXiv:2303.15389 , 2023.\nA. Szot, A. Clegg, E. Undersander, E. Wijmans, Y . Zhao, J. Turner, N. Maestre, M. Mukadam, D. Chaplot, O. Maksymets,\nA. Gokaslan, V . V ondrus, S. Dharur, F. Meier, W. Galuba, A. Chang, Z. Kira, V . Koltun, J. Malik, M. Savva, and\nD. Batra, \u201cHabitat 2.0: Training home assistants to rearrange their habitat,\u201d in Advances in Neural Information\nProcessing Systems (NeurIPS) , 2021.\nJ. Takamatsu, K. Sasabuchi, N. Wake, A. Kanehira, and K. Ikeuchi, \u201cLearning-from-observation system considering\nhardware-level reusability,\u201d arXiv preprint arXiv:2212.09242 , 2022.\nC. Tang, D. Huang, W. Ge, W. Liu, and H. Zhang, \u201cGraspgpt: Leveraging semantic knowledge from a large language\nmodel for task-oriented grasping,\u201d IEEE Robotics and Automation Letters , 2023.\nR. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, \u201cStanford alpaca: An\ninstruction-following llama model,\u201d https://github.com/tatsu-lab/stanford_alpaca, 2023.\n64", "start_char_idx": 2382, "end_char_idx": 3846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44566039-a56e-4c9b-9e47-8a4a12716f68": {"__data__": {"id_": "44566039-a56e-4c9b-9e47-8a4a12716f68", "embedding": null, "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1f43b35-2583-4bd8-a71b-50ed390f8005", "node_type": "4", "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b469731c5c54f13291f850acdb53c2dd29e96e9dbb4d33c2ee4c62b2f8200779", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "838d7d92-0e12-4c6a-9a7c-75a39f614939", "node_type": "1", "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "9e6706c2873810a1f3cacfdb5f07e6be61dc7834407ea8d287bebae8c42a73b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b25350e0-f3b8-4187-bb7b-e004630ca4bf", "node_type": "1", "metadata": {}, "hash": "93fc80f1689268d183ee8bac2fde9706ff5b4330e002fb04624399b05fd5a528", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nH. Teshima, N. Wake, D. Thomas, Y . Nakashima, H. Kawasaki, and K. Ikeuchi, \u201cDeep gesture generation for social\nrobots using type-specific libraries,\u201d in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS) . IEEE, 2022, pp. 8286\u20138291.\nG. Tevet, S. Raab, B. Gordon, Y . Shafir, D. Cohen-Or, and A. H. Bermano, \u201cHuman motion diffusion model,\u201d arXiv\npreprint arXiv:2209.14916 , 2022.\nJ. Thomason, D. Gordan, and Y . Bisk, \u201cShifting the baseline: Single modality performance on visual navigation & qa,\u201d\narXiv preprint arXiv:1811.00613 , 2018.\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, \u201cDomain randomization for transferring deep\nneural networks from simulation to the real world,\u201d in 2017 IEEE/RSJ international conference on intelligent robots\nand systems (IROS) . IEEE, 2017, pp. 23\u201330.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar\net al. , \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971 , 2023.\nN. Tsoi, A. Xiang, P. Yu, S. S. Sohn, G. Schwartz, S. Ramesh, M. Hussein, A. W. Gupta, M. Kapadia, and M. V\u00e1zquez,\n\u201cSean 2.0: Formalizing and generating social situations for robot navigation,\u201d IEEE Robotics and Automation Letters ,\nvol. 7, no. 4, pp. 11 047\u201311 054, 2022.\nN. Wake, R. Arakawa, I. Yanokura, T. Kiyokawa, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi, \u201cA learning-from-\nobservation framework: One-shot robot teaching for grasp-manipulation-release household operations,\u201d in 2021\nIEEE/SICE International Symposium on System Integration (SII) . IEEE, 2021.\nN. Wake, I. Yanokura, K. Sasabuchi, and K. Ikeuchi, \u201cVerbal focus-of-attention system for learning-from-observation,\u201d\nin2021 IEEE International Conference on Robotics and Automation (ICRA) , 2021, pp. 10 377\u201310 384.\nN. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi, \u201cInteractive task encoding system for learning-from-\nobservation,\u201d in 2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM) , 2023, pp.\n1061\u20131066.\n\u2014\u2014, \u201cBias in emotion recognition with chatgpt,\u201d arXiv preprint arXiv:2310.11753 , 2023.\n\u2014\u2014, \u201cGpt models meet robotic applications: Co-speech gesturing chat system,\u201d arXiv preprint arXiv:2306.01741 ,\n2023.\n\u2014\u2014, \u201cGpt-4v(ision) for robotics: Multimodal task planning from human demonstration,\u201d arXiv preprint\narXiv:2311.12015 , 2023.\n\u2014\u2014, \u201cChatgpt empowered long-step robot control in various environments: A case application,\u201d IEEE Access , vol. 11,\npp. 95 060\u201395 078, 2023.\nN. Wake, D. Saito, K. Sasabuchi, H. Koike, and K. Ikeuchi, \u201cText-driven object affordance for guiding grasp-type\nrecognition in multimodal robot teaching,\u201d Machine Vision and Applications , vol. 34, no. 4, p. 58, 2023.", "start_char_idx": 0, "end_char_idx": 2846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b25350e0-f3b8-4187-bb7b-e004630ca4bf": {"__data__": {"id_": "b25350e0-f3b8-4187-bb7b-e004630ca4bf", "embedding": null, "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1f43b35-2583-4bd8-a71b-50ed390f8005", "node_type": "4", "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "b469731c5c54f13291f850acdb53c2dd29e96e9dbb4d33c2ee4c62b2f8200779", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44566039-a56e-4c9b-9e47-8a4a12716f68", "node_type": "1", "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "94f45ae2320f902c614b9ba1946bdb19345d01babdb1f5742897ae1ba9c04d89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecff162a-6e3e-4227-a8af-9db731be6f26", "node_type": "1", "metadata": {}, "hash": "36afa9243d32d4efda92de2ec73cc321a97f45dac172f0c5a047c2967b6a5da0", "class_name": "RelatedNodeInfo"}}, "text": "\u2014\u2014, \u201cGpt models meet robotic applications: Co-speech gesturing chat system,\u201d arXiv preprint arXiv:2306.01741 ,\n2023.\n\u2014\u2014, \u201cGpt-4v(ision) for robotics: Multimodal task planning from human demonstration,\u201d arXiv preprint\narXiv:2311.12015 , 2023.\n\u2014\u2014, \u201cChatgpt empowered long-step robot control in various environments: A case application,\u201d IEEE Access , vol. 11,\npp. 95 060\u201395 078, 2023.\nN. Wake, D. Saito, K. Sasabuchi, H. Koike, and K. Ikeuchi, \u201cText-driven object affordance for guiding grasp-type\nrecognition in multimodal robot teaching,\u201d Machine Vision and Applications , vol. 34, no. 4, p. 58, 2023.\nB. Wang, Q. Huang, B. Deb, A. L. Halfaker, L. Shao, D. McDuff, A. Awadallah, D. Radev, and J. Gao, \u201cLogical\ntransformers: Infusing logical structures into pre-trained language models,\u201d in Proceedings of ACL 2023 , May 2023.\nD. Wang, Q. Huang, M. Jackson, and J. Gao, \u201cRetrieve what you need: A mutual learning framework for open-domain\nquestion answering,\u201d March 2023. [Online]. Available: https://www.microsoft.com/en-us/research/publication/\nretrieve-what-you-need-a-mutual-learning-framework-for-open-domain-question-answering/\nG. Wang, Y . Xie, Y . Jiang, A. Mandlekar, C. Xiao, Y . Zhu, L. Fan, and A. Anandkumar, \u201cV oyager: An open-ended\nembodied agent with large language models,\u201d arXiv preprint arXiv:2305.16291 , 2023.\nL. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y . Lin et al. , \u201cA survey on large\nlanguage model based autonomous agents,\u201d arXiv preprint arXiv:2308.11432 , 2023.\nP. Wang, Q. Wu, C. Shen, A. v. d. Hengel, and A. Dick, \u201cExplicit knowledge-based reasoning for visual question\nanswering,\u201d arXiv preprint arXiv:1511.02570 , 2015.\n65", "start_char_idx": 2245, "end_char_idx": 3938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecff162a-6e3e-4227-a8af-9db731be6f26": {"__data__": {"id_": "ecff162a-6e3e-4227-a8af-9db731be6f26", "embedding": null, "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d32f4023-54c4-4085-bfb3-58698e54ac23", "node_type": "4", "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "dc2ea1f1bdd29e25f542f620f3a93bc80f07bd9b1a655c9d1ad306c47fc02bda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b25350e0-f3b8-4187-bb7b-e004630ca4bf", "node_type": "1", "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0b476008f606cd007d30e52b92881dcea471a47418177c40dc86f7b579e51b34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11abd28-184c-45c5-9b6a-cf5194a5c647", "node_type": "1", "metadata": {}, "hash": "280630fa9eaff14f8164fa4b53369b500783cf361cc16d0529d85a97db8630dc", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nP. Wang, Q. Wu, C. Shen, A. Dick, and A. Van Den Hengel, \u201cFvqa: Fact-based visual question answering,\u201d TPAMI ,\nvol. 40, no. 10, pp. 2413\u20132427, 2017.\nX. Wang, W. Xiong, H. Wang, and W. Y . Wang, \u201cLook before you leap: Bridging model-free and model-based\nreinforcement learning for planned-ahead vision-and-language navigation,\u201d in The European Conference on Computer\nVision (ECCV) , September 2018.\nX. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y .-F. Weng, W. Y . Wang, and L. Zhang, \u201cReinforced cross-modal\nmatching and self-supervised imitation learning for vision-language navigation,\u201d in CVPR 2019 , June 2019.\nY . Wang, Y . He, Y . Li, K. Li, J. Yu, X. Ma, X. Chen, Y . Wang, P. Luo, Z. Liu, Y . Wang, L. Wang, and Y . Qiao, \u201cInternvid:\nA large-scale video-text dataset for multimodal understanding and generation,\u201d 2023.\nY . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, \u201cSelf-instruct: Aligning language\nmodel with self generated instructions,\u201d arXiv preprint arXiv:2212.10560 , 2022.\nY . Wang, Z. Xian, F. Chen, T.-H. Wang, Y . Wang, K. Fragkiadaki, Z. Erickson, D. Held, and C. Gan, \u201cRobogen: Towards\nunleashing infinite data for automated robot learning via generative simulation,\u201d arXiv preprint arXiv:2311.01455 ,\n2023.\nZ. Wang, Y . Chen, T. Liu, Y . Zhu, W. Liang, and S. Huang, \u201cHumanise: Language-conditioned human\nmotion generation in 3d scenes,\u201d in Advances in Neural Information Processing Systems , S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates,\nInc., 2022, pp. 14 959\u201314 971. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2022/file/\n6030db5195150ac86d942186f4abdad8-Paper-Conference.pdf\nZ. Wang, S. Cai, A. Liu, X. Ma, and Y . Liang, \u201cDescribe, explain, plan and select: Interactive planning with large\nlanguage models enables open-world multi-task agents,\u201d arXiv preprint arXiv:2302.01560 , 2023.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou et al. , \u201cChain-of-thought prompting elicits\nreasoning in large language models,\u201d Advances in Neural Information Processing Systems , vol. 35, pp. 24 824\u201324 837,\n2022.\nWorld Health Organization and World Bank, \u201cTracking universal health coverage: First global monitoring report,\u201d\nwww.who.int/healthinfo/universal_health_coverage/report/2015/en, Jun 2015.\nQ. Wu, G. Bansal, J. Zhang, Y . Wu, S. Zhang, E. E. Zhu, B. Li, L. Jiang, X. Zhang, and C. Wang,\n\u201cAutogen: Enabling next-gen llm applications via multi-agent conversation,\u201d Microsoft, Tech. Rep.\nMSR-TR-2023-33, August 2023. [Online]. Available: https://www.microsoft.com/en-us/research/publication/\nautogen-enabling-next-gen-llm-applications-via-multi-agent-conversation-framework/\nZ. Xi, W. Chen, X. Guo, W. He, Y .", "start_char_idx": 0, "end_char_idx": 2856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e11abd28-184c-45c5-9b6a-cf5194a5c647": {"__data__": {"id_": "e11abd28-184c-45c5-9b6a-cf5194a5c647", "embedding": null, "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d32f4023-54c4-4085-bfb3-58698e54ac23", "node_type": "4", "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "dc2ea1f1bdd29e25f542f620f3a93bc80f07bd9b1a655c9d1ad306c47fc02bda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecff162a-6e3e-4227-a8af-9db731be6f26", "node_type": "1", "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c98bd724b814a9e51541ec4e57a161260b0320fee26b42d1ad982ff483d66010", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c804ce2-7a9d-4680-a886-2168ce2a2325", "node_type": "1", "metadata": {}, "hash": "4f121d66ec89784d6a979b4b553257a73163b36fc1df56e48a8acc0e289560a3", "class_name": "RelatedNodeInfo"}}, "text": "35, pp. 24 824\u201324 837,\n2022.\nWorld Health Organization and World Bank, \u201cTracking universal health coverage: First global monitoring report,\u201d\nwww.who.int/healthinfo/universal_health_coverage/report/2015/en, Jun 2015.\nQ. Wu, G. Bansal, J. Zhang, Y . Wu, S. Zhang, E. E. Zhu, B. Li, L. Jiang, X. Zhang, and C. Wang,\n\u201cAutogen: Enabling next-gen llm applications via multi-agent conversation,\u201d Microsoft, Tech. Rep.\nMSR-TR-2023-33, August 2023. [Online]. Available: https://www.microsoft.com/en-us/research/publication/\nautogen-enabling-next-gen-llm-applications-via-multi-agent-conversation-framework/\nZ. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou et al. , \u201cThe rise and potential of\nlarge language model based agents: A survey,\u201d arXiv preprint arXiv:2309.07864 , 2023.\nF. Xia, A. R. Zamir, Z.-Y . He, A. Sax, J. Malik, and S. Savarese, \u201cGibson Env: real-world perception for embodied\nagents,\u201d in Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on . IEEE, 2018.\nM. Xu, P. Huang, W. Yu, S. Liu, X. Zhang, Y . Niu, T. Zhang, F. Xia, J. Tan, and D. Zhao, \u201cCreative robot tool use with\nlarge language models,\u201d arXiv preprint arXiv:2310.13065 , 2023.\nJ. Yang, Y . Dong, S. Liu, B. Li, Z. Wang, C. Jiang, H. Tan, J. Kang, Y . Zhang, K. Zhou et al. , \u201cOctopus: Embodied\nvision-language programmer from environmental feedback,\u201d arXiv preprint arXiv:2310.08588 , 2023.\nK. Yang, S. Ji, T. Zhang, Q. Xie, and S. Ananiadou, \u201cOn the evaluations of chatgpt and emotion-enhanced prompting\nfor mental health analysis,\u201d arXiv preprint arXiv:2304.03347 , 2023.\nZ. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang, \u201cMm-react: Prompting\nchatgpt for multimodal reasoning and action,\u201d 2023.\n66", "start_char_idx": 2224, "end_char_idx": 3993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c804ce2-7a9d-4680-a886-2168ce2a2325": {"__data__": {"id_": "2c804ce2-7a9d-4680-a886-2168ce2a2325", "embedding": null, "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dce8f4b3-83ff-43d4-b590-497cd9cacca3", "node_type": "4", "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "799aaaaa7ba85a198ee940dc26f7a037933135bb4a89e429cbd99a88faf00eb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11abd28-184c-45c5-9b6a-cf5194a5c647", "node_type": "1", "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5de3059f81ee613024f0c1a12d97859c419ac71f12899fbc6c161b1a11dbde6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f79262f0-f7a8-4eb2-a245-40f39e581a44", "node_type": "1", "metadata": {}, "hash": "67420cf43034388d8465108b969c1f8fc86c41549c0658c0e8740b24c4ec097d", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and K. Narasimhan, \u201cTree of thoughts: Deliberate problem\nsolving with large language models,\u201d 2023.\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao, \u201cReact: Synergizing reasoning and acting in\nlanguage models,\u201d 2023.\nQ. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y . Zhou, J. Wang, A. Hu, P. Shi, Y . Shi, C. Li, Y . Xu, H. Chen, J. Tian, Q. Qi,\nJ. Zhang, and F. Huang, \u201cmplug-owl: Modularization empowers large language models with multimodality,\u201d 2023.\nY . Ye, H. You, and J. Du, \u201cImproved trust in human-robot collaboration with chatgpt,\u201d IEEE Access , 2023.\nP. Young, A. Lai, M. Hodosh, and J. Hockenmaier, \u201cFrom image descriptions to visual denotations: New similarity\nmetrics for semantic inference over event descriptions,\u201d Proceedings of the Annual Meeting of the Association for\nComputational Linguistics , 2014.\nJ. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-Li, X. Lv, H. Peng, Z. Yao, X. Zhang, H. Li et al. , \u201cKola: Carefully\nbenchmarking world knowledge of large language models,\u201d arXiv preprint arXiv:2306.09296 , 2023.\nL. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, \u201cModeling context in referring expressions,\u201d in Computer\nVision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,\nPart II 14 . Springer, 2016, pp. 69\u201385.\nW. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L. Chiang, T. Erez, L. Hasenclever, J. Humplik\net al. , \u201cLanguage to rewards for robotic skill synthesis,\u201d arXiv preprint arXiv:2306.08647 , 2023.\nR. Zellers, X. Lu, J. Hessel, Y . Yu, J. S. Park, J. Cao, A. Farhadi, and Y . Choi, \u201cMerlot: Multimodal neural script\nknowledge models,\u201d 2021.\nR. Zellers, J. Lu, X. Lu, Y . Yu, Y . Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and Y . Choi, \u201cMerlot reserve:\nNeural script knowledge through vision and language and sound,\u201d 2022.\nA. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V . Sindhwani\net al. , \u201cTransporter networks: Rearranging the visual world for robotic manipulation,\u201d in Conference on Robot\nLearning . PMLR, 2021, pp. 726\u2013747.\nA. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y . Dong, and J. Tang, \u201cAgenttuning: Enabling generalized agent abilities for\nllms,\u201d 2023.\nL. Zha, Y . Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, and D. Sadigh, \u201cDistilling and retrieving\ngeneralizable knowledge for robot manipulation via language corrections,\u201d arXiv preprint arXiv:2311.10678 , 2023.", "start_char_idx": 0, "end_char_idx": 2602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f79262f0-f7a8-4eb2-a245-40f39e581a44": {"__data__": {"id_": "f79262f0-f7a8-4eb2-a245-40f39e581a44", "embedding": null, "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dce8f4b3-83ff-43d4-b590-497cd9cacca3", "node_type": "4", "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "799aaaaa7ba85a198ee940dc26f7a037933135bb4a89e429cbd99a88faf00eb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c804ce2-7a9d-4680-a886-2168ce2a2325", "node_type": "1", "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "43255c20ad85eae7f299bf9f5e600057b0f8ee04f12c3394195d7b920ab17cb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3404458-2367-4287-ac43-c2735a4501b8", "node_type": "1", "metadata": {}, "hash": "e3b246e01ee937e05e97060fc771b1234e01084e6d33c0617b20c85f9f5c487d", "class_name": "RelatedNodeInfo"}}, "text": "Sindhwani\net al. , \u201cTransporter networks: Rearranging the visual world for robotic manipulation,\u201d in Conference on Robot\nLearning . PMLR, 2021, pp. 726\u2013747.\nA. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y . Dong, and J. Tang, \u201cAgenttuning: Enabling generalized agent abilities for\nllms,\u201d 2023.\nL. Zha, Y . Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, and D. Sadigh, \u201cDistilling and retrieving\ngeneralizable knowledge for robot manipulation via language corrections,\u201d arXiv preprint arXiv:2311.10678 , 2023.\nM. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu, \u201cMotiondiffuse: Text-driven human motion generation\nwith diffusion model,\u201d arXiv preprint arXiv:2208.15001 , 2022.\nS. Zhang, X. Song, Y . Bai, W. Li, Y . Chu, and S. Jiang, \u201cHierarchical object-to-zone graph for object navigation,\u201d in\nProceedings of the IEEE/CVF international conference on computer vision , 2021, pp. 15 130\u201315 140.\nW. Zhao, Y . Zhao, X. Lu, S. Wang, Y . Tong, and B. Qin, \u201cIs chatgpt equipped with emotional dialogue capabilities?\u201d\narXiv preprint arXiv:2304.09582 , 2023.\nY . Zhao, I. Misra, P. Kr\u00e4henb\u00fchl, and R. Girdhar, \u201cLearning video representations from large language models,\u201d in\narXiv preprint arXiv:2212.04501 , 2022.\nL. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E.\nGonzalez, and I. Stoica, \u201cJudging llm-as-a-judge with mt-bench and chatbot arena,\u201d 2023.\nZ. Zhong, J. Cao, S. Gu, S. Xie, W. Gao, L. Luo, Z. Yan, H. Zhao, and G. Zhou, \u201cAssist: Interactive scene nodes for\nscalable and realistic indoor simulation,\u201d arXiv preprint arXiv:2311.06211 , 2023.\nG. Zhou, Y . Hong, and Q. Wu, \u201cNavgpt: Explicit reasoning in vision-and-language navigation with large language\nmodels,\u201d arXiv preprint arXiv:2305.16986 , 2023.\n67", "start_char_idx": 2090, "end_char_idx": 3879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3404458-2367-4287-ac43-c2735a4501b8": {"__data__": {"id_": "d3404458-2367-4287-ac43-c2735a4501b8", "embedding": null, "metadata": {"page_label": "68", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92b143b6-1ce4-41ad-966a-4c14f9ea6e65", "node_type": "4", "metadata": {"page_label": "68", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "074acf300503570406e91650625f23f2e0c181797371217fc6b658194a9a10c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f79262f0-f7a8-4eb2-a245-40f39e581a44", "node_type": "1", "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "c20e34130e7feb195c7232af248e997cad01116733d98b6b20346d1682aceaaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f21e70b4-0313-4f8d-b1b8-571cb3b27c7c", "node_type": "1", "metadata": {}, "hash": "a08504cd08cc0ddbf5d5b7847161d42b06a96a32fcaa301eb17175de46824bb2", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nH. Zhou, M. Ding, W. Peng, M. Tomizuka, L. Shao, and C. Gan, \u201cGeneralizable long-horizon manipulations with large\nlanguage models,\u201d arXiv preprint arXiv:2310.02264 , 2023.\nX. Zhou, R. Girdhar, A. Joulin, P. Kr\u00e4henb\u00fchl, and I. Misra, \u201cDetecting twenty-thousand classes using image-level\nsupervision,\u201d in ECCV , 2022.\nY . Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, \u201cAnalyzing and mitigating object\nhallucination in large vision-language models,\u201d arXiv preprint arXiv:2310.00754 , 2023.\nD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, \u201cMinigpt-4: Enhancing vision-language understanding with\nadvanced large language models,\u201d 2023.\nJ.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, \u201cUnpaired image-to-image translation using cycle-consistent adversarial\nnetworks,\u201d in Proceedings of the IEEE international conference on computer vision , 2017, pp. 2223\u20132232.\nS. Zhu, A. Kimmel, K. E. Bekris, and A. Boularias, \u201cFast model identification via physics engines for data-efficient\npolicy search,\u201d arXiv preprint arXiv:1710.08893 , 2017.\nX. Zhu, J. Wang, L. Zhang, Y . Zhang, R. Gan, J. Zhang, and Y . Yang, \u201cSolving math word problem via cooperative\nreasoning induced language models,\u201d arXiv preprint arXiv:2210.16257 , 2022.\nY . Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, \u201cTarget-driven visual navigation in\nindoor scenes using deep reinforcement learning,\u201d in Robotics and Automation (ICRA), 2017 IEEE International\nConference on . IEEE, 2017, pp. 3357\u20133364.\n68", "start_char_idx": 0, "end_char_idx": 1591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f21e70b4-0313-4f8d-b1b8-571cb3b27c7c": {"__data__": {"id_": "f21e70b4-0313-4f8d-b1b8-571cb3b27c7c", "embedding": null, "metadata": {"page_label": "69", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b83bc826-7b87-4a30-b8fe-f869c1865357", "node_type": "4", "metadata": {"page_label": "69", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5b11436e12248f747420509038951addebf1c12613120936418d3a9d6618387f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3404458-2367-4287-ac43-c2735a4501b8", "node_type": "1", "metadata": {"page_label": "68", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "074acf300503570406e91650625f23f2e0c181797371217fc6b658194a9a10c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0d868f3-d451-45d3-922d-1e421d7f7b67", "node_type": "1", "metadata": {}, "hash": "ad606962fa6f5105f61e03eb9a3c2892183016e7ffcab3ea59aa73895eade8ee", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nAppendix for\nAgent AI\nA GPT-4V Agent Prompt Details\nUnless specified, we use the default system prompt for GPT-4V . We show a detailed description of the process to prompt\nGPT-4V for Minecraft and how we generate GPT-4V\u2019s responses in Fig. 31.\nB GPT-4V for Bleeding Edge\nBleeding Edge is a third person team-based combat game where players attempt to capture objective points or collect\nmore resources than the enemy team. We show an example input and output when prompting GPT-4V for the game\nBleeding Edge in Fig. 32. Compared to Minecraft, we qualitatively find that GPT-4V has a less thorough understanding\nof the visual content and game rules. This is likely due to the (1) the amount of minecraft data present in GPT-4V\u2019s\ntraining data and (2) the visual complexity of Bleeding Edge compared to Minecraft.\nC GPT-4V for Microsoft Fight Simulator\nAs shown in Fig. 33, a GPT-4V based agent can provide the high-level action description for the player in Microsoft\nFlight Simulator. It describes how the player is in the process of flying an aircraft, shown by the cockpit perspective\nand external views of the plane, managing various flight controls and instruments to maintain appropriate airspeed and\naltitude while navigating through the virtual airspace.\nD GPT-4V for Assassin\u2019s Creed Odyssey\nAs shown in Fig. 34, the GPT-4V agent provides the high-level action description of the character in the images being\nengaged in nighttime combat during a historical battle, wielding a spear and utilizing special abilities, such as the Bull\nRush, to fight against multiple enemies marked with level indicators, while managing their health and ability meters\nin a third-person action role-playing setting. GPT-4V also describes that the player is engaged in a large-scale melee\ncombat scenario, controlling a character with red accents on their armor, and is prompted to use a \"Bull Rush Ability\"\nas indicated by the on-screen instructions. The environment is dark and fiery, suggesting a battlefield, and the player\u2019s\ncharacter is facing multiple enemies marked with level indicators and health bars overhead. The action is part of a\ntutorial or combat sequence where the player learns and executes combat abilities.\nE GPT-4V for GEARS of WAR 4\nAs shown in Fig. 35, Based on the sequence of 10 frames of video provided, the GPT-4V agent can provide a general\ndescription of the player\u2019s actions in the video game:\n\u201cThe player is navigating a character through an industrial environment at night, taking strategic cover behind various\nobjects like crates and walls. The character is armed and engages in combat with enemy entities. The player uses\nfirearms and possibly melee attacks, as indicated by close-quarters combat. They appear to manage their ammunition\nand health, indicated by visual cues such as a flashing red indicator on the screen, which often signifies damage to the\ncharacter.\nAdditionally, the player seems to be moving tactically, shifting between different cover positions to advance or defend\nagainst the enemy. There is evidence of a protective shield being activated at some point, which suggests the player\nis responding to incoming fire by using defensive abilities. The character also seems to be reloading their weapon,\nsuggesting an element of resource management during combat. Blood and impact effects indicate successful hits on the\nenemies.\nThe player\u2019s character collaborates with allies, as other characters are seen in the vicinity, engaging in similar combat\nactions. The environment has interactive elements, such as exploding barrels, which the player might use to their\nadvantage.\n69", "start_char_idx": 0, "end_char_idx": 3693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0d868f3-d451-45d3-922d-1e421d7f7b67": {"__data__": {"id_": "b0d868f3-d451-45d3-922d-1e421d7f7b67", "embedding": null, "metadata": {"page_label": "70", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7115dff1-c5cf-4a75-bada-c9b08c6b5d65", "node_type": "4", "metadata": {"page_label": "70", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0c8d0e9c5abd3d4127e1cf3c45073e6fa17b9441c217514d065df0555cd78973", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f21e70b4-0313-4f8d-b1b8-571cb3b27c7c", "node_type": "1", "metadata": {"page_label": "69", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5b11436e12248f747420509038951addebf1c12613120936418d3a9d6618387f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e71b283-4c02-4db1-9bed-5bc9d0ecd13d", "node_type": "1", "metadata": {}, "hash": "617d81b0e2c483a0c6da23af177bb86d8054fb0c804a9e10e33834a0d29e650f", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 31: We provide code snippets to show the complete prompting process for calling GPT-4V for Minecraft videos.\nFirst, we encode the frames of the video and then call the GPT-4V endpoint. The model\u2019s response is shown on the\nbottom of the screen.\n70", "start_char_idx": 0, "end_char_idx": 324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e71b283-4c02-4db1-9bed-5bc9d0ecd13d": {"__data__": {"id_": "4e71b283-4c02-4db1-9bed-5bc9d0ecd13d", "embedding": null, "metadata": {"page_label": "71", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a00d3a8-124b-4c4e-b88b-23fc4f082415", "node_type": "4", "metadata": {"page_label": "71", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5cbfc597d87629760b8eeafc5a31f0b0c5d0c8baa1fc7e244906ea1ce0c3e48f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0d868f3-d451-45d3-922d-1e421d7f7b67", "node_type": "1", "metadata": {"page_label": "70", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "0c8d0e9c5abd3d4127e1cf3c45073e6fa17b9441c217514d065df0555cd78973", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e852deb-c95b-4374-8fd2-22203f03e62f", "node_type": "1", "metadata": {}, "hash": "38874774cc3038b86c880b188f3a5aee0091a0091b5ff42fd74c222110a7f59c", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nPrompt:  \nThese are frames of a video of a Bleeding Edge player ordered from  \nleft to right and top to bottom as a grid. Give a simple, but precise  \ndescription of what the player is doing in 1 sentence. Be speci\ufb01c  \nabout important items, entities, and actions. In your description do  \nnot mention speci\ufb01c frame numbers or the name of the game.  \nVideo input: \nOutput: \nThe player begins by running around the  \nmap, passing through different  \ncheckpoints and interacting with several  \ncapture points, then fights against an  \nenemy player, and finally captures an  \nobjective while being attacked by another  \nenemy. GPT-4-Vision \n \nFigure 32: GPT-4V is able to produce meaningful content for games like Bleeding Edge that have 3rd person viewpoints\nand visually complex scenes. In order to input a large number of frames (48) to GPT-4V , we input the frames as a grid\nwith frame numbers overlaid on each frame (as shown above).\n71", "start_char_idx": 0, "end_char_idx": 1009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e852deb-c95b-4374-8fd2-22203f03e62f": {"__data__": {"id_": "7e852deb-c95b-4374-8fd2-22203f03e62f", "embedding": null, "metadata": {"page_label": "72", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cec41cbf-066a-4510-a9d5-625587e2a77f", "node_type": "4", "metadata": {"page_label": "72", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "647e5d28f84004eef24eaf23fa52afaced5064f6cf69f0264c8ef8a89cab27bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e71b283-4c02-4db1-9bed-5bc9d0ecd13d", "node_type": "1", "metadata": {"page_label": "71", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "5cbfc597d87629760b8eeafc5a31f0b0c5d0c8baa1fc7e244906ea1ce0c3e48f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f49006a7-0846-43c9-801e-fd2d2fb70923", "node_type": "1", "metadata": {}, "hash": "08b99d070fad20c110dc562eee7f3ed1688342c68a69bceba7321c5924fc861f", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 33: GPT-4V is able to produce meaningful content description for Microsoft Fight Simulator games. The\nGPT-4V agent can provide a high-level action description as the player in the flight simulator is in the process of flying\nan aircraft, shown by the cockpit perspective and external views of the plane, managing various flight controls and\ninstruments to maintain appropriate airspeed and altitude while navigating through the virtual airspace. In order to input\na large video, we select a number of key frames (6) to send to GPT-4V . We input each key frame individually and do\nnot use a grid (as shown above).\n72", "start_char_idx": 0, "end_char_idx": 693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f49006a7-0846-43c9-801e-fd2d2fb70923": {"__data__": {"id_": "f49006a7-0846-43c9-801e-fd2d2fb70923", "embedding": null, "metadata": {"page_label": "73", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58c13c4c-dffa-4fc8-b4fe-268526805d24", "node_type": "4", "metadata": {"page_label": "73", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "48549b5cb54edff969b09b4fba992dbc1d5eb8689b4270b128e55d5eb282b0ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e852deb-c95b-4374-8fd2-22203f03e62f", "node_type": "1", "metadata": {"page_label": "72", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "647e5d28f84004eef24eaf23fa52afaced5064f6cf69f0264c8ef8a89cab27bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd7e7980-04bf-407a-be9c-090d101b927d", "node_type": "1", "metadata": {}, "hash": "b91cb99e2b786a7f2de5ae2edbb485ecb38a52f250dda0bcc54374eff6f0b02a", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 34: GPT-4V is able to produce meaningful content description for the game, Assassin\u2019s Creed Odyssey. The\nGPT-4V-based agent can also provide a high-level action description of \"the character in the images [being] engaged\nin nighttime combat during a historical battle, wielding a spear and utilizing special abilities, such as the Bull Rush,\nto fight against multiple enemies marked with level indicators, while managing their health and ability meters in a\nthird-person action role-playing setting.\" In order to input a large video, we select a number of key frames (8) to send to\nGPT-4V . We input each key frame individually and do not use a grid (as shown above).\n73", "start_char_idx": 0, "end_char_idx": 748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd7e7980-04bf-407a-be9c-090d101b927d": {"__data__": {"id_": "fd7e7980-04bf-407a-be9c-090d101b927d", "embedding": null, "metadata": {"page_label": "74", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b49f866a-0639-4cd2-aa2e-0d707579c033", "node_type": "4", "metadata": {"page_label": "74", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "702e5840ad332852728ccdb780492f16b525babc517b2dd2b1dcefbde2874854", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f49006a7-0846-43c9-801e-fd2d2fb70923", "node_type": "1", "metadata": {"page_label": "73", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "48549b5cb54edff969b09b4fba992dbc1d5eb8689b4270b128e55d5eb282b0ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2891904d-8a76-4387-be1f-dad2b6008549", "node_type": "1", "metadata": {}, "hash": "6cc4391f6213e6c45f58d35157497e4abb3080c0fa976cfbb6dfefe498dcc04c", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 35: GPT-4V is able to produce meaningful content description for GEARS of WAR 4 games. The GPT-4V\nagent provides the high-level action description of the player being engaged in a combat scenario within a visually rich,\nfuturistic environment, reminiscent of industrial or military complexes. The character, armored and equipped with a\nfirearm, is navigating through this area, using cover and aiming systems to engage with enemies. Actions captured\ninclude the player character aiming from behind cover, firing upon enemies, taking damage as indicated by visual\ncues on the screen, and performing close-quarters combat. The environment is marked by night-time lighting with\nartificial sources highlighting areas of interest or action. There are also moments where the player\u2019s character is seen\nin a transparent, outlined form, which could denote a special ability or a temporary state like invincibility or stealth.\nIn order to input a large video, we select a number of key frames (10) to send to GPT-4V . We input each key frame\nindividually and do not use a grid (as shown above).\n74", "start_char_idx": 0, "end_char_idx": 1166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2891904d-8a76-4387-be1f-dad2b6008549": {"__data__": {"id_": "2891904d-8a76-4387-be1f-dad2b6008549", "embedding": null, "metadata": {"page_label": "75", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a7070fa-c40f-4d5c-aa1e-514bf592c6df", "node_type": "4", "metadata": {"page_label": "75", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2ec3f6c1a4947c0493b920222a37e29b9ac33749aed5543bef5416c3a680a595", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd7e7980-04bf-407a-be9c-090d101b927d", "node_type": "1", "metadata": {"page_label": "74", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "702e5840ad332852728ccdb780492f16b525babc517b2dd2b1dcefbde2874854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2c9bb47-8605-49c4-b568-b76bbef577dd", "node_type": "1", "metadata": {}, "hash": "3714d7b52079f06dd2ac99749264f7cc47b26103752667efdc34a4edbb8ec754", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nThe scenes depict fast-paced action with a focus on strategic movement, shooting, and situational awareness to overcome\nadversaries and possibly achieve a specific objective within the game.\"\nF GPT-4V for Starfield\nAs showed in Fig. 36, based on the sequence of 10 frames of video provided for Starfield, the GPT-4V agent provides a\ngeneral description of the player\u2019s actions in a video game. According to GPT-4V: \"The images depict a series of events\nwithin the game Starfield, showcasing the player\u2019s interactions within a mining environment. Here\u2019s a description of the\nsequence:\n1) The player stands in a dimly lit underground cavern with stalactites, accompanied by another character in similar\nspace gear. They face a highlighted waypoint, indicating an objective or location of interest.\n2) Moving through an industrial interior, likely a mining station, the player approaches another character. The HUD\n(Heads-Up Display) shows a dialogue line from \"Supervisor Lin,\" which suggests NPC (non-player character) interaction\nand possibly a mission briefing or update.\n3) The player seems to be following the NPC, Supervisor Lin, through the cavern, hinting at a guided segment of\ngameplay where the NPC leads the player to a specific location or task.\n4) A dialogue from Supervisor Lin suggests a decision point or mission update, possibly directing the player to a new\nobjective related to a \"big vein\" of resources.\n5) The player encounters Supervisor Lin again, who provides instruction or information about metal deposits, implying\nthe start of a mining task or exploration of a new area.\n6) In this frame, the player appears to be observing or interacting with a piece of machinery, with another character,\nIsabelle, indicating an action or dialogue related to equipment operation or technical work.\n7) The player looks down at a rough terrain with two glowing objects, which could represent resources to be collected\nor points of interest for the mission.\n8) The player examines a specific resource, indicated by a HUD marker as \"Beryllium Deposit,\" suggesting a resource\ngathering or mining activity.\n9) The player traverses through the caverns, potentially searching for additional resources or navigating to the next\nobjective, as indicated by another character in space gear ahead.\n10) The player is positioned in a darker area of the cavern, with two glowing markers indicating a path or objective\npoints to be reached.\nThroughout these images, the player seems to be engaged in a mission involving exploration, NPC interaction, resource\ncollection, and possibly machinery operation within a mining environment. The HUD elements like waypoints, dialogue\ntext, and mission updates provide context to the player\u2019s actions and the game\u2019s objectives.\"\n75", "start_char_idx": 0, "end_char_idx": 2837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2c9bb47-8605-49c4-b568-b76bbef577dd": {"__data__": {"id_": "a2c9bb47-8605-49c4-b568-b76bbef577dd", "embedding": null, "metadata": {"page_label": "76", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1fdb859b-654d-4324-a676-ad7ea35d7d9b", "node_type": "4", "metadata": {"page_label": "76", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "f1459b0bbc86dcd15a4ec5bf7001a6abe6219431d3589b0678cb13167080e4fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2891904d-8a76-4387-be1f-dad2b6008549", "node_type": "1", "metadata": {"page_label": "75", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "2ec3f6c1a4947c0493b920222a37e29b9ac33749aed5543bef5416c3a680a595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38638883-d6ab-4987-a56f-3ad4b53533d4", "node_type": "1", "metadata": {}, "hash": "745be34a27baaae062cb0394a394c8cf734a8dfd2ea9b002fc74d42ad58fe264", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nFigure 36: GPT-4V is able to produce meaningful content description for the X-Box console game Starfield. The\nGPT-4V agent provides a high-level action description for the space-themed exploration game. In order to input a large\nvideo, we select a number of key frames (10) to send to GPT-4V . We input each key frame individually and do not use a\ngrid (as shown above).\n76", "start_char_idx": 0, "end_char_idx": 444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38638883-d6ab-4987-a56f-3ad4b53533d4": {"__data__": {"id_": "38638883-d6ab-4987-a56f-3ad4b53533d4", "embedding": null, "metadata": {"page_label": "77", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24b856dd-514a-43db-9004-4f3026f302b8", "node_type": "4", "metadata": {"page_label": "77", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "80e6730d87cfeb72600e962dbd0215c3e3a950084a935e61f1931be37bb1c6e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2c9bb47-8605-49c4-b568-b76bbef577dd", "node_type": "1", "metadata": {"page_label": "76", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "f1459b0bbc86dcd15a4ec5bf7001a6abe6219431d3589b0678cb13167080e4fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85100e28-43c5-4028-b79a-9b30d4b80f99", "node_type": "1", "metadata": {}, "hash": "194c280af35488ceee1345e2535fa2217e797c1e2c47f944ef0d17d8de785803", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nAuthor Biographies\nZane Durante PhD Student in Computer Science at Stanford University\nEmail: durante@stanford.edu\nWeb: https://zanedurante.github.io/\nZane Durante is a PhD student at Stanford Vision Lab advised by Dr. Fei-Fei Li. H His research interests include\nvideo understanding, connecting computer vision with natural language, and AI applications in hospital care. He is\nleading an ongoing collaboration between Stanford Medical School and Stanford Vision Lab to develop a dataset of\nnatural language descriptions of video clips of hospital scenes. His PhD work is supported by NSF\u2019s Graduate Research\nFellowship.\nQiuyuan Huang Principal Researcher at Microsoft Research, Redmond, WA, USA.\nEmail: qihua@microsoft.com\nWeb: https://www.microsoft.com/en-us/research/people/qihua/\nQiuyuan Huang is a principal researcher in the deep learning group at Microsoft Research (MSR), Redmond, WA.\nHer current research interests are mainly in the deep learning, multi-modality, and natural language processing areas,\nspecifically on Agent AI for Gaming, Robotics and Healthcare; Knowledge-reasoning Intelligence for Interactive AI;\nNeuro-symbolic Computation for Inference Reasoning; and Large Foundation models for NLP and Multi-modality.\nNaoki Wake Researcher at Microsoft, Redmond, WA, USA.\nEmail: naoki.wake@microsoft.com\nWeb: https://www.microsoft.com/en-us/research/people/nawake/\nNaoki Wake is a researcher in the Applied Robotics Research group at Microsoft. His current research involves\nthe development of multimodal perception systems for robots and co-speech gesturing systems. His past research\nhas spanned auditory neuroscience, neuro-rehabilitation, and speech processing. Naoki received his B.S. degree in\nEngineering in 2014, and his Ph.D. in Information Science and Technology in 2019, both from the University of Tokyo.\nRan Gong PhD Student in Computer Science at University of California, Los Angeles.\nEmail: nikepupu@ucla.edu\nWeb: https://nikepupu.github.io\nRan Gong, is a PhD student at the UCLA VCLA Lab. His research lies in the intersection of Robotics, Computer Vision,\nComputer Graphics, and Machine Learning. His research focuses on embodied simulation and interaction with a goal\nof creating intelligent behaviors that can solve diverse tasks in diverse environments as well as well as the capability of\ncollaborating with humans. He received his B.S. degree in Computer Science and Engineering at the University of\nCalifornia, Los Angeles.\nJae Sung Park PhD Student at University of Washington\nEmail: jspark96@cs.washington.edu\nWeb: https://homes.cs.washington.edu/~jspark96/\nJae Sung is a PhD student advised by Yejin Choi and Ali Farhadi. His research focuses on developing models with\nmultimodal commonsense reasoning. He is interested in equipping models with grounding linguistic concepts to visual\nmodalities, and having them understand multimedia content in a way that humans process the visual information. Jae\nSung received his B.S. degree in Computer Science at University of California, Berkeley.\nBidipta Sarkar Undergraduate Student at Stanford University\nEmail: bidiptas@stanford.edu\nWeb: https://bsarkar321.github.io/\nBidipta Sarkar is a senior undergraduate student at Stanford University and a member of Stanford\u2019s ILIAD lab. His\nresearch focuses on creating AI agents that can interact with their environment and safely work alongside humans and\nother autonomous agents.\nRohan Taori PhD Student in Computer Science at Stanford University\nEmail: rtaori@cs.stanford.edu\nWeb: https://www.rohantaori.com/\n77", "start_char_idx": 0, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85100e28-43c5-4028-b79a-9b30d4b80f99": {"__data__": {"id_": "85100e28-43c5-4028-b79a-9b30d4b80f99", "embedding": null, "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2564698f-3ad0-4c30-b77c-7c7ef22904f3", "node_type": "4", "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "272b67dbf5c0dfe2d0c04156fec596a0fb5c445986e6a2e658aad863e8e3cdae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38638883-d6ab-4987-a56f-3ad4b53533d4", "node_type": "1", "metadata": {"page_label": "77", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "80e6730d87cfeb72600e962dbd0215c3e3a950084a935e61f1931be37bb1c6e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01f75143-86aa-4940-9e35-392a96ce4301", "node_type": "1", "metadata": {}, "hash": "3ed90e04b98e5ccb53147e023e8034aaa3cc8b547bdc96c5df9d91f04aa83e52", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nRohan Taori is a PhD student at the Stanford AI Lab. His research focuses on studying the foundations of machine\nlearning in the context of real-world systems. Most recently, Rohan has pushed forward the frontier of open-source\nlarge language models, finetuning them to be helpful, general-purpose assistants. Rohan is also very interested in\naugmenting language models with multi-modality capability to allow them to reason over images and videos. Rohan\nreceived his Bachelor\u2019s degree in Computer Science at UC Berkeley.\nYusuke Noda Principal Software Engineer. Microsoft Gaming, Redmond, WA.\nEmail: yusuke.noda@microsoft.com\nWeb: https://www.linkedin.com/in/yusuke-noda-908797/\nYusuke Noda is a principal software engineer at Microsoft Gaming Platform. He has led the development of gaming\ninfrastructure for Xbox One and Xbox Cloud Gaming and has over 16 years of experience developing efficient\ninfrastructure for cloud and gaming technologies.\nDemetri Terzopoulos UCLA Distinguished Professor, Academy Award winning computer scientist and entrepreneur.\nEmail: dt@cs.ucla.edu\nWeb: https://web.cs.ucla.edu/~dt/\nDr. Demetri Terzopoulos is a Distinguished Professor and Chancellor\u2019s Professor of Computer Science at the University\nof California, Los Angeles, where he directs the UCLA Computer Graphics & Vision Laboratory. He is also Co-Founder\nand Chief Scientist of V oxelCloud, Inc., a multinational healthcare AI company. He is or was a Guggenheim Fellow, a\nFellow of the Royal Society (FRS) of London and the Royal Society of Canada (FRSC), a Fellow of the Association for\nComputing Machinery (ACM), a Life Fellow of the Institute of Electrical and Electronics Engineers (IEEE), a Fellow\nof the Asia-Pacific Artificial Intelligence Association (AAIA), a Distinguished Fellow of the International Engineering\nand Technology Institute (IETI), a Member of the European Academy of Sciences (EAS) and the New York Academy\nof Sciences (NYAS), and a Life Member of Sigma Xi.\nYejin Choi Wisnner-Slivaka Chair and Brett Helsel Professor at University of Washington, Senior Research Manager\nat Allen Institute for Artificial Intelligence, and MacArthur Fellow.\nEmail: yejin@cs.washington.edu\nWeb: https://homes.cs.washington.edu/~yejin/\nYejin is a Wisnner-Slivaka Chair and Brett Helsel Professor at University of Washington and Senior Research Manager\nat Allen Institute of Artifical Intelligence. She has won the Anita Borg Early Career Award in 2018. She was the\nrecipient of MacArthur Fellow foundation fellowship in 2020. She has received outstanding paper award in AAAI\n2020, Neurips 2021, ICML 2022, and ACL 2023, and the best paper award in NAACL 2022 and ACL 2023. She\nis one of the main organizers of COLM 2024, an academic venue focused on the study of language modeling. Her\nmain research interests are commonsense reasoning in the fields of Natural Language Processing, Machine Learning,\nArtificial Intelligence, with broader interests in Computer Vision and Digital Humanities.\nFei-Fei Li Professor of Computer Science at Stanford University\nEmail: feifeili@stanford.edu\nWeb: https://profiles.stanford.edu/fei-fei-li/\nFei-Fei is the inaugural Sequoia Professor in the Computer Science Department at Stanford University, and Co-Director\nof Stanford\u2019s Human-Centered AI Institute. She served as the Director of Stanford\u2019s AI Lab from 2013 to 2018. And\nduring her sabbatical from Stanford from January 2017 to September 2018, Dr. Li was Vice President at Google and\nserved as Chief Scientist of AI/ML at Google Cloud. Since then she has served as a Board member or advisor in various\npublic or private companies.\nKatsushi Ikeuchi Senior Principal Research Manager at Microsoft and an IEEE Life Fellow. Microsoft Research,\nRedmond, WA. IEEE Life Fellow.\nEmail: katsuike@microsoft.com\nWeb: https://www.microsoft.com/en-us/research/people/katsuike/\nDr. Katsushi Ikeuchi received the BE degree in Mechanical Engineering from Kyoto University in 1973 and the PhD\ndegree in Information Engineering from the University of Tokyo in 1978.", "start_char_idx": 0, "end_char_idx": 4108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01f75143-86aa-4940-9e35-392a96ce4301": {"__data__": {"id_": "01f75143-86aa-4940-9e35-392a96ce4301", "embedding": null, "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2564698f-3ad0-4c30-b77c-7c7ef22904f3", "node_type": "4", "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "272b67dbf5c0dfe2d0c04156fec596a0fb5c445986e6a2e658aad863e8e3cdae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85100e28-43c5-4028-b79a-9b30d4b80f99", "node_type": "1", "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "ecef62e00b4b3b277ac6469f3f344d4ad62d991a567e5270eb7fbd31631dc02b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d60fae33-fb5c-421d-bf64-62b86084a67f", "node_type": "1", "metadata": {}, "hash": "8d17e500eb48f7cfb69279c5c7ca6985ed2577885e4e995949e3094f18ccdd11", "class_name": "RelatedNodeInfo"}}, "text": "She served as the Director of Stanford\u2019s AI Lab from 2013 to 2018. And\nduring her sabbatical from Stanford from January 2017 to September 2018, Dr. Li was Vice President at Google and\nserved as Chief Scientist of AI/ML at Google Cloud. Since then she has served as a Board member or advisor in various\npublic or private companies.\nKatsushi Ikeuchi Senior Principal Research Manager at Microsoft and an IEEE Life Fellow. Microsoft Research,\nRedmond, WA. IEEE Life Fellow.\nEmail: katsuike@microsoft.com\nWeb: https://www.microsoft.com/en-us/research/people/katsuike/\nDr. Katsushi Ikeuchi received the BE degree in Mechanical Engineering from Kyoto University in 1973 and the PhD\ndegree in Information Engineering from the University of Tokyo in 1978. After working at the Artificial Intelligence\nLaboratory, Massachusetts Institute of Technology for three years, Electrotechnical Laboratory, Ministry of International\nTrade and Industry, Japanese government for five years, and Robotics Institute, Carnegie Mellon University for 10 years,\n78", "start_char_idx": 3361, "end_char_idx": 4399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d60fae33-fb5c-421d-bf64-62b86084a67f": {"__data__": {"id_": "d60fae33-fb5c-421d-bf64-62b86084a67f", "embedding": null, "metadata": {"page_label": "79", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a3c9273-ad19-4c9a-9649-c838e6ad875b", "node_type": "4", "metadata": {"page_label": "79", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1c860877db77db02bdc88d1d0a52805ed1f9967a144084f1fb61205a90ae1506", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01f75143-86aa-4940-9e35-392a96ce4301", "node_type": "1", "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "81b3761cd41cb26c9d4ac7c2083cd55eb570b9c19a99738cefe6074fb91396b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b762d901-cfba-4814-8568-81f5e6e63826", "node_type": "1", "metadata": {}, "hash": "84b414d29a4bfe6b5f97a5ee613cba22f2ded2756569b9464e0ba929377bac9c", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nInstitute of Industrial Science, the University of Tokyo for 19 years, he joined Microsoft as a Principal Researcher in\n2015. During this tenure of CMU and UTokyo, he supervised more than 50 PhD students.\nHis research interest spans computer vision, robotics, and computer graphics. In these research fields, he has received\nseveral best paper awards, including the David Marr Prize in computational vision and IEEE Robotics and Automation\nSociety K. S. Fu memorial best transaction paper.\nHis community service includes: general chair of IROS95, ITSC99, IV01, ICCV05, ACCV07, ICCV17; program chair\nof CVPR96, ICCV03, ICRA09, ICPR12, ICCV15; EIC of IJCV (2000-2017), EIC of IJ ITS (2012-2014), associate\neditor of IEEE Trans. RA, IEEE Trans. PAMI; and a distinguished lecturer of IEEE Signal Processing Society in\n2000-2002, IEEE Robotics and Automation Society in 2004-2006 and IEEE Computer Society in 2008-2010.\nThrough these research and society service, he was awarded a (life) fellow from IEEE, IEICE, IPSJ and RSJ. He\nreceived the Distinguished Researcher Award from IEEE-PAMI, the Medal of Honor with Purple Ribbon from Japanese\nEmperor, the Okawa prize from Okawa foundation as well as a couple of research achievement awards from Japanese\nprofessional societies.\nHoi Vo Technical Fellow. Microsoft Gaming and X-Box Emerging Technologies, Redmond, WA.\nEmail: hoiv@microsoft.com\nWeb: https://www.linkedin.com/in/hoi-vo-193420/\nHoi V o is a Technical Fellow from MS Gaming Division focusing on bridging AGI with new Gaming IP. He has played a\npivotal role in establishing the Xbox Cloud Gaming service has led cloud and gaming efforts across teams in Microsoft,\nincluding Microsoft\u2019s Windows Azure platform. His focus is on the optimization of AI models to run efficiently at the\nedge while leveraging the cloud for scenarios that exceed the hardware capabilities, including fine-tuning models to be\nmore engaging with various playing experiences.\nJianfeng Gao Distinguished Scientist and Vice President at Microsoft and an IEEE Fellow. Microsoft Research,\nRedmond, WA. IEEE Fellow.\nEmail: jfgao@microsoft.com\nWeb: https://www.microsoft.com/en-us/research/people/jfgao/\nDr. Jianfeng Gao is Distinguished Scientist and Vice President at Microsoft Research, IEEE Fellow, ACM Distinguished\nMember, and the current head of the Deep Learning Group at Microsoft Research. From 2014 to 2017, he was Partner\nResearch Manager in Business AI at Microsoft AI Research and at Deep Learning Technology Center (DLTC) at\nMicrosoft Research, Redmond. He lead the development of AI solutions to Predictive Sales and Marketing. He also\nworks on deep learning for text and image processing (see ACL/SIGIR 2018 Tutorial, Deep Learning 2017 Tutorial\nand IJCAI 2016 Tutorial or MS internal site) and lead the development of AI systems for dialogue, machine reading\ncomprehension (MRC), and question answering (QA). From 2022, he leads the research of self-improving AI where\nLLMs (e.g., ChatGPT/GPT4) are augmented and adapted for the development of commercial AI systems.\n79", "start_char_idx": 0, "end_char_idx": 3131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b762d901-cfba-4814-8568-81f5e6e63826": {"__data__": {"id_": "b762d901-cfba-4814-8568-81f5e6e63826", "embedding": null, "metadata": {"page_label": "80", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date", "file_path"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c78e45bc-2dee-4d7e-9ede-cd389d66c3fa", "node_type": "4", "metadata": {"page_label": "80", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "acf5a93fb4075c7024faba630f2bce8e9375431a6c78d44920a1a6051905ad3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d60fae33-fb5c-421d-bf64-62b86084a67f", "node_type": "1", "metadata": {"page_label": "79", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}, "hash": "1c860877db77db02bdc88d1d0a52805ed1f9967a144084f1fb61205a90ae1506", "class_name": "RelatedNodeInfo"}}, "text": "Agent AI:\nSurveying the Horizons of Multimodal Interaction A P REPRINT\nAcknowledgements\nWe are especially grateful to Peter Lee, Doug Burger, Desney Tan, Johannes Gehrke, Ryen White, Ece Kamar, Subhojit\nSom, and Kareem Choudhry for their advices, enormous support, and encouragement. We thank to Haiyan Zhang,\nSpencer Perreault, Dave Bignell, Katja Hofmann, Sam Devlin, Shanzheng Tan, Raluca Georgescu, Bill Dolan, Nebojsa\nJojic, Sudha Rao, Adrian Brown, Andrzej Banburski-Fahey, Jianwei Yang for the early insightful discussions and helps\nof Gaming. We appreciate Kiran Muthabatulla, Antonio Criminisi, Tom Cashman, Nguyen Bach, Jennifer Marsman,\nJaron Lanier from Mesh team, OCTO of Microsoft, and Microsoft office team for their mix-reality work, dataset work,\nand their generous helps and feedback for the project. We special thanks to Paul Bennett, Corby Rosset, Michel Galley,\nChenglong Wang, Baolin Peng, Hao Chen, Silviu Cucerzan, Ahmed Awadallah, Saleema Amershi for their suggestion\nand comments for the NLP part. The authors gratefully acknowledge Paul Smolensky, Yonatan Bisk, Kezhen Chen,\nBorui Wang, Liangke Gui, Dingmin Wang, Xin (Eric) Wang, Zhe Gan, Xiaojian Ma, Zilong Zheng, Song-chun Zhu,\nDragomir R. Radev, Daniel McDuff, Harry Shum for the related previous works, comments, suggestions, painstaking\nmultiple reviews of this paper, and their pointers to the literature. Finally, we would like to really appreciate Microsoft\nHolulens team, Microsoft X-box team, and Meta Quest team for their generous provision of the equipment; MSR\nCentral Engineering (CE) team, Microsoft 343 team for the data collection and sharing; Microsoft AOAI and GCR\nteam for their Azure-OpenAI endpoint supporting.\nWe would like to thank our colleagues from Stanford\u2019s Partnership in AI-assisted Care, who helped inform the medical\napplications explored in this work. In particular, we would like to thank Ehsan Adeli, Paul Tang, Amit Kaushal, Roger\nBohn, Kevin Schulman, and Arnold Milstein for their clinical expertise and guidance.\nThis research was supported by Microsoft Research project Fair 2023, Microsoft HackBox 2023, OCTO team.\n80", "start_char_idx": 0, "end_char_idx": 2138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"81cdf377-8113-4980-8b5e-0a2f384af6cd": {"doc_hash": "c942b0b16a6ba63b04a5b1b5f01ec54ca2964287faeb71260f1ca5930281e9d4", "ref_doc_id": "cb9e5f88-8af3-4b52-b78c-1b15532bde96"}, "c5a56d15-5de0-482a-8045-45d63103d8c2": {"doc_hash": "fb7b0a5ed12c2b2888f6e927bc346ca008e4d2fdb8f4132b6e7899c3c4351383", "ref_doc_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d"}, "d2606b0d-d911-4a9a-a32a-cedfd67ba04d": {"doc_hash": "59993d68e8f65ee8595e130f76400ebed6acfedcc1af416ee1fc6e78e7620f4b", "ref_doc_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d"}, "10f95459-622b-4d7c-8a75-d7da83407b25": {"doc_hash": "276a7642b8805811ebfc84f20998500940302f45b30b4ae792738f176d4fdc4f", "ref_doc_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d"}, "2734990a-266e-46d8-afba-ede52abe7fdc": {"doc_hash": "8628a276f818bbbaaac7114f1bcd9677619e969508ee164d465652e0e37339b9", "ref_doc_id": "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d"}, "b6f8ec65-31f6-4edd-9cd8-d2e7dada01b4": {"doc_hash": "9154002315f7df25477c2a4bb3471712b1f04afc8e502358103f245ff49bec29", "ref_doc_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7"}, "341b7487-7583-4265-8d9d-a784e40ca00a": {"doc_hash": "0e7c130756a0b5dfd8581e8a7b4816367090d118079956076126e6cc5296c67a", "ref_doc_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7"}, "80d0e71a-6b8b-4d6c-9574-d9ef431ee753": {"doc_hash": "bbf49ddcfa4f0cfc1a4e5993b1db462af179b0fd3c6b63ea8d9804e93fb61ed6", "ref_doc_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7"}, "068754be-3e1d-4282-bb2c-2a4c5c95cd7c": {"doc_hash": "95cbea69d883b6d1f2047a99ada3669656d28a2c35ede83fbde49b634f67e1d7", "ref_doc_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7"}, "b5fa4170-3305-474b-b507-3540bf92eb79": {"doc_hash": "a8395677a57335474f6aed071b8379486b1f0810ab99a5d872c1f6e5b7b9ca6d", "ref_doc_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7"}, "e4b24378-77d9-46a7-844a-4f5b9176661e": {"doc_hash": "c85f68e990a47f8f24c9a09ec3717160bfbd8709eaf21d6ae157fdd191a128eb", "ref_doc_id": "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7"}, "3a38e7a1-46b1-4515-9da5-66d3076df1a7": {"doc_hash": "e91590c556d044cb84eb2929b71df0cc096667c3a3f490fafc12e03b39a09769", "ref_doc_id": "0c6642c2-3767-4031-aa01-cfae22714085"}, "69852459-d242-4ee8-af63-b1f48ebdfd21": {"doc_hash": "47ed8f1c3219defa8258cbd292a0d1935e6f38bef74a95f0678fd6fc63fb0405", "ref_doc_id": "a97d1adc-c0f6-4420-9548-7b569b72a06f"}, "d1825435-cf21-4e5c-a227-4496aa8a2a3e": {"doc_hash": "3818fc2ee2907410508ff364a602b6a161878d69115022811783b4c690a88e24", "ref_doc_id": "a97d1adc-c0f6-4420-9548-7b569b72a06f"}, "0158dc11-f838-471c-9d82-1355826b0b2c": {"doc_hash": "2ab6aaed5fd292cc5f41fee59b362062aa320eb9718b844cd033557da38177ca", "ref_doc_id": "1f3e4fe8-abc2-494a-96e6-68a2d06962f9"}, "37b7c7e6-02fb-445d-a6cb-354e6c2f46a0": {"doc_hash": "0b78723aefd4a79902751a3277eeb3112263a9d9632d22775f91f8b4c4e921cf", "ref_doc_id": "1f3e4fe8-abc2-494a-96e6-68a2d06962f9"}, "3dc18b0a-0d85-40b5-a7cc-123007d0bb74": {"doc_hash": "61631dca5ce0f8d083f8a40d769915ff665423088a950e8dc28e91a49a123902", "ref_doc_id": "065f9110-844b-4e66-b977-7d5bafb7e81c"}, "3450a045-2bd4-47fb-aee6-4123b6ed71db": {"doc_hash": "80c74a9fa99fa4367d49e37e04b03da371f2e113ebcc7ec65e2686c1322b3979", "ref_doc_id": "065f9110-844b-4e66-b977-7d5bafb7e81c"}, "f0f6d2a8-8a83-45fd-b126-4c144aa7b99c": {"doc_hash": "daf8cc871e90c588cfa0178a2733e893afd284eeea3dff8d4d6c93df36a3decf", "ref_doc_id": "4cd14a75-06c9-41c3-a79c-766096002532"}, "058624aa-3281-48c3-9007-1aa2e0b1c654": {"doc_hash": "2ae0f1ba95b5d0c3feb473b365c3c3b57e4027f4178a8109fd6c3caf48d99721", "ref_doc_id": "bd7fe1c8-bbbe-47c6-9806-26f3d8fd5d05"}, "66aed68a-5b01-468c-8c0c-79a85e5d3fc6": {"doc_hash": "44480c1fb2ed877be145d9c6f29c58b9e1ae04887f05e048529854b21f5f11d3", "ref_doc_id": "dc08dc10-0ffc-43bd-a1a2-b416435300ae"}, "0a474ebd-2604-4648-bf82-3b5260e97a56": {"doc_hash": "51acb918f5f572da82e969b1a37614aa67ffcd38457d242a0777f8044908bd3e", "ref_doc_id": "78ac21d3-d4e2-432f-9d8f-6d5454633ddf"}, "cc4ba41a-d951-4293-9239-3a4140235c12": {"doc_hash": "ff924a2428a3f3545fc7bfcd9d84d5504b7b9be6fb476126ead1e8898105ff96", "ref_doc_id": "78ac21d3-d4e2-432f-9d8f-6d5454633ddf"}, "7ff778eb-45ae-419e-998d-7694c8ca980d": {"doc_hash": "a7186bf97917d7698c8d39f0a13dce85c19db341806028f6550af55c83ce2d6c", "ref_doc_id": "3613fd4e-e733-44ba-ba02-a699034694d7"}, "71d6ff1b-be09-471d-9421-190980a063e8": {"doc_hash": "34b93644802fc5db37a88fe91e6a35fe7754abf494a88f4078c7d17beae59cce", "ref_doc_id": "739862a7-883d-4fc3-ac4f-9ea7e5658add"}, "bc9dfe85-1466-42a3-9d49-cdad7b633955": {"doc_hash": "d5654504313bbdc71bc3f2d076f0ffd73672c9a7b2404e140f570c1c82745170", "ref_doc_id": "72b39cf4-4bff-4439-aaeb-59ae990f2130"}, "1cb0e16d-55e4-416e-94c4-b45d05a3516f": {"doc_hash": "2f7728187a3eba03b98f639c7ba1ecf624207ae670d2dbdc7ad2dbd179ea71c0", "ref_doc_id": "b1e59be4-bdc5-4ed4-9f0c-e6c4b12e308f"}, "427dcd85-6f64-4975-8d5c-f1d74833b537": {"doc_hash": "debf1084f672b794571b42328e5025faf0a854c5508cc43c50f7342acf397ff3", "ref_doc_id": "48d8965c-c3d3-4b0b-86ca-79e67ca0d5fd"}, "58ac2fd7-4809-49ff-85c8-d1e42efae233": {"doc_hash": "d9d377ec700fefae70033dda60a2b328f47b5488b3ce44874da56860352f84f7", "ref_doc_id": "dfe73f08-25c6-4f70-93b2-b3b3104776e4"}, "1c1d2e9f-6e08-49a0-ab8c-0c5854191b6f": {"doc_hash": "e8bb44608d0f08df4b421611b1f72d640899b5b9b8b1c5a77d961b51251416c0", "ref_doc_id": "2b20f27f-24ca-47d6-ba95-591b509dbb83"}, "13b2453f-f279-4d4e-bf74-a051e1e955f6": {"doc_hash": "cd74ea43c42919810cc4a4516705fa0cd415ddf3e6dc53a23182f8a7ff0505af", "ref_doc_id": "2b20f27f-24ca-47d6-ba95-591b509dbb83"}, "88331076-f849-41c8-81eb-82835373c36b": {"doc_hash": "2ca49ade9a55c923ebfb33255db93fb4f20c228aaf46776ab820986858649eb4", "ref_doc_id": "9f01b989-b4a4-4787-ba77-ae4066721911"}, "fbd86485-c5e6-45b9-8356-e0e9b9cbc054": {"doc_hash": "6c003e495751f7a563a892d546b85d24dd9628206619185e5078c2a7bd8ede70", "ref_doc_id": "9f01b989-b4a4-4787-ba77-ae4066721911"}, "2413bbcb-e474-4694-9f29-836ce83b572d": {"doc_hash": "08a2b685c603b3f7fb5b98b6e34e7d8f4a71e29176099ce5e6d9f2c176e9b37b", "ref_doc_id": "230ec16e-1645-47ad-a8e7-19d480d15d33"}, "f9a4fa6c-ce7a-4980-8c8c-873147cff90e": {"doc_hash": "b535d01eb0d9f8058f3a5a01da193e66c116171d217cc443014970c96c132921", "ref_doc_id": "08534bd2-dac8-462b-af06-01f5ccc93a23"}, "595a9f74-c856-4a6d-a667-10b128b02bc6": {"doc_hash": "f088bc6ca9b7cadf4bfbfe3534f07f37025736a558b2c966a4d4e6ee0b359519", "ref_doc_id": "08534bd2-dac8-462b-af06-01f5ccc93a23"}, "852d7686-a25d-4195-b529-775ab3f21c5f": {"doc_hash": "a21fee2ceb6c196e6d275364a94a33b7d15560c32be919e35a67f60ab5ec18a2", "ref_doc_id": "38470abe-515b-4c66-80ba-22edbec22ea8"}, "44b03b3c-2789-44e2-a99e-77f12e8fea54": {"doc_hash": "237d6e82b52fe72ad97ab51eeeef1b4f920afd09ed918333cf6062b48458779b", "ref_doc_id": "6ba492ac-4fa3-492e-858b-b8fe2a35bcdc"}, "c8302bc0-81ee-40f1-bdbf-caccdab84cda": {"doc_hash": "80be00e7cacd94a19e434df536cf1abd6cb7708770b589b2d0e9b4e1289860ef", "ref_doc_id": "a06d8914-305f-4ad4-b739-9c2375dcdc40"}, "29a81eb8-17df-48d1-86d2-40e941b92255": {"doc_hash": "98e272c682ad39ccce822a33fa723a1426d3748c07dc6095a407b5d1cde6dcee", "ref_doc_id": "a06d8914-305f-4ad4-b739-9c2375dcdc40"}, "96e353a9-4908-481d-8e5d-fd05a52f2ef9": {"doc_hash": "0346afcfbba1fd3d235118b8cd1b6126d28d844f9aca27b433e83c9e68c7cd0d", "ref_doc_id": "a623e6a9-e397-44f9-86c9-8f17f27ee947"}, "0914fd40-165c-47b1-a081-58f401544be5": {"doc_hash": "f3a0c5fc31d71f1f0e8ee210fd53b3dcfd7c677af7d36ec50f9f9d11e256d926", "ref_doc_id": "4a7ca561-9225-4e39-82b9-88d6c29a01a4"}, "808c86eb-44f5-4ce9-a86f-91129c1aa6d8": {"doc_hash": "4b9440ff7696eac3a64679cfbb7a4206b76e90b0d301d530002dc2ca37492daf", "ref_doc_id": "9b534062-2e70-4154-bf4b-f355754e47c2"}, "fba36423-407d-4f57-b200-151590f4a123": {"doc_hash": "b57ab1fd0a94841f0d66c4e0c380047b89e0615ebd6e451abda808261eb6368b", "ref_doc_id": "4410f48a-e4ab-4d02-a93c-ec923628a92a"}, "a7136962-c247-477a-84bb-f30c66e24127": {"doc_hash": "5559e77bc4f25f4e627ef90c4fa53fac590e4afb2290b2f09663b9162f553f28", "ref_doc_id": "94d46a4a-e6d9-469f-9b88-0297c7e3d5e8"}, "374b597f-258e-41d9-8109-5fe078eb56b4": {"doc_hash": "cefa1a62b50be57c5684b2e8269cec7004de17d5bd5d5ad4375529f1cf51fc53", "ref_doc_id": "e8ac5fe3-8b1f-445f-a17c-8126f838aa7c"}, "a7837387-ba7d-41b5-baa1-ed78eef23b3a": {"doc_hash": "94618e13610a01460db1d8db8b688d324baa5e48973916057545a6813c55de61", "ref_doc_id": "e8ac5fe3-8b1f-445f-a17c-8126f838aa7c"}, "063ec42a-d491-4d3b-af99-fc71da0fef33": {"doc_hash": "c7abbef0de02121a1be93b6648b3036d2cc6bc7c72ed22e05dc40b0ba018de7f", "ref_doc_id": "04f3fca9-1f41-4558-a9d7-78676f89e1bf"}, "fcce4ea6-d681-43e3-a336-5ea57dc78c55": {"doc_hash": "706a2850d7ff2a13bcb71760f51ed371872fbd3123ac55dc98d50935c18d38e1", "ref_doc_id": "e65f4928-49af-4ee6-bcb2-bf1517b76326"}, "604c584c-ea8d-4dab-bd4c-7804f0945aa7": {"doc_hash": "e95684ec5355375757ca1c2bad6d0615678397660ff5661ff2edee400e7826b9", "ref_doc_id": "d67bf198-a220-4934-8ea7-57b9a7c9ecdc"}, "6bb2238f-9999-4421-9590-421a863d9ac9": {"doc_hash": "e81d64838f5c06f1cfdbecca6b67ef06d790c940b50b5ba615d0efda2672aaba", "ref_doc_id": "c1b36c56-23a4-4da0-888e-b5698d51aad7"}, "a20b837b-4842-4885-95cb-bcdb8f1ebdab": {"doc_hash": "0e70c33193fc985d952f55d72a5f83c5984e0b06ecf3a57edd263eee922272ff", "ref_doc_id": "5bd20468-a0d4-46cd-a56e-60f1dcea63c4"}, "fc550ec8-ce7c-46dc-87df-4f97e580b1ce": {"doc_hash": "492b9b24027e03763858ecedcb798d1667207f7642573cede0392127a68ede29", "ref_doc_id": "54ebdb15-9f28-41b0-806d-a5c4f2a1f802"}, "3ad53d7f-65a4-4999-97c9-eb593ff33d46": {"doc_hash": "193703c36af24f1c803c8f1b60e4554a5c688a90453412a8d43342f1db2701b6", "ref_doc_id": "15dac06e-2c16-4bcb-8f39-bf2e67c2c0e7"}, "1cc1aac5-103f-463c-9e0b-2f4fd891f4a2": {"doc_hash": "249373e07cda350d98a30d352cd8d5181e7e3bb5a185be503c9013ca28f199a5", "ref_doc_id": "0446c95b-6313-4390-94b6-e09f81e40077"}, "6d980368-43e5-4a48-84c1-a9cd2a4879fa": {"doc_hash": "4c563898ac91faa7f1cbff628d82fa1a2f7714f3746403418c7a91186b4fc8c8", "ref_doc_id": "f35eb9bf-0ac9-47f3-837c-3c7d846a7236"}, "c607c463-e403-41d9-8627-ade64ae4bed2": {"doc_hash": "10ff40982610c52c3afcc3b0f58cf7b152bb6293ace1b0ba161fefeab27e0b92", "ref_doc_id": "f35eb9bf-0ac9-47f3-837c-3c7d846a7236"}, "1f607a44-8299-4331-9afe-f4fb156e6ce9": {"doc_hash": "07fe2f01d9a0cd76ce749894c9ed0e34b347f21f4b7dd177e332f87ebec776e2", "ref_doc_id": "5d0789f8-f0c1-4acd-8c77-319eb8b019c4"}, "1c3e12b0-2d14-4d79-8562-f0656b34b158": {"doc_hash": "34e8660971f4b353853202944454c529d855bd97187fb38a713ddeba562de9ad", "ref_doc_id": "08828317-a4c9-4175-bb99-72783dd7978e"}, "85835b48-0796-44f6-bb10-7a0464242cb3": {"doc_hash": "c7fd3ea27492311d031e751d1d29c1d4182bfac2f9f22ddfccee0992deb15d4b", "ref_doc_id": "00455c0b-f906-40f7-b5b7-c7dee663cb48"}, "84112dac-86dd-459a-a869-64404be4bb1b": {"doc_hash": "1c669c412c99924a36daf88c3b70b7a186ee05fd4ff647d33d7abc6247619837", "ref_doc_id": "7a10e64c-0e2b-4c1b-b28b-2f06df29a4b7"}, "49e765db-71cd-465e-861a-1f0d282ec453": {"doc_hash": "efe9eeb707b5c4af056d669952eff77b255f78a2fb60f00abfc28103ab38c8ef", "ref_doc_id": "7007df59-703f-4894-b8e7-1db035832e86"}, "6091070b-ea62-4766-95df-7cf31091e64d": {"doc_hash": "c557e47ee8ce2f5db2b6322c5169cb0173df141388d36e75cfeaddd6cb618382", "ref_doc_id": "14310310-b79d-4265-9928-b3aa27ee2f70"}, "e848314c-ff29-4279-b64f-dd76484ba2f7": {"doc_hash": "eac444011a34d69f9336e65dcec48641b5a520b74f0b298427c905131e23a2b9", "ref_doc_id": "c8850023-e7f3-41c6-a310-c00f090aaa7d"}, "67a74e0f-e3c6-472a-b2ee-15cfd7f42f87": {"doc_hash": "5c6d5fedfdb54c58c43e4e7f40b0839f7bc696705af9319bdb4d891dec7769be", "ref_doc_id": "cedbdaba-b3a4-4ac2-aa20-3208b1c78c81"}, "8e62965f-9708-4b8e-9ea5-d03c111572e8": {"doc_hash": "ed866641d5c3c75658021e4658cb8ca4f6939481b894cb37ab05413da6d1eb07", "ref_doc_id": "d2cb18a4-f1c9-4f1d-953c-41f259daee3e"}, "a861fc75-c1ae-4d50-844b-1d547be3f285": {"doc_hash": "caa02aa31958c2d589f4fca6a2735b1b53f7b26c26a0a1cb187ad212331d8925", "ref_doc_id": "be488adb-e601-4510-a159-4e96ed46ab3f"}, "3adb0812-80a6-4cc7-a963-84f2e047dbf0": {"doc_hash": "20d65827c3f3c23fbc5c12e90d5a19ee735333305066d652535cb571ab83a8cd", "ref_doc_id": "1e6efbdb-6889-4f2e-a21c-6acc585ab376"}, "a2b87b05-baa6-4afb-92a3-07dc0c8b9c8c": {"doc_hash": "b7a61f5ebc94d52945a9380f9fd0b94069a389004349af3cc07580f9012c6700", "ref_doc_id": "96182cd8-481e-4d67-8e85-c35156048fae"}, "0b1e329a-ec59-4717-a8e0-7c396687ab66": {"doc_hash": "9b003fe614803faa9c1fd974a418e2b5dd3b2dbb77e33730fd19787610d6e16e", "ref_doc_id": "f1535bf5-47b2-4404-b642-9bbb965494ba"}, "2a672486-b062-4bf6-b1ce-76a87991fbc0": {"doc_hash": "9ba40227678629d444623bbb526080f27b2b0d7f722eda21390b59764e0ccfda", "ref_doc_id": "a916cabb-d677-4c41-9884-fa54c13efdb4"}, "d9f77a88-be87-4576-832f-a125d49ae211": {"doc_hash": "1c45dd6e5d6945a052b23557fbe3c984c33ca67282379a9d5f828908c87a9e7d", "ref_doc_id": "8c6e7d93-8978-41b4-95e4-6ee94320b0dc"}, "d4190b0d-afac-4b87-9202-3254270dfb09": {"doc_hash": "2607c2dcbf57676cb69f2a007d14033c012336190169520638fc7b11e62c859a", "ref_doc_id": "61d95ae2-f09d-4399-9da7-ea9bef5476b9"}, "4c531495-f655-4d17-b0be-6dbf8dcc6ef0": {"doc_hash": "46680d6a6d92f822914eb4c93338cee7e5393d8ff75434b5d05a0d9f7cc5429e", "ref_doc_id": "61d95ae2-f09d-4399-9da7-ea9bef5476b9"}, "7ce55b2b-206c-4ec4-a7a0-0b1bb0f01ae3": {"doc_hash": "50f4119da7731d5872c1903fbab9529aa37ba665d62ecec4ae97b4ccafb16fba", "ref_doc_id": "95933316-7a71-427e-b9c0-e06efcad6ae1"}, "cf96fb4b-4b3d-4713-966b-3a468d45b7ed": {"doc_hash": "7bbd79a728a31157bc1bbc4c38f0d2667240a200f4298197d41a168ce6ea61c0", "ref_doc_id": "95933316-7a71-427e-b9c0-e06efcad6ae1"}, "2b9769aa-b3b6-4f55-b50a-7aa3ccbbfb42": {"doc_hash": "767dbf7640d71c72d8d48dda3940528649950c6d966e81fbb8d413d94efea04e", "ref_doc_id": "c19c292d-8da0-48de-b26c-aa5c123a337c"}, "e8b65c63-ad79-40bf-9e1d-7143ca333727": {"doc_hash": "a4080e067a9a401190c8d9dac1cc067d7d3c66e1e3cce49483e5934bcb7d1195", "ref_doc_id": "c19c292d-8da0-48de-b26c-aa5c123a337c"}, "2e42970f-1f45-4073-bef1-75fa29a6f6ba": {"doc_hash": "1408b24c1c7d3aaca571da74749f13fd482a5816ec1bc4d86cc05eade94ba3d3", "ref_doc_id": "058bd4d8-a556-4dcb-ad25-dfac46c57434"}, "16151346-725c-44ec-a945-27f9f0a7c6e2": {"doc_hash": "9b47aa9ee5f8ab01eff4f23ad2ee23048ab98619b43897f17bd5fd6b9478801c", "ref_doc_id": "058bd4d8-a556-4dcb-ad25-dfac46c57434"}, "1d088a73-aae6-4f4b-98a5-eee8d2397450": {"doc_hash": "9dac96691acc3705eea7358bd755c9994edcadb89d690a3ae3374c17a1618f48", "ref_doc_id": "d2ac4098-cd18-4001-8ffa-9ad26be07692"}, "790b28cd-c953-4dfe-b43d-685a8c4e0c45": {"doc_hash": "efe8798727fa0f4c7739de1c9b092c6fc2a1023468bfd4136075066f1604df36", "ref_doc_id": "d2ac4098-cd18-4001-8ffa-9ad26be07692"}, "8e23fbeb-575d-489f-a39d-a385ef36cb11": {"doc_hash": "b96fdb6b8a4e4949c703d32e30f3f27d8d84dd13b8345b0267e3747f5017d0ed", "ref_doc_id": "d21f56c2-1414-46b0-bc4c-39912d033e8d"}, "e49e7ae9-4792-4883-ac15-ceabc46dcf45": {"doc_hash": "a6fbc9296885078bf759f83977a756e8bd09a182957620cb22243a235f0ecbbf", "ref_doc_id": "d21f56c2-1414-46b0-bc4c-39912d033e8d"}, "e3137545-1112-402a-b525-2938c73bdd79": {"doc_hash": "b684b69a3598dc2e8865ea933ac42e32040356990655c933b2ee718b46d1bf18", "ref_doc_id": "a8b769cb-8614-491e-b3df-8d1ae8a2c929"}, "3e53680b-6d72-44cc-8fed-84d3b04b789a": {"doc_hash": "0ba57ba1177aa94603799d18fc4bb8db77fc252dca107ae035830e76865a8b36", "ref_doc_id": "a8b769cb-8614-491e-b3df-8d1ae8a2c929"}, "f8263039-d6db-46a8-b374-df7f7d9a5a09": {"doc_hash": "e0672f9a02e692e60b795b53e44f94c4631eac7dc3434000cac1cb57cc212295", "ref_doc_id": "e8e43f67-dc65-49b6-aac1-822a40b660d2"}, "7f137748-e707-4064-b7a0-df25fb6b9d61": {"doc_hash": "6207fe27408aa1170c32a8c402a21ac851b846debc709582ebba5f1f78266d4e", "ref_doc_id": "e8e43f67-dc65-49b6-aac1-822a40b660d2"}, "acf58a5f-764d-4c3b-bf54-5f4eb150a5b9": {"doc_hash": "e6238570714fc9eeea7ce972f7f640df0404a1c8bcb004974c360de9e315026a", "ref_doc_id": "03dd72de-9ab8-4b8b-9111-2f3b82955706"}, "f84a2e94-2eff-47d5-a308-291c030f4895": {"doc_hash": "945c0c408d2cffb4605893f70c5745b06a555fd4baab847f81bb2f27ce1c5a5a", "ref_doc_id": "03dd72de-9ab8-4b8b-9111-2f3b82955706"}, "8879b566-f473-4f07-97ef-f66f9dfb8d0a": {"doc_hash": "859687a6122a4348e611eca4a6c3ec6e4380c5b7544ef59fc86961ad4ff4f087", "ref_doc_id": "61413c3c-f1cd-4f2d-ba03-e8d046214179"}, "838d7d92-0e12-4c6a-9a7c-75a39f614939": {"doc_hash": "9e6706c2873810a1f3cacfdb5f07e6be61dc7834407ea8d287bebae8c42a73b2", "ref_doc_id": "61413c3c-f1cd-4f2d-ba03-e8d046214179"}, "44566039-a56e-4c9b-9e47-8a4a12716f68": {"doc_hash": "94f45ae2320f902c614b9ba1946bdb19345d01babdb1f5742897ae1ba9c04d89", "ref_doc_id": "e1f43b35-2583-4bd8-a71b-50ed390f8005"}, "b25350e0-f3b8-4187-bb7b-e004630ca4bf": {"doc_hash": "0b476008f606cd007d30e52b92881dcea471a47418177c40dc86f7b579e51b34", "ref_doc_id": "e1f43b35-2583-4bd8-a71b-50ed390f8005"}, "ecff162a-6e3e-4227-a8af-9db731be6f26": {"doc_hash": "c98bd724b814a9e51541ec4e57a161260b0320fee26b42d1ad982ff483d66010", "ref_doc_id": "d32f4023-54c4-4085-bfb3-58698e54ac23"}, "e11abd28-184c-45c5-9b6a-cf5194a5c647": {"doc_hash": "5de3059f81ee613024f0c1a12d97859c419ac71f12899fbc6c161b1a11dbde6d", "ref_doc_id": "d32f4023-54c4-4085-bfb3-58698e54ac23"}, "2c804ce2-7a9d-4680-a886-2168ce2a2325": {"doc_hash": "43255c20ad85eae7f299bf9f5e600057b0f8ee04f12c3394195d7b920ab17cb7", "ref_doc_id": "dce8f4b3-83ff-43d4-b590-497cd9cacca3"}, "f79262f0-f7a8-4eb2-a245-40f39e581a44": {"doc_hash": "c20e34130e7feb195c7232af248e997cad01116733d98b6b20346d1682aceaaa", "ref_doc_id": "dce8f4b3-83ff-43d4-b590-497cd9cacca3"}, "d3404458-2367-4287-ac43-c2735a4501b8": {"doc_hash": "074acf300503570406e91650625f23f2e0c181797371217fc6b658194a9a10c8", "ref_doc_id": "92b143b6-1ce4-41ad-966a-4c14f9ea6e65"}, "f21e70b4-0313-4f8d-b1b8-571cb3b27c7c": {"doc_hash": "5b11436e12248f747420509038951addebf1c12613120936418d3a9d6618387f", "ref_doc_id": "b83bc826-7b87-4a30-b8fe-f869c1865357"}, "b0d868f3-d451-45d3-922d-1e421d7f7b67": {"doc_hash": "0c8d0e9c5abd3d4127e1cf3c45073e6fa17b9441c217514d065df0555cd78973", "ref_doc_id": "7115dff1-c5cf-4a75-bada-c9b08c6b5d65"}, "4e71b283-4c02-4db1-9bed-5bc9d0ecd13d": {"doc_hash": "5cbfc597d87629760b8eeafc5a31f0b0c5d0c8baa1fc7e244906ea1ce0c3e48f", "ref_doc_id": "0a00d3a8-124b-4c4e-b88b-23fc4f082415"}, "7e852deb-c95b-4374-8fd2-22203f03e62f": {"doc_hash": "647e5d28f84004eef24eaf23fa52afaced5064f6cf69f0264c8ef8a89cab27bd", "ref_doc_id": "cec41cbf-066a-4510-a9d5-625587e2a77f"}, "f49006a7-0846-43c9-801e-fd2d2fb70923": {"doc_hash": "48549b5cb54edff969b09b4fba992dbc1d5eb8689b4270b128e55d5eb282b0ef", "ref_doc_id": "58c13c4c-dffa-4fc8-b4fe-268526805d24"}, "fd7e7980-04bf-407a-be9c-090d101b927d": {"doc_hash": "702e5840ad332852728ccdb780492f16b525babc517b2dd2b1dcefbde2874854", "ref_doc_id": "b49f866a-0639-4cd2-aa2e-0d707579c033"}, "2891904d-8a76-4387-be1f-dad2b6008549": {"doc_hash": "2ec3f6c1a4947c0493b920222a37e29b9ac33749aed5543bef5416c3a680a595", "ref_doc_id": "8a7070fa-c40f-4d5c-aa1e-514bf592c6df"}, "a2c9bb47-8605-49c4-b568-b76bbef577dd": {"doc_hash": "f1459b0bbc86dcd15a4ec5bf7001a6abe6219431d3589b0678cb13167080e4fb", "ref_doc_id": "1fdb859b-654d-4324-a676-ad7ea35d7d9b"}, "38638883-d6ab-4987-a56f-3ad4b53533d4": {"doc_hash": "80e6730d87cfeb72600e962dbd0215c3e3a950084a935e61f1931be37bb1c6e7", "ref_doc_id": "24b856dd-514a-43db-9004-4f3026f302b8"}, "85100e28-43c5-4028-b79a-9b30d4b80f99": {"doc_hash": "ecef62e00b4b3b277ac6469f3f344d4ad62d991a567e5270eb7fbd31631dc02b", "ref_doc_id": "2564698f-3ad0-4c30-b77c-7c7ef22904f3"}, "01f75143-86aa-4940-9e35-392a96ce4301": {"doc_hash": "81b3761cd41cb26c9d4ac7c2083cd55eb570b9c19a99738cefe6074fb91396b5", "ref_doc_id": "2564698f-3ad0-4c30-b77c-7c7ef22904f3"}, "d60fae33-fb5c-421d-bf64-62b86084a67f": {"doc_hash": "1c860877db77db02bdc88d1d0a52805ed1f9967a144084f1fb61205a90ae1506", "ref_doc_id": "1a3c9273-ad19-4c9a-9649-c838e6ad875b"}, "b762d901-cfba-4814-8568-81f5e6e63826": {"doc_hash": "acf5a93fb4075c7024faba630f2bce8e9375431a6c78d44920a1a6051905ad3c", "ref_doc_id": "c78e45bc-2dee-4d7e-9ede-cd389d66c3fa"}}, "docstore/ref_doc_info": {"cb9e5f88-8af3-4b52-b78c-1b15532bde96": {"node_ids": ["81cdf377-8113-4980-8b5e-0a2f384af6cd"], "metadata": {"page_label": "1", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "e064faac-17f0-4cc0-ac62-5ebc7d18fa7d": {"node_ids": ["c5a56d15-5de0-482a-8045-45d63103d8c2", "d2606b0d-d911-4a9a-a32a-cedfd67ba04d", "10f95459-622b-4d7c-8a75-d7da83407b25", "2734990a-266e-46d8-afba-ede52abe7fdc"], "metadata": {"page_label": "2", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "0cc5dcc6-0d3c-462f-9eb7-a623138f0bf7": {"node_ids": ["b6f8ec65-31f6-4edd-9cd8-d2e7dada01b4", "341b7487-7583-4265-8d9d-a784e40ca00a", "80d0e71a-6b8b-4d6c-9574-d9ef431ee753", "068754be-3e1d-4282-bb2c-2a4c5c95cd7c", "b5fa4170-3305-474b-b507-3540bf92eb79", "e4b24378-77d9-46a7-844a-4f5b9176661e"], "metadata": {"page_label": "3", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "0c6642c2-3767-4031-aa01-cfae22714085": {"node_ids": ["3a38e7a1-46b1-4515-9da5-66d3076df1a7"], "metadata": {"page_label": "4", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "a97d1adc-c0f6-4420-9548-7b569b72a06f": {"node_ids": ["69852459-d242-4ee8-af63-b1f48ebdfd21", "d1825435-cf21-4e5c-a227-4496aa8a2a3e"], "metadata": {"page_label": "5", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "1f3e4fe8-abc2-494a-96e6-68a2d06962f9": {"node_ids": ["0158dc11-f838-471c-9d82-1355826b0b2c", "37b7c7e6-02fb-445d-a6cb-354e6c2f46a0"], "metadata": {"page_label": "6", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "065f9110-844b-4e66-b977-7d5bafb7e81c": {"node_ids": ["3dc18b0a-0d85-40b5-a7cc-123007d0bb74", "3450a045-2bd4-47fb-aee6-4123b6ed71db"], "metadata": {"page_label": "7", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "4cd14a75-06c9-41c3-a79c-766096002532": {"node_ids": ["f0f6d2a8-8a83-45fd-b126-4c144aa7b99c"], "metadata": {"page_label": "8", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "bd7fe1c8-bbbe-47c6-9806-26f3d8fd5d05": {"node_ids": ["058624aa-3281-48c3-9007-1aa2e0b1c654"], "metadata": {"page_label": "9", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "dc08dc10-0ffc-43bd-a1a2-b416435300ae": {"node_ids": ["66aed68a-5b01-468c-8c0c-79a85e5d3fc6"], "metadata": {"page_label": "10", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "78ac21d3-d4e2-432f-9d8f-6d5454633ddf": {"node_ids": ["0a474ebd-2604-4648-bf82-3b5260e97a56", "cc4ba41a-d951-4293-9239-3a4140235c12"], "metadata": {"page_label": "11", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "3613fd4e-e733-44ba-ba02-a699034694d7": {"node_ids": ["7ff778eb-45ae-419e-998d-7694c8ca980d"], "metadata": {"page_label": "12", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "739862a7-883d-4fc3-ac4f-9ea7e5658add": {"node_ids": ["71d6ff1b-be09-471d-9421-190980a063e8"], "metadata": {"page_label": "13", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "72b39cf4-4bff-4439-aaeb-59ae990f2130": {"node_ids": ["bc9dfe85-1466-42a3-9d49-cdad7b633955"], "metadata": {"page_label": "14", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "b1e59be4-bdc5-4ed4-9f0c-e6c4b12e308f": {"node_ids": ["1cb0e16d-55e4-416e-94c4-b45d05a3516f"], "metadata": {"page_label": "15", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "48d8965c-c3d3-4b0b-86ca-79e67ca0d5fd": {"node_ids": ["427dcd85-6f64-4975-8d5c-f1d74833b537"], "metadata": {"page_label": "16", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "dfe73f08-25c6-4f70-93b2-b3b3104776e4": {"node_ids": ["58ac2fd7-4809-49ff-85c8-d1e42efae233"], "metadata": {"page_label": "17", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "2b20f27f-24ca-47d6-ba95-591b509dbb83": {"node_ids": ["1c1d2e9f-6e08-49a0-ab8c-0c5854191b6f", "13b2453f-f279-4d4e-bf74-a051e1e955f6"], "metadata": {"page_label": "18", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "9f01b989-b4a4-4787-ba77-ae4066721911": {"node_ids": ["88331076-f849-41c8-81eb-82835373c36b", "fbd86485-c5e6-45b9-8356-e0e9b9cbc054"], "metadata": {"page_label": "19", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "230ec16e-1645-47ad-a8e7-19d480d15d33": {"node_ids": ["2413bbcb-e474-4694-9f29-836ce83b572d"], "metadata": {"page_label": "20", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "08534bd2-dac8-462b-af06-01f5ccc93a23": {"node_ids": ["f9a4fa6c-ce7a-4980-8c8c-873147cff90e", "595a9f74-c856-4a6d-a667-10b128b02bc6"], "metadata": {"page_label": "21", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "38470abe-515b-4c66-80ba-22edbec22ea8": {"node_ids": ["852d7686-a25d-4195-b529-775ab3f21c5f"], "metadata": {"page_label": "22", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "6ba492ac-4fa3-492e-858b-b8fe2a35bcdc": {"node_ids": ["44b03b3c-2789-44e2-a99e-77f12e8fea54"], "metadata": {"page_label": "23", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "a06d8914-305f-4ad4-b739-9c2375dcdc40": {"node_ids": ["c8302bc0-81ee-40f1-bdbf-caccdab84cda", "29a81eb8-17df-48d1-86d2-40e941b92255"], "metadata": {"page_label": "24", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "a623e6a9-e397-44f9-86c9-8f17f27ee947": {"node_ids": ["96e353a9-4908-481d-8e5d-fd05a52f2ef9"], "metadata": {"page_label": "25", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "4a7ca561-9225-4e39-82b9-88d6c29a01a4": {"node_ids": ["0914fd40-165c-47b1-a081-58f401544be5"], "metadata": {"page_label": "26", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "9b534062-2e70-4154-bf4b-f355754e47c2": {"node_ids": ["808c86eb-44f5-4ce9-a86f-91129c1aa6d8"], "metadata": {"page_label": "27", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "4410f48a-e4ab-4d02-a93c-ec923628a92a": {"node_ids": ["fba36423-407d-4f57-b200-151590f4a123"], "metadata": {"page_label": "28", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "94d46a4a-e6d9-469f-9b88-0297c7e3d5e8": {"node_ids": ["a7136962-c247-477a-84bb-f30c66e24127"], "metadata": {"page_label": "29", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "e8ac5fe3-8b1f-445f-a17c-8126f838aa7c": {"node_ids": ["374b597f-258e-41d9-8109-5fe078eb56b4", "a7837387-ba7d-41b5-baa1-ed78eef23b3a"], "metadata": {"page_label": "30", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "04f3fca9-1f41-4558-a9d7-78676f89e1bf": {"node_ids": ["063ec42a-d491-4d3b-af99-fc71da0fef33"], "metadata": {"page_label": "31", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "e65f4928-49af-4ee6-bcb2-bf1517b76326": {"node_ids": ["fcce4ea6-d681-43e3-a336-5ea57dc78c55"], "metadata": {"page_label": "32", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "d67bf198-a220-4934-8ea7-57b9a7c9ecdc": {"node_ids": ["604c584c-ea8d-4dab-bd4c-7804f0945aa7"], "metadata": {"page_label": "33", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "c1b36c56-23a4-4da0-888e-b5698d51aad7": {"node_ids": ["6bb2238f-9999-4421-9590-421a863d9ac9"], "metadata": {"page_label": "34", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "5bd20468-a0d4-46cd-a56e-60f1dcea63c4": {"node_ids": ["a20b837b-4842-4885-95cb-bcdb8f1ebdab"], "metadata": {"page_label": "35", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "54ebdb15-9f28-41b0-806d-a5c4f2a1f802": {"node_ids": ["fc550ec8-ce7c-46dc-87df-4f97e580b1ce"], "metadata": {"page_label": "36", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "15dac06e-2c16-4bcb-8f39-bf2e67c2c0e7": {"node_ids": ["3ad53d7f-65a4-4999-97c9-eb593ff33d46"], "metadata": {"page_label": "37", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "0446c95b-6313-4390-94b6-e09f81e40077": {"node_ids": ["1cc1aac5-103f-463c-9e0b-2f4fd891f4a2"], "metadata": {"page_label": "38", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "f35eb9bf-0ac9-47f3-837c-3c7d846a7236": {"node_ids": ["6d980368-43e5-4a48-84c1-a9cd2a4879fa", "c607c463-e403-41d9-8627-ade64ae4bed2"], "metadata": {"page_label": "39", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "5d0789f8-f0c1-4acd-8c77-319eb8b019c4": {"node_ids": ["1f607a44-8299-4331-9afe-f4fb156e6ce9"], "metadata": {"page_label": "40", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "08828317-a4c9-4175-bb99-72783dd7978e": {"node_ids": ["1c3e12b0-2d14-4d79-8562-f0656b34b158"], "metadata": {"page_label": "41", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "00455c0b-f906-40f7-b5b7-c7dee663cb48": {"node_ids": ["85835b48-0796-44f6-bb10-7a0464242cb3"], "metadata": {"page_label": "42", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "7a10e64c-0e2b-4c1b-b28b-2f06df29a4b7": {"node_ids": ["84112dac-86dd-459a-a869-64404be4bb1b"], "metadata": {"page_label": "43", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "7007df59-703f-4894-b8e7-1db035832e86": {"node_ids": ["49e765db-71cd-465e-861a-1f0d282ec453"], "metadata": {"page_label": "44", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "14310310-b79d-4265-9928-b3aa27ee2f70": {"node_ids": ["6091070b-ea62-4766-95df-7cf31091e64d"], "metadata": {"page_label": "45", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "c8850023-e7f3-41c6-a310-c00f090aaa7d": {"node_ids": ["e848314c-ff29-4279-b64f-dd76484ba2f7"], "metadata": {"page_label": "46", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "cedbdaba-b3a4-4ac2-aa20-3208b1c78c81": {"node_ids": ["67a74e0f-e3c6-472a-b2ee-15cfd7f42f87"], "metadata": {"page_label": "47", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "d2cb18a4-f1c9-4f1d-953c-41f259daee3e": {"node_ids": ["8e62965f-9708-4b8e-9ea5-d03c111572e8"], "metadata": {"page_label": "48", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "be488adb-e601-4510-a159-4e96ed46ab3f": {"node_ids": ["a861fc75-c1ae-4d50-844b-1d547be3f285"], "metadata": {"page_label": "49", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "1e6efbdb-6889-4f2e-a21c-6acc585ab376": {"node_ids": ["3adb0812-80a6-4cc7-a963-84f2e047dbf0"], "metadata": {"page_label": "50", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "96182cd8-481e-4d67-8e85-c35156048fae": {"node_ids": ["a2b87b05-baa6-4afb-92a3-07dc0c8b9c8c"], "metadata": {"page_label": "51", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "f1535bf5-47b2-4404-b642-9bbb965494ba": {"node_ids": ["0b1e329a-ec59-4717-a8e0-7c396687ab66"], "metadata": {"page_label": "52", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "a916cabb-d677-4c41-9884-fa54c13efdb4": {"node_ids": ["2a672486-b062-4bf6-b1ce-76a87991fbc0"], "metadata": {"page_label": "53", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "8c6e7d93-8978-41b4-95e4-6ee94320b0dc": {"node_ids": ["d9f77a88-be87-4576-832f-a125d49ae211"], "metadata": {"page_label": "54", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "61d95ae2-f09d-4399-9da7-ea9bef5476b9": {"node_ids": ["d4190b0d-afac-4b87-9202-3254270dfb09", "4c531495-f655-4d17-b0be-6dbf8dcc6ef0"], "metadata": {"page_label": "55", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "95933316-7a71-427e-b9c0-e06efcad6ae1": {"node_ids": ["7ce55b2b-206c-4ec4-a7a0-0b1bb0f01ae3", "cf96fb4b-4b3d-4713-966b-3a468d45b7ed"], "metadata": {"page_label": "56", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "c19c292d-8da0-48de-b26c-aa5c123a337c": {"node_ids": ["2b9769aa-b3b6-4f55-b50a-7aa3ccbbfb42", "e8b65c63-ad79-40bf-9e1d-7143ca333727"], "metadata": {"page_label": "57", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "058bd4d8-a556-4dcb-ad25-dfac46c57434": {"node_ids": ["2e42970f-1f45-4073-bef1-75fa29a6f6ba", "16151346-725c-44ec-a945-27f9f0a7c6e2"], "metadata": {"page_label": "58", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "d2ac4098-cd18-4001-8ffa-9ad26be07692": {"node_ids": ["1d088a73-aae6-4f4b-98a5-eee8d2397450", "790b28cd-c953-4dfe-b43d-685a8c4e0c45"], "metadata": {"page_label": "59", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "d21f56c2-1414-46b0-bc4c-39912d033e8d": {"node_ids": ["8e23fbeb-575d-489f-a39d-a385ef36cb11", "e49e7ae9-4792-4883-ac15-ceabc46dcf45"], "metadata": {"page_label": "60", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "a8b769cb-8614-491e-b3df-8d1ae8a2c929": {"node_ids": ["e3137545-1112-402a-b525-2938c73bdd79", "3e53680b-6d72-44cc-8fed-84d3b04b789a"], "metadata": {"page_label": "61", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "e8e43f67-dc65-49b6-aac1-822a40b660d2": {"node_ids": ["f8263039-d6db-46a8-b374-df7f7d9a5a09", "7f137748-e707-4064-b7a0-df25fb6b9d61"], "metadata": {"page_label": "62", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "03dd72de-9ab8-4b8b-9111-2f3b82955706": {"node_ids": ["acf58a5f-764d-4c3b-bf54-5f4eb150a5b9", "f84a2e94-2eff-47d5-a308-291c030f4895"], "metadata": {"page_label": "63", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "61413c3c-f1cd-4f2d-ba03-e8d046214179": {"node_ids": ["8879b566-f473-4f07-97ef-f66f9dfb8d0a", "838d7d92-0e12-4c6a-9a7c-75a39f614939"], "metadata": {"page_label": "64", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "e1f43b35-2583-4bd8-a71b-50ed390f8005": {"node_ids": ["44566039-a56e-4c9b-9e47-8a4a12716f68", "b25350e0-f3b8-4187-bb7b-e004630ca4bf"], "metadata": {"page_label": "65", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "d32f4023-54c4-4085-bfb3-58698e54ac23": {"node_ids": ["ecff162a-6e3e-4227-a8af-9db731be6f26", "e11abd28-184c-45c5-9b6a-cf5194a5c647"], "metadata": {"page_label": "66", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "dce8f4b3-83ff-43d4-b590-497cd9cacca3": {"node_ids": ["2c804ce2-7a9d-4680-a886-2168ce2a2325", "f79262f0-f7a8-4eb2-a245-40f39e581a44"], "metadata": {"page_label": "67", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "92b143b6-1ce4-41ad-966a-4c14f9ea6e65": {"node_ids": ["d3404458-2367-4287-ac43-c2735a4501b8"], "metadata": {"page_label": "68", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "b83bc826-7b87-4a30-b8fe-f869c1865357": {"node_ids": ["f21e70b4-0313-4f8d-b1b8-571cb3b27c7c"], "metadata": {"page_label": "69", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "7115dff1-c5cf-4a75-bada-c9b08c6b5d65": {"node_ids": ["b0d868f3-d451-45d3-922d-1e421d7f7b67"], "metadata": {"page_label": "70", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "0a00d3a8-124b-4c4e-b88b-23fc4f082415": {"node_ids": ["4e71b283-4c02-4db1-9bed-5bc9d0ecd13d"], "metadata": {"page_label": "71", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "cec41cbf-066a-4510-a9d5-625587e2a77f": {"node_ids": ["7e852deb-c95b-4374-8fd2-22203f03e62f"], "metadata": {"page_label": "72", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "58c13c4c-dffa-4fc8-b4fe-268526805d24": {"node_ids": ["f49006a7-0846-43c9-801e-fd2d2fb70923"], "metadata": {"page_label": "73", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "b49f866a-0639-4cd2-aa2e-0d707579c033": {"node_ids": ["fd7e7980-04bf-407a-be9c-090d101b927d"], "metadata": {"page_label": "74", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "8a7070fa-c40f-4d5c-aa1e-514bf592c6df": {"node_ids": ["2891904d-8a76-4387-be1f-dad2b6008549"], "metadata": {"page_label": "75", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "1fdb859b-654d-4324-a676-ad7ea35d7d9b": {"node_ids": ["a2c9bb47-8605-49c4-b568-b76bbef577dd"], "metadata": {"page_label": "76", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "24b856dd-514a-43db-9004-4f3026f302b8": {"node_ids": ["38638883-d6ab-4987-a56f-3ad4b53533d4"], "metadata": {"page_label": "77", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "2564698f-3ad0-4c30-b77c-7c7ef22904f3": {"node_ids": ["85100e28-43c5-4028-b79a-9b30d4b80f99", "01f75143-86aa-4940-9e35-392a96ce4301"], "metadata": {"page_label": "78", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "1a3c9273-ad19-4c9a-9649-c838e6ad875b": {"node_ids": ["d60fae33-fb5c-421d-bf64-62b86084a67f"], "metadata": {"page_label": "79", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}, "c78e45bc-2dee-4d7e-9ede-cd389d66c3fa": {"node_ids": ["b762d901-cfba-4814-8568-81f5e6e63826"], "metadata": {"page_label": "80", "file_name": "E:/Python_work/LLM_MetaGPT/Publish/SelfLearnAssis_BasedOnLanguageModel/RAGTool/files/2401.03568v2.pdf", "file_path": "E:\\Python_work\\LLM_MetaGPT\\Publish\\SelfLearnAssis_BasedOnLanguageModel\\RAGTool\\files\\2401.03568v2.pdf", "file_type": "application/pdf", "file_size": 46299893, "creation_date": "2024-12-30", "last_modified_date": "2024-12-30"}}}}