# 深度学习思维链，推导过程 
## 背景&相关历史
良好推理的标志是，思维互相构建，以创建新的想法和结论。如果你分享一系列称述，并且其他人能跟得上并得出相同的结论，那么你就建立了一个良好的推理过程。图灵测试也是一系列将世界模型简化后，测试算法是否能做出正确决策的测试；相似的，象棋游戏是更简单的世界模型。<br>
拿象棋游戏来看算法逻辑推导方式的演进，象棋游戏中人们最开始是将获胜作为目的，着重提高算法“判断当下位置的获胜可能性”和“预测下一个潜在位置”的能力。<br>1）最开始是从计算当下棋局，直接模拟3步（或多步）之内的所有下棋的可能来判断下一步该如何走的，但是拿国际象棋来说，每一状态下就有35种左右的可能选择，这样计算几步之内就有天量可能（更不要说围棋每一步都有200种以上的可能性）；<br>2）上世纪90年代时出现了新的方法，将棋盘格位置作为输入，并输出一个单一的值，自我博弈利用胜负作为反馈来提高效果，经过百万次迭代训练学会了评估国际象棋的位置，算法在国际象棋上达到了人类水平；3）<br>但是围棋更加复杂，即使只是探索3步之内都存在大量计算。于是到了2014年，通过模仿落子在各个位置的人类行为，计算这些位置的落子概率，研究人员clark和stock.Lee构建了一个新的神经网络，学会了预测人类在当前情况下会下哪一步棋（输出每个棋盘上位置的概率密度分布），但他的表现仍停留在业余水平；<br>4）解决这个问题的方式是：利用“蒙特卡洛搜索树”和“预测当前位置的胜率的神经网络”的结合，即在树结构回溯路径的时候进行一个当下位置的神经网络预测，回溯朝着最有希望的方向。这个方法比直接计算所有可能性理论上降低准确性，但是节约大把算力（即Alpha go的方法，利用更大的神经网络来预测位置直觉和可能方向，并引导蒙特卡洛树筛选出更加好的方向）；<br>
<br>
一些观点认为：对大语言模型的分析可以看出，LLM模型可以模拟几乎所有的世界模型，只要给足上下文。并且，他能够根据任何情况下的建议合理的行动，“Large Language Models are Zero-Shot Reasoners”论文指出，只需要在prompt结尾加上“让我们逐步思考”，就能促使模型开始一系列简单的思考。这些思维链提高了推理任务表现，同时仍然像快速直觉一样，朝着第一个合理的思路前进。<br>
但是，在这种情况下，如果他沿着一条思路走不通，他可能毅然决然的陷入死胡同，无法找到正确的答案。这催生了一些实验：让大模型不沿着单一思维链走，沿着如思维导图一样的多思维链走，然后使用语言模型自身评估哪些看起来最有前途。这也就是COT的最开始思想，类似于AlphaGo 搜索可能的前进序列\方向的思维过程，而不是单纯把所有的移动\前进过程模拟出来。推理、探索和评估路径，在逻辑种尝试多种办法，最终确定最有希望的一种。<br>
但要这么做，你需要一个类人类的老师，给予大模型奖励信号，当执行有效步骤出错时，进行惩罚。OPenAI的一篇论文“Let's Verify Step by Step”，指出了一个方法，模型会推理问题，进行一系列思考，并且对每个步骤进行逐步反馈，处理并逐渐发现更好的推理策略。这导致了推理能力大幅提升。从这里开始，推理所需计算量与执行过程中拿的标记数量成比例关系，同时也影响推理准确性。意味着，推理链条更长，比用更大的模型，对结果准确性更有用。<br>
但是让模型推理链条更长，不是解决更复杂问题的办法，甚至会在特定情况下让他们更弱，更容易原地打转。因此研究人员提出ARC挑战，旨在迫使模型发现新的从未见过的模式，迫使他们进行推理，即从头开始，而不是依赖记忆中的方案。

## 单看一篇论文：基于符号思维链的逻辑推理 SymbCOT
思维链：<br>
一阶逻辑：

## PPO & GRPO
### PPO 解析
<img src="./DLReasoningDeepSeekR1/PPO.png">
首先在上图的 Pair good/bad answers 和 Pretrained Model 以及 RW 步骤中训练 reward model r(s,a)，优化正负样本之间的距离：<br>
L R​ (r ϕ,D)=−E x,y ,y )∼D[logσ(r ​ (x,y w​ )−r ϕ​ (x,y l​ ))]

### GRPO 解析
RL+LLM : <br>
OnPolicy: 每次训练都基于自己的生成模型（Actor），通过教练（Critic）反馈奖励；效率较高，没有模型自生成，问题是模型训练后可能能力不够；<br>
OffPolicy:基于现有标注的情况进行分析，可能有训练样本与模型不匹配的情况；优势是更可能达到模型能力上限，但是效率低。<br>
PPO示意图（PPO很消耗资源）：<br>
<img src="./DLReasoningDeepSeekR1/deepseek_4.png"><br>
1. 整个算法运行起来有四个模型在运行：除了Actor Model ，和专家模型 Critic Model 之外还有 Reward Model 和Reference Model。 
2. Critic Model 通常跟 Actor Model  相当大小
3. LLM 通常只有最后一个 token 会被奖励模型打分，训练在每个token上都准确的价值函数难

GRPO 避免了像PPO那样使用额外的 Critic Model(即Value Model) 近似， 而是使用同一问题下多个采样输出的平均奖励作为 基线。<br>
<img src="./DLReasoningDeepSeekR1/deepseek_5.png"><br>
上面是对比PPO和GRPO的图，如果将PPO中的 Value Model 直接去掉，根据传统的强化学习的经验是无法保证效果的，这个想法相当于直接让Policy Model 对齐Reward Model。 Critic Model 一般是在为了强化学习更加稳定，方差比较少的情况上才加的。现在去掉Critic Model，直接退化成为 Policy Model 是不合理的。为了保证效果能够好，整个算法的关键在于找到一个baseline，使得网络整体在训练的过程中趋于稳定（特别是做过强化学习的都了解，训练是很难保证稳定的，当然这个任务能成功也得益于LLM的泛化性是非常好的的特点）。<br>
解决方案：原先的Critic Model 参与最后的求 Award。 现在没有Critic Model了，直接转变为暴力采样N次，求均值。另外现在变为，Policy Model 去跟 Reference Model 计算 KL 散度。<br>
所以，可见，GRPO 算法没有额外的价值函数，使用的是组内的平均奖励作为基线，使用的组内相对奖励优势函数，这与奖励模型通常在同一问题的不同输出之间进行比较的性质是相符的；  GRPO 直接将训练策略的 πθ 与参考策略的 πref之间计算KL散度，加入到损失函数中，而不是像PPO那样在奖励中添加KL惩罚项。<br>
<br>